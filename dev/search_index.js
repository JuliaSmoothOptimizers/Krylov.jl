var documenterSearchIndex = {"docs":
[{"location":"solvers/spd/","page":"Symmetric positive definite linear systems","title":"Symmetric positive definite linear systems","text":"cg\ncr\ncg_lanczos","category":"page"},{"location":"solvers/spd/#Krylov.cg","page":"Symmetric positive definite linear systems","title":"Krylov.cg","text":"(x, stats) = cg(A, b::AbstractVector{T};\n                M=I, atol::T=√eps(T), rtol::T=√eps(T), restart::Bool=false,\n                itmax::Int=0, radius::T=zero(T), linesearch::Bool=false,\n                verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nThe conjugate gradient method to solve the symmetric linear system Ax=b.\n\nThe method does not abort if A is not definite.\n\nA preconditioner M may be provided in the form of a linear operator and is assumed to be symmetric and positive definite. M also indicates the weighted norm in which residuals are measured.\n\nIf itmax=0, the default number of iterations is set to 2 * n, with n = length(b).\n\nReference\n\nM. R. Hestenes and E. Stiefel, Methods of conjugate gradients for solving linear systems, Journal of Research of the National Bureau of Standards, 49(6), pp. 409–436, 1952.\n\n\n\n\n\n","category":"function"},{"location":"solvers/spd/#Krylov.cr","page":"Symmetric positive definite linear systems","title":"Krylov.cr","text":"(x, stats) = cr(A, b::AbstractVector{T};\n                M=I, atol::T=√eps(T), rtol::T=√eps(T), γ::T=√eps(T), itmax::Int=0,\n                radius::T=zero(T), verbose::Int=0, linesearch::Bool=false, history::Bool=false) where T <: AbstractFloat\n\nA truncated version of Stiefel’s Conjugate Residual method to solve the symmetric linear system Ax = b or the least-squares problem min ‖b - Ax‖. The matrix A must be positive semi-definite.\n\nA preconditioner M may be provided in the form of a linear operator and is assumed to be symmetric and positive definite. M also indicates the weighted norm in which residuals are measured.\n\nIn a linesearch context, 'linesearch' must be set to 'true'.\n\nIf itmax=0, the default number of iterations is set to 2 * n, with n = length(b).\n\nReferences\n\nM. R. Hestenes and E. Stiefel, Methods of conjugate gradients for solving linear systems, Journal of Research of the National Bureau of Standards, 49(6), pp. 409–436, 1952.\nM-A. Dahito and D. Orban, The Conjugate Residual Method in Linesearch and Trust-Region Methods, SIAM Journal on Optimization, 29(3), pp. 1988–2025, 2019.\n\n\n\n\n\n","category":"function"},{"location":"solvers/spd/#Krylov.cg_lanczos","page":"Symmetric positive definite linear systems","title":"Krylov.cg_lanczos","text":"(x, stats) = cg_lanczos(A, b::AbstractVector{T};\n                        M=I, atol::T=√eps(T), rtol::T=√eps(T), itmax::Int=0,\n                        check_curvature::Bool=false, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nThe Lanczos version of the conjugate gradient method to solve the symmetric linear system\n\nAx = b\n\nThe method does not abort if A is not definite.\n\nA preconditioner M may be provided in the form of a linear operator and is assumed to be symmetric and positive definite.\n\nReferences\n\nA. Frommer and P. Maass, Fast CG-Based Methods for Tikhonov-Phillips Regularization, SIAM Journal on Scientific Computing, 20(5), pp. 1831–1850, 1999.\nC. C. Paige and M. A. Saunders, Solution of Sparse Indefinite Systems of Linear Equations, SIAM Journal on Numerical Analysis, 12(4), pp. 617–629, 1975.\n\n\n\n\n\n(x, stats) = cg_lanczos(A, b::AbstractVector{T}, shifts::AbstractVector{T};\n                        M=I, atol::T=√eps(T), rtol::T=√eps(T), itmax::Int=0,\n                        check_curvature::Bool=false, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nThe Lanczos version of the conjugate gradient method to solve a family of shifted systems\n\n(A + αI) x = b  (α = α₁, ..., αₙ)\n\nThe method does not abort if A + αI is not definite.\n\nA preconditioner M may be provided in the form of a linear operator and is assumed to be symmetric and positive definite.\n\n\n\n\n\n","category":"function"},{"location":"solvers/sp_sqd/","page":"Saddle-point and symmetric quasi-definite systems","title":"Saddle-point and symmetric quasi-definite systems","text":"tricg\ntrimr","category":"page"},{"location":"solvers/sp_sqd/#Krylov.tricg","page":"Saddle-point and symmetric quasi-definite systems","title":"Krylov.tricg","text":"(x, y, stats) = tricg(A, b::AbstractVector{T}, c::AbstractVector{T};\n                      M=I, N=I, atol::T=√eps(T), rtol::T=√eps(T),\n                      spd::Bool=false, snd::Bool=false, flip::Bool=false,\n                      τ::T=one(T), ν::T=-one(T), itmax::Int=0, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nTriCG solves the symmetric linear system\n\n[ τE    A ] [ x ] = [ b ]\n[  Aᵀ  νF ] [ y ]   [ c ],\n\nwhere τ and ν are real numbers, E = M⁻¹ ≻ 0 and F = N⁻¹ ≻ 0. TriCG could breakdown if τ = 0 or ν = 0. It's recommended to use TriMR in these cases.\n\nBy default, TriCG solves symmetric and quasi-definite linear systems with τ = 1 and ν = -1. If flip = true, TriCG solves another known variant of SQD systems where τ = -1 and ν = 1. If spd = true, τ = ν = 1 and the associated symmetric and positive definite linear system is solved. If snd = true, τ = ν = -1 and the associated symmetric and negative definite linear system is solved. τ and ν are also keyword arguments that can be directly modified for more specific problems.\n\nTriCG is based on the preconditioned orthogonal tridiagonalization process and its relation with the preconditioned block-Lanczos process.\n\n[ M   0 ]\n[ 0   N ]\n\nindicates the weighted norm in which residuals are measured. It's the Euclidean norm when M and N are identity operators.\n\nTriCG stops when itmax iterations are reached or when ‖rₖ‖ ≤ atol + ‖r₀‖ * rtol. atol is an absolute tolerance and rtol is a relative tolerance.\n\nAdditional details can be displayed if verbose mode is enabled (verbose > 0). Information will be displayed every verbose iterations.\n\nReference\n\nA. Montoison and D. Orban, TriCG and TriMR: Two Iterative Methods for Symmetric Quasi-Definite Systems, SIAM Journal on Scientific Computing, 43(4), pp. 2502–2525, 2021.\n\n\n\n\n\n","category":"function"},{"location":"solvers/sp_sqd/#Krylov.trimr","page":"Saddle-point and symmetric quasi-definite systems","title":"Krylov.trimr","text":"(x, y, stats) = trimr(A, b::AbstractVector{T}, c::AbstractVector{T};\n                      M=I, N=I, atol::T=√eps(T), rtol::T=√eps(T),\n                      spd::Bool=false, snd::Bool=false, flip::Bool=false, sp::Bool=false,\n                      τ::T=one(T), ν::T=-one(T), itmax::Int=0, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nTriMR solves the symmetric linear system\n\n[ τE    A ] [ x ] = [ b ]\n[  Aᵀ  νF ] [ y ]   [ c ],\n\nwhere τ and ν are real numbers, E = M⁻¹ ≻ 0 and F = N⁻¹ ≻ 0. TriMR handles saddle-point systems (τ = 0 or ν = 0) and adjoint systems (τ = 0 and ν = 0) without any risk of breakdown.\n\nBy default, TriMR solves symmetric and quasi-definite linear systems with τ = 1 and ν = -1. If flip = true, TriMR solves another known variant of SQD systems where τ = -1 and ν = 1. If spd = true, τ = ν = 1 and the associated symmetric and positive definite linear system is solved. If snd = true, τ = ν = -1 and the associated symmetric and negative definite linear system is solved. If sp = true, τ = 1, ν = 0 and the associated saddle-point linear system is solved. τ and ν are also keyword arguments that can be directly modified for more specific problems.\n\nTriMR is based on the preconditioned orthogonal tridiagonalization process and its relation with the preconditioned block-Lanczos process.\n\n[ M   0 ]\n[ 0   N ]\n\nindicates the weighted norm in which residuals are measured. It's the Euclidean norm when M and N are identity operators.\n\nTriMR stops when itmax iterations are reached or when ‖rₖ‖ ≤ atol + ‖r₀‖ * rtol. atol is an absolute tolerance and rtol is a relative tolerance.\n\nAdditional details can be displayed if verbose mode is enabled (verbose > 0). Information will be displayed every verbose iterations.\n\nReference\n\nA. Montoison and D. Orban, TriCG and TriMR: Two Iterative Methods for Symmetric Quasi-Definite Systems, SIAM Journal on Scientific Computing, 43(4), pp. 2502–2525, 2021.\n\n\n\n\n\n","category":"function"},{"location":"factorization-free/#factorization-free","page":"Factorization-free operators","title":"Factorization-free operators","text":"","category":"section"},{"location":"factorization-free/","page":"Factorization-free operators","title":"Factorization-free operators","text":"All methods are factorization free, which means that you only need to provide operator-vector products.","category":"page"},{"location":"factorization-free/","page":"Factorization-free operators","title":"Factorization-free operators","text":"The A, M or N input arguments of Krylov.jl solvers can be any object that represents a linear operator. That object must implement mul!, for multiplication with a vector, size() and eltype(). For certain methods it must also implement adjoint().","category":"page"},{"location":"factorization-free/","page":"Factorization-free operators","title":"Factorization-free operators","text":"Some methods only require A * v products, whereas other ones also require A' * u products. In the latter case, adjoint(A) must also be implemented.","category":"page"},{"location":"factorization-free/","page":"Factorization-free operators","title":"Factorization-free operators","text":"A * v A * v and A' * u\nCG, CR CGLS, CRLS, CGNE, CRMR\nSYMMLQ, CG-LANCZOS, MINRES, MINRES-QLP LSLQ, LSQR, LSMR, LNLQ, CRAIG, CRAIGMR\nDIOM, FOM, DQGMRES, GMRES BiLQ, QMR, BiLQR, USYMLQ, USYMQR, TriLQR\nCGS, BICGSTAB TriCG, TriMR, USYMLQR","category":"page"},{"location":"factorization-free/","page":"Factorization-free operators","title":"Factorization-free operators","text":"We strongly recommend LinearOperators.jl to model matrix-free operators, but other packages such as LinearMaps.jl, DiffEqOperators.jl or your own operator can be used as well.","category":"page"},{"location":"factorization-free/","page":"Factorization-free operators","title":"Factorization-free operators","text":"With LinearOperators.jl, operators are defined as","category":"page"},{"location":"factorization-free/","page":"Factorization-free operators","title":"Factorization-free operators","text":"A = LinearOperator(type, nrows, ncols, symmetric, hermitian, prod, tprod, ctprod)","category":"page"},{"location":"factorization-free/","page":"Factorization-free operators","title":"Factorization-free operators","text":"where","category":"page"},{"location":"factorization-free/","page":"Factorization-free operators","title":"Factorization-free operators","text":"type is the operator element type;\nnrow and ncol are its dimensions;\nsymmetric and hermitian should be set to true or false;\nprod(v), tprod(w) and ctprod(u) are called when writing A * v, tranpose(A) * w, and A' * u, respectively.","category":"page"},{"location":"factorization-free/","page":"Factorization-free operators","title":"Factorization-free operators","text":"See the tutorial and the detailed documentation for more informations on LinearOperators.jl.","category":"page"},{"location":"factorization-free/#Examples","page":"Factorization-free operators","title":"Examples","text":"","category":"section"},{"location":"factorization-free/","page":"Factorization-free operators","title":"Factorization-free operators","text":"In the field of nonlinear optimization, finding critical points of a continuous function frequently involves linear systems with a Hessian or Jacobian as coefficient. Materializing such operators as matrices is expensive in terms of operations and memory consumption and is unreasonable for high-dimensional problems. However, it is often possible to implement efficient Hessian-vector and Jacobian-vector products, for example with the help of automatic differentiation tools, and used within Krylov solvers. We now illustrate variants with explicit matrices and with matrix-free operators for two well-known optimization methods.","category":"page"},{"location":"factorization-free/#Example-1:-Newton's-Method-for-convex-optimization","page":"Factorization-free operators","title":"Example 1: Newton's Method for convex optimization","text":"","category":"section"},{"location":"factorization-free/","page":"Factorization-free operators","title":"Factorization-free operators","text":"At each iteration of Newton's method applied to a mathcalC^2 strictly convex function f  mathbbR^n rightarrow mathbbR, a descent direction direction is determined by minimizing the quadratic Taylor model of f:","category":"page"},{"location":"factorization-free/","page":"Factorization-free operators","title":"Factorization-free operators","text":"min_d in mathbbR^nf(x_k) + nabla f(x_k)^T d + tfrac12d^T nabla^2 f(x_k) d","category":"page"},{"location":"factorization-free/","page":"Factorization-free operators","title":"Factorization-free operators","text":"which is equivalent to solving the symmetric and positive-definite system","category":"page"},{"location":"factorization-free/","page":"Factorization-free operators","title":"Factorization-free operators","text":"nabla^2 f(x_k) d  = -nabla f(x_k)","category":"page"},{"location":"factorization-free/","page":"Factorization-free operators","title":"Factorization-free operators","text":"The system above can be solved with the conjugate gradient method as follows, using the explicit Hessian:","category":"page"},{"location":"factorization-free/","page":"Factorization-free operators","title":"Factorization-free operators","text":"using ForwardDiff, Krylov\n\nxk = -ones(4)\n\nf(x) = (x[1] - 1)^2 + (x[2] - 2)^2 + (x[3] - 3)^2 + (x[4] - 4)^2\n\ng(x) = ForwardDiff.gradient(f, x)\n\nH(x) = ForwardDiff.hessian(f, x)\n\nd, stats = cg(H(xk), -g(xk))","category":"page"},{"location":"factorization-free/","page":"Factorization-free operators","title":"Factorization-free operators","text":"The explicit Hessian can be replaced by a linear operator that only computes Hessian-vector products:","category":"page"},{"location":"factorization-free/","page":"Factorization-free operators","title":"Factorization-free operators","text":"using ForwardDiff, LinearOperators, Krylov\n\nxk = -ones(4)\n\nf(x) = (x[1] - 1)^2 + (x[2] - 2)^2 + (x[3] - 3)^2 + (x[4] - 4)^2\n\ng(x) = ForwardDiff.gradient(f, x)\n\nH(v) = ForwardDiff.derivative(t -> g(xk + t * v), 0.0)\nopH = LinearOperator(Float64, 4, 4, true, true, v -> H(v))\n\ncg(opH, -g(xk))","category":"page"},{"location":"factorization-free/#Example-2:-The-Gauss-Newton-Method-for-Nonlinear-Least-Squares","page":"Factorization-free operators","title":"Example 2: The Gauss-Newton Method for Nonlinear Least Squares","text":"","category":"section"},{"location":"factorization-free/","page":"Factorization-free operators","title":"Factorization-free operators","text":"At each iteration of the Gauss-Newton method applied to a nonlinear least-squares objective f(x) = tfrac12 F(x)^2 where F  mathbbR^n rightarrow mathbbR^m is mathcalC^1, we solve the subproblem:","category":"page"},{"location":"factorization-free/","page":"Factorization-free operators","title":"Factorization-free operators","text":"min_d in mathbbR^ntfrac12J(x_k) d + F(x_k)^2","category":"page"},{"location":"factorization-free/","page":"Factorization-free operators","title":"Factorization-free operators","text":"where J(x) is the Jacobian of F at x.","category":"page"},{"location":"factorization-free/","page":"Factorization-free operators","title":"Factorization-free operators","text":"An appropriate iterative method to solve the above linear least-squares problems is LSMR. We could pass the explicit Jacobian to LSMR as follows:","category":"page"},{"location":"factorization-free/","page":"Factorization-free operators","title":"Factorization-free operators","text":"using ForwardDiff, Krylov\n\nxk = ones(2)\n\nF(x) = [x[1]^4 - 3; exp(x[2]) - 2; log(x[1]) - x[2]^2]\n\nJ(x) = ForwardDiff.jacobian(F, x)\n\nd, stats = lsmr(J(xk), -F(xk))","category":"page"},{"location":"factorization-free/","page":"Factorization-free operators","title":"Factorization-free operators","text":"However, the explicit Jacobian can be replaced by a linear operator that only computes Jacobian-vector and transposed Jacobian-vector products:","category":"page"},{"location":"factorization-free/","page":"Factorization-free operators","title":"Factorization-free operators","text":"using LinearAlgebra, ForwardDiff, LinearOperators, Krylov\n\nxk = ones(2)\n\nF(x) = [x[1]^4 - 3; exp(x[2]) - 2; log(x[1]) - x[2]^2]\n\nJ(v) = ForwardDiff.derivative(t -> F(xk + t * v), 0)\nJᵀ(u) = ForwardDiff.gradient(x -> dot(F(x), u), xk)\nopJ = LinearOperator(Float64, 3, 2, false, false, v -> J(v), w -> Jᵀ(w), u -> Jᵀ(u))\n\nlsmr(opJ, -F(xk))","category":"page"},{"location":"factorization-free/","page":"Factorization-free operators","title":"Factorization-free operators","text":"Note that preconditioners can be also implemented as abstract operators. For instance, we could compute the Cholesky factorization of M and N and create linear operators that perform the forward and backsolves.","category":"page"},{"location":"factorization-free/","page":"Factorization-free operators","title":"Factorization-free operators","text":"Krylov methods combined with factorization free operators allow to reduce computation time and memory requirements considerably by avoiding building and storing the system matrix. In the field of partial differential equations, the implementation of high-performance factorization free operators and assembly free preconditioning is a subject of active research.","category":"page"},{"location":"inplace/#In-place-methods","page":"In-place methods","title":"In-place methods","text":"","category":"section"},{"location":"inplace/","page":"In-place methods","title":"In-place methods","text":"All solvers in Krylov.jl have an in-place variant implemented in a method whose name ends with !. A workspace (KrylovSolver) that contains the storage needed by a Krylov method can be used to solve multiple linear systems that have the same dimensions in the same floating-point precision. Each KrylovSolver has two constructors:","category":"page"},{"location":"inplace/","page":"In-place methods","title":"In-place methods","text":"XyzSolver(A, b)\nXyzSolver(m, n, S)","category":"page"},{"location":"inplace/","page":"In-place methods","title":"In-place methods","text":"Xyz is the name of the Krylov method with lowercase letters except its first one (Cg, Minres, Lsmr, Bicgstab, ...). Given an operator A and a right-hand side b, you can create a KrylovSolver based on the size of A and the type of b or explicitly give the dimensions (m, n) and the storage type S.","category":"page"},{"location":"inplace/","page":"In-place methods","title":"In-place methods","text":"For example, use S = Vector{Float64} if you want to solve linear systems in double precision on the CPU and S = CuVector{Float32} if you want to solve linear systems in single precision on an Nvidia GPU.","category":"page"},{"location":"inplace/","page":"In-place methods","title":"In-place methods","text":"note: Note\nDiomSolver, DqgmresSolver and CgLanczosShiftSolver require an additional argument (memory or nshifts).","category":"page"},{"location":"inplace/","page":"In-place methods","title":"In-place methods","text":"The workspace is always the first argument of the in-place methods:","category":"page"},{"location":"inplace/","page":"In-place methods","title":"In-place methods","text":"minres_solver = MinresSolver(n, n, Vector{Float64})\nx, stats = minres!(minres_solver, A1, b1)\n\ndqgmres_solver = DqgmresSolver(n, n, memory, Vector{BigFloat})\nx, stats = dqgmres!(dqgmres_solver, A2, b2)\n\nlsqr_solver = LsqrSolver(m, n, CuVector{Float32})\nx, stats = lsqr!(lsqr_solver, A3, b3)","category":"page"},{"location":"inplace/","page":"In-place methods","title":"In-place methods","text":"A generic function solve! is also available and dispatches to the appropriate Krylov method.","category":"page"},{"location":"inplace/","page":"In-place methods","title":"In-place methods","text":"Krylov.solve!","category":"page"},{"location":"inplace/#Krylov.solve!","page":"In-place methods","title":"Krylov.solve!","text":"solve!(solver, args...; kwargs...)\n\nUse the in-place Krylov method associated to solver.\n\n\n\n\n\n","category":"function"},{"location":"inplace/#Examples","page":"In-place methods","title":"Examples","text":"","category":"section"},{"location":"inplace/","page":"In-place methods","title":"In-place methods","text":"We illustrate the use of in-place Krylov solvers with two well-known optimization methods. The details of the optimization methods are described in the section about Factorization-free operators.","category":"page"},{"location":"inplace/#Example-1:-Newton's-method-for-convex-optimization-without-linesearch","page":"In-place methods","title":"Example 1: Newton's method for convex optimization without linesearch","text":"","category":"section"},{"location":"inplace/","page":"In-place methods","title":"In-place methods","text":"using Krylov\n\nfunction newton(∇f, ∇²f, x₀; itmax = 200, tol = 1e-8)\n\n    n = length(x₀)\n    x = copy(x₀)\n    gx = ∇f(x)\n    \n    iter = 0\n    S = typeof(x)\n    solver = CgSolver(n, n, S)\n\n    solved = false\n    tired = false\n\n    while !(solved || tired)\n \n        Hx = ∇²f(x)                       # Compute ∇²f(xₖ)\n        Δx, stats = cg!(solver, Hx, -gx)  # Solve ∇²f(xₖ)Δx = -∇f(xₖ)\n        x = x + Δx                        # Update xₖ₊₁ = xₖ + Δx\n        gx = ∇f(x)                        # ∇f(xₖ₊₁)\n        \n        iter += 1\n        solved = norm(gx) ≤ tol\n        tired = iter ≥ itmax\n    end\n    return x\nend","category":"page"},{"location":"inplace/#Example-2:-The-Gauss-Newton-method-for-nonlinear-least-squares-without-linesearch","page":"In-place methods","title":"Example 2: The Gauss-Newton method for nonlinear least squares without linesearch","text":"","category":"section"},{"location":"inplace/","page":"In-place methods","title":"In-place methods","text":"using Krylov\n\nfunction gauss_newton(F, JF, x₀; itmax = 200, tol = 1e-8)\n\n    n = length(x₀)\n    x = copy(x₀)\n    Fx = F(x)\n    m = length(Fx)\n    \n    iter = 0\n    S = typeof(x)\n    solver = LsmrSolver(m, n, S)\n\n    solved = false\n    tired = false\n\n    while !(solved || tired)\n \n        Jx = JF(x)                          # Compute J(xₖ)\n        Δx, stats = lsmr!(solver, Jx, -Fx)  # Minimize ‖J(xₖ)Δx + F(xₖ)‖\n        x = x + Δx                          # Update xₖ₊₁ = xₖ + Δx\n        Fx_old = Fx                         # F(xₖ)\n        Fx = F(x)                           # F(xₖ₊₁)\n        \n        iter += 1\n        solved = norm(Fx - Fx_old) / norm(Fx) ≤ tol\n        tired = iter ≥ itmax\n    end\n    return x\nend","category":"page"},{"location":"solvers/as/","page":"Adjoint systems","title":"Adjoint systems","text":"bilqr\ntrilqr","category":"page"},{"location":"solvers/as/#Krylov.bilqr","page":"Adjoint systems","title":"Krylov.bilqr","text":"(x, t, stats) = bilqr(A, b::AbstractVector{T}, c::AbstractVector{T};\n                      atol::T=√eps(T), rtol::T=√eps(T), transfer_to_bicg::Bool=true,\n                      itmax::Int=0, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nCombine BiLQ and QMR to solve adjoint systems.\n\n[0  A] [t] = [b]\n[Aᵀ 0] [x]   [c]\n\nBiLQ is used for solving primal system Ax = b. QMR is used for solving dual system Aᵀt = c.\n\nAn option gives the possibility of transferring from the BiLQ point to the BiCG point, when it exists. The transfer is based on the residual norm.\n\nReference\n\nA. Montoison and D. Orban, BiLQ: An Iterative Method for Nonsymmetric Linear Systems with a Quasi-Minimum Error Property, SIAM Journal on Matrix Analysis and Applications, 41(3), pp. 1145–1166, 2020.\n\n\n\n\n\n","category":"function"},{"location":"solvers/as/#Krylov.trilqr","page":"Adjoint systems","title":"Krylov.trilqr","text":"(x, t, stats) = trilqr(A, b::AbstractVector{T}, c::AbstractVector{T};\n                       atol::T=√eps(T), rtol::T=√eps(T), transfer_to_usymcg::Bool=true,\n                       itmax::Int=0, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nCombine USYMLQ and USYMQR to solve adjoint systems.\n\n[0  A] [t] = [b]\n[Aᵀ 0] [x]   [c]\n\nUSYMLQ is used for solving primal system Ax = b. USYMQR is used for solving dual system Aᵀt = c.\n\nAn option gives the possibility of transferring from the USYMLQ point to the USYMCG point, when it exists. The transfer is based on the residual norm.\n\nReference\n\nA. Montoison and D. Orban, BiLQ: An Iterative Method for Nonsymmetric Linear Systems with a Quasi-Minimum Error Property, SIAM Journal on Matrix Analysis and Applications, 41(3), pp. 1145–1166, 2020.\n\n\n\n\n\n","category":"function"},{"location":"api/#Stats-Types","page":"API","title":"Stats Types","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Krylov.KrylovStats\nKrylov.SimpleStats\nKrylov.LanczosStats\nKrylov.LanczosShiftStats\nKrylov.SymmlqStats\nKrylov.AdjointStats\nKrylov.LNLQStats\nKrylov.LSLQStats","category":"page"},{"location":"api/#Krylov.KrylovStats","page":"API","title":"Krylov.KrylovStats","text":"Abstract type for statistics returned by a solver\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.SimpleStats","page":"API","title":"Krylov.SimpleStats","text":"Type for statistics returned by the majority of Krylov solvers, the attributes are:\n\nsolved\ninconsistent\nresiduals\nAresiduals\nstatus\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.LanczosStats","page":"API","title":"Krylov.LanczosStats","text":"Type for statistics returned by CG-LANCZOS, the attributes are:\n\nsolved\nresiduals\nindefinite\nAnorm\nAcond\nstatus\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.LanczosShiftStats","page":"API","title":"Krylov.LanczosShiftStats","text":"Type for statistics returned by CG-LANCZOS with shifts, the attributes are:\n\nsolved\nresiduals\nindefinite\nAnorm\nAcond\nstatus\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.SymmlqStats","page":"API","title":"Krylov.SymmlqStats","text":"Type for statistics returned by SYMMLQ, the attributes are:\n\nsolved\nresiduals\nresidualscg\nerrors\nerrorscg\nAnorm\nAcond\nstatus\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.AdjointStats","page":"API","title":"Krylov.AdjointStats","text":"Type for statistics returned by adjoint systems solvers BiLQR and TriLQR, the attributes are:\n\nsolved_primal\nsolved_dual\nresiduals_primal\nresiduals_dual\nstatus\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.LNLQStats","page":"API","title":"Krylov.LNLQStats","text":"Type for statistics returned by the LNLQ method, the attributes are:\n\nsolved\nresiduals\nerrorwithbnd\nerrorbndx\nerrorbndy\nstatus\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.LSLQStats","page":"API","title":"Krylov.LSLQStats","text":"Type for statistics returned by the LSLQ method, the attributes are:\n\nsolved\ninconsistent\nresiduals\nAresiduals\nerr_lbnds\nerrorwithbnd\nerrubndslq\nerrubndscg\nstatus\n\n\n\n\n\n","category":"type"},{"location":"api/#Solver-Types","page":"API","title":"Solver Types","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"KrylovSolver\nMinresSolver\nCgSolver\nCrSolver\nSymmlqSolver\nCgLanczosSolver\nCgLanczosShiftSolver\nMinresQlpSolver\nDiomSolver\nFomSolver\nDqgmresSolver\nGmresSolver\nUsymlqSolver\nUsymqrSolver\nTricgSolver\nTrimrSolver\nTrilqrSolver\nCgsSolver\nBicgstabSolver\nBilqSolver\nQmrSolver\nBilqrSolver\nCglsSolver\nCrlsSolver\nCgneSolver\nCrmrSolver\nLslqSolver\nLsqrSolver\nLsmrSolver\nLnlqSolver\nCraigSolver\nCraigmrSolver","category":"page"},{"location":"api/#Krylov.KrylovSolver","page":"API","title":"Krylov.KrylovSolver","text":"Abstract type for using Krylov solvers in-place\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.MinresSolver","page":"API","title":"Krylov.MinresSolver","text":"Type for storing the vectors required by the in-place version of MINRES.\n\nThe outer constructors\n\nsolver = MinresSolver(n, m, S; window :: Int=5)\nsolver = MinresSolver(A, b; window :: Int=5)\n\nmay be used in order to create these vectors.\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.CgSolver","page":"API","title":"Krylov.CgSolver","text":"Type for storing the vectors required by the in-place version of CG.\n\nThe outer constructors\n\nsolver = CgSolver(n, m, S)\nsolver = CgSolver(A, b)\n\nmay be used in order to create these vectors.\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.CrSolver","page":"API","title":"Krylov.CrSolver","text":"Type for storing the vectors required by the in-place version of CR.\n\nThe outer constructors\n\nsolver = CrSolver(n, m, S)\nsolver = CrSolver(A, b)\n\nmay be used in order to create these vectors.\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.SymmlqSolver","page":"API","title":"Krylov.SymmlqSolver","text":"Type for storing the vectors required by the in-place version of SYMMLQ.\n\nThe outer constructors\n\nsolver = SymmlqSolver(n, m, S)\nsolver = SymmlqSolver(A, b)\n\nmay be used in order to create these vectors.\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.CgLanczosSolver","page":"API","title":"Krylov.CgLanczosSolver","text":"Type for storing the vectors required by the in-place version of CG-LANCZOS.\n\nThe outer constructors\n\nsolver = CgLanczosSolver(n, m, S)\nsolver = CgLanczosSolver(A, b)\n\nmay be used in order to create these vectors.\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.CgLanczosShiftSolver","page":"API","title":"Krylov.CgLanczosShiftSolver","text":"Type for storing the vectors required by the in-place version of CG-LANCZOS with shifts.\n\nThe outer constructors\n\nsolver = CgLanczosShiftSolver(n, m, nshifts, S)\nsolver = CgLanczosShiftSolver(A, b, nshifts)\n\nmay be used in order to create these vectors.\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.MinresQlpSolver","page":"API","title":"Krylov.MinresQlpSolver","text":"Type for storing the vectors required by the in-place version of MINRES-QLP.\n\nThe outer constructors\n\nsolver = MinresQlpSolver(n, m, S)\nsolver = MinresQlpSolver(A, b)\n\nmay be used in order to create these vectors.\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.DiomSolver","page":"API","title":"Krylov.DiomSolver","text":"Type for storing the vectors required by the in-place version of DIOM.\n\nThe outer constructors\n\nsolver = DiomSolver(n, m, memory, S)\nsolver = DiomSolver(A, b, memory)\n\nmay be used in order to create these vectors.\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.FomSolver","page":"API","title":"Krylov.FomSolver","text":"Type for storing the vectors required by the in-place version of FOM.\n\nThe outer constructors\n\nsolver = FomSolver(n, m, memory, S)\nsolver = FomSolver(A, b, memory)\n\nmay be used in order to create these vectors.\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.DqgmresSolver","page":"API","title":"Krylov.DqgmresSolver","text":"Type for storing the vectors required by the in-place version of DQGMRES.\n\nThe outer constructors\n\nsolver = DqgmresSolver(n, m, memory, S)\nsolver = DqgmresSolver(A, b, memory)\n\nmay be used in order to create these vectors.\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.GmresSolver","page":"API","title":"Krylov.GmresSolver","text":"Type for storing the vectors required by the in-place version of GMRES.\n\nThe outer constructors\n\nsolver = GmresSolver(n, m, memory, S)\nsolver = GmresSolver(A, b, memory)\n\nmay be used in order to create these vectors.\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.UsymlqSolver","page":"API","title":"Krylov.UsymlqSolver","text":"Type for storing the vectors required by the in-place version of USYMLQ.\n\nThe outer constructors\n\nsolver = UsymlqSolver(n, m, S)\nsolver = UsymlqSolver(A, b)\n\nmay be used in order to create these vectors.\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.UsymqrSolver","page":"API","title":"Krylov.UsymqrSolver","text":"Type for storing the vectors required by the in-place version of USYMQR.\n\nThe outer constructors\n\nsolver = UsymqrSolver(n, m, S)\nsolver = UsymqrSolver(A, b)\n\nmay be used in order to create these vectors.\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.TricgSolver","page":"API","title":"Krylov.TricgSolver","text":"Type for storing the vectors required by the in-place version of TRICG.\n\nThe outer constructors\n\nsolver = TricgSolver(n, m, S)\nsolver = TricgSolver(A, b)\n\nmay be used in order to create these vectors.\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.TrimrSolver","page":"API","title":"Krylov.TrimrSolver","text":"Type for storing the vectors required by the in-place version of TRIMR.\n\nThe outer constructors\n\nsolver = TrimrSolver(n, m, S)\nsolver = TrimrSolver(A, b)\n\nmay be used in order to create these vectors.\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.TrilqrSolver","page":"API","title":"Krylov.TrilqrSolver","text":"Type for storing the vectors required by the in-place version of TRILQR.\n\nThe outer constructors\n\nsolver = TrilqrSolver(n, m, S)\nsolver = TrilqrSolver(A, b)\n\nmay be used in order to create these vectors.\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.CgsSolver","page":"API","title":"Krylov.CgsSolver","text":"Type for storing the vectors required by the in-place version of CGS.\n\nThe outer constructorss\n\nsolver = CgsSolver(n, m, S)\nsolver = CgsSolver(A, b)\n\nmay be used in order to create these vectors.\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.BicgstabSolver","page":"API","title":"Krylov.BicgstabSolver","text":"Type for storing the vectors required by the in-place version of BICGSTAB.\n\nThe outer constructors\n\nsolver = BicgstabSolver(n, m, S)\nsolver = BicgstabSolver(A, b)\n\nmay be used in order to create these vectors.\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.BilqSolver","page":"API","title":"Krylov.BilqSolver","text":"Type for storing the vectors required by the in-place version of BILQ.\n\nThe outer constructors\n\nsolver = BilqSolver(n, m, S)\nsolver = BilqSolver(A, b)\n\nmay be used in order to create these vectors.\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.QmrSolver","page":"API","title":"Krylov.QmrSolver","text":"Type for storing the vectors required by the in-place version of QMR.\n\nThe outer constructors\n\nsolver = QmrSolver(n, m, S)\nsolver = QmrSolver(A, b)\n\nmay be used in order to create these vectors.\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.BilqrSolver","page":"API","title":"Krylov.BilqrSolver","text":"Type for storing the vectors required by the in-place version of BILQR.\n\nThe outer constructors\n\nsolver = BilqrSolver(n, m, S)\nsolver = BilqrSolver(A, b)\n\nmay be used in order to create these vectors.\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.CglsSolver","page":"API","title":"Krylov.CglsSolver","text":"Type for storing the vectors required by the in-place version of CGLS.\n\nThe outer constructors\n\nsolver = CglsSolver(n, m, S)\nsolver = CglsSolver(A, b)\n\nmay be used in order to create these vectors.\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.CrlsSolver","page":"API","title":"Krylov.CrlsSolver","text":"Type for storing the vectors required by the in-place version of CRLS.\n\nThe outer constructors\n\nsolver = CrlsSolver(n, m, S)\nsolver = CrlsSolver(A, b)\n\nmay be used in order to create these vectors.\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.CgneSolver","page":"API","title":"Krylov.CgneSolver","text":"Type for storing the vectors required by the in-place version of CGNE.\n\nThe outer constructors\n\nsolver = CgneSolver(n, m, S)\nsolver = CgneSolver(A, b)\n\nmay be used in order to create these vectors.\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.CrmrSolver","page":"API","title":"Krylov.CrmrSolver","text":"Type for storing the vectors required by the in-place version of CRMR.\n\nThe outer constructors\n\nsolver = CrmrSolver(n, m, S)\nsolver = CrmrSolver(A, b)\n\nmay be used in order to create these vectors.\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.LslqSolver","page":"API","title":"Krylov.LslqSolver","text":"Type for storing the vectors required by the in-place version of LSLQ.\n\nThe outer constructors\n\nsolver = LslqSolver(n, m, S)\nsolver = LslqSolver(A, b)\n\nmay be used in order to create these vectors.\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.LsqrSolver","page":"API","title":"Krylov.LsqrSolver","text":"Type for storing the vectors required by the in-place version of LSQR.\n\nThe outer constructors\n\nsolver = LsqrSolver(n, m, S)\nsolver = LsqrSolver(A, b)\n\nmay be used in order to create these vectors.\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.LsmrSolver","page":"API","title":"Krylov.LsmrSolver","text":"Type for storing the vectors required by the in-place version of LSMR.\n\nThe outer constructors\n\nsolver = LsmrSolver(n, m, S)\nsolver = LsmrSolver(A, b)\n\nmay be used in order to create these vectors.\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.LnlqSolver","page":"API","title":"Krylov.LnlqSolver","text":"Type for storing the vectors required by the in-place version of LNLQ.\n\nThe outer constructors\n\nsolver = LnlqSolver(n, m, S)\nsolver = LnlqSolver(A, b)\n\nmay be used in order to create these vectors.\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.CraigSolver","page":"API","title":"Krylov.CraigSolver","text":"Type for storing the vectors required by the in-place version of CRAIG.\n\nThe outer constructors\n\nsolver = CraigSolver(n, m, S)\nsolver = CraigSolver(A, b)\n\nmay be used in order to create these vectors.\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.CraigmrSolver","page":"API","title":"Krylov.CraigmrSolver","text":"Type for storing the vectors required by the in-place version of CRAIGMR.\n\nThe outer constructors\n\nsolver = CraigmrSolver(n, m, S)\nsolver = CraigmrSolver(A, b)\n\nmay be used in order to create these vectors.\n\n\n\n\n\n","category":"type"},{"location":"api/#Utilities","page":"API","title":"Utilities","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Krylov.roots_quadratic\nKrylov.sym_givens\nKrylov.to_boundary\nKrylov.vec2str\nKrylov.ktypeof\nKrylov.kzeros\nKrylov.kones","category":"page"},{"location":"api/#Krylov.roots_quadratic","page":"API","title":"Krylov.roots_quadratic","text":"roots = roots_quadratic(q₂, q₁, q₀; nitref)\n\nFind the real roots of the quadratic\n\nq(x) = q₂ x² + q₁ x + q₀,\n\nwhere q₂, q₁ and q₀ are real. Care is taken to avoid numerical cancellation. Optionally, nitref steps of iterative refinement may be performed to improve accuracy. By default, nitref=1.\n\n\n\n\n\n","category":"function"},{"location":"api/#Krylov.sym_givens","page":"API","title":"Krylov.sym_givens","text":"(c, s, ρ) = sym_givens(a, b)\n\nNumerically stable symmetric Givens reflection. Given a and b reals, return (c, s, ρ) such that\n\n[ c  s ] [ a ] = [ ρ ]\n[ s -c ] [ b ] = [ 0 ].\n\n\n\n\n\nNumerically stable symmetric Givens reflection. Given a and b complexes, return (c, s, ρ) with c real and (s, ρ) complexes such that\n\n[ c   s ] [ a ] = [ ρ ]\n[ s̅  -c ] [ b ] = [ 0 ].\n\n\n\n\n\n","category":"function"},{"location":"api/#Krylov.to_boundary","page":"API","title":"Krylov.to_boundary","text":"roots = to_boundary(x, d, radius; flip, xNorm2, dNorm2)\n\nGiven a trust-region radius radius, a vector x lying inside the trust-region and a direction d, return σ1 and σ2 such that\n\n‖x + σi d‖ = radius, i = 1, 2\n\nin the Euclidean norm. If known, ‖x‖² may be supplied in xNorm2.\n\nIf flip is set to true, σ1 and σ2 are computed such that\n\n‖x - σi d‖ = radius, i = 1, 2.\n\n\n\n\n\n","category":"function"},{"location":"api/#Krylov.vec2str","page":"API","title":"Krylov.vec2str","text":"s = vec2str(x; ndisp)\n\nDisplay an array in the form\n\n[ -3.0e-01 -5.1e-01  1.9e-01 ... -2.3e-01 -4.4e-01  2.4e-01 ]\n\nwith (ndisp - 1)/2 elements on each side.\n\n\n\n\n\n","category":"function"},{"location":"api/#Krylov.ktypeof","page":"API","title":"Krylov.ktypeof","text":"S = ktypeof(v)\n\nReturn a dense storage type S based on the type of v.\n\n\n\n\n\n","category":"function"},{"location":"api/#Krylov.kzeros","page":"API","title":"Krylov.kzeros","text":"v = kzeros(S, n)\n\nCreate an AbstractVector of storage type S of length n only composed of zero.\n\n\n\n\n\n","category":"function"},{"location":"api/#Krylov.kones","page":"API","title":"Krylov.kones","text":"v = kones(S, n)\n\nCreate an AbstractVector of storage type S of length n only composed of one.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ls/","page":"Least-squares problems","title":"Least-squares problems","text":"cgls\ncrls\nlslq\nlsqr\nlsmr","category":"page"},{"location":"solvers/ls/#Krylov.cgls","page":"Least-squares problems","title":"Krylov.cgls","text":"(x, stats) = cgls(A, b::AbstractVector{T};\n                  M=I, λ::T=zero(T), atol::T=√eps(T), rtol::T=√eps(T),\n                  radius::T=zero(T), itmax::Int=0, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the regularized linear least-squares problem\n\nminimize ‖b - Ax‖₂² + λ‖x‖₂²\n\nusing the Conjugate Gradient (CG) method, where λ ≥ 0 is a regularization parameter. This method is equivalent to applying CG to the normal equations\n\n(AᵀA + λI) x = Aᵀb\n\nbut is more stable.\n\nCGLS produces monotonic residuals ‖r‖₂ but not optimality residuals ‖Aᵀr‖₂. It is formally equivalent to LSQR, though can be slightly less accurate, but simpler to implement.\n\nReferences\n\nM. R. Hestenes and E. Stiefel. Methods of conjugate gradients for solving linear systems, Journal of Research of the National Bureau of Standards, 49(6), pp. 409–436, 1952.\nA. Björck, T. Elfving and Z. Strakos, Stability of Conjugate Gradient and Lanczos Methods for Linear Least Squares Problems, SIAM Journal on Matrix Analysis and Applications, 19(3), pp. 720–736, 1998.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ls/#Krylov.crls","page":"Least-squares problems","title":"Krylov.crls","text":"(x, stats) = crls(A, b::AbstractVector{T};\n                  M=I, λ::T=zero(T), atol::T=√eps(T), rtol::T=√eps(T),\n                  radius::T=zero(T), itmax::Int=0, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the linear least-squares problem\n\nminimize ‖b - Ax‖₂² + λ‖x‖₂²\n\nusing the Conjugate Residuals (CR) method. This method is equivalent to applying MINRES to the normal equations\n\n(AᵀA + λI) x = Aᵀb.\n\nThis implementation recurs the residual r := b - Ax.\n\nCRLS produces monotonic residuals ‖r‖₂ and optimality residuals ‖Aᵀr‖₂. It is formally equivalent to LSMR, though can be substantially less accurate, but simpler to implement.\n\nReference\n\nD. C.-L. Fong, Minimum-Residual Methods for Sparse, Least-Squares using Golubg-Kahan Bidiagonalization, Ph.D. Thesis, Stanford University, 2011.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ls/#Krylov.lslq","page":"Least-squares problems","title":"Krylov.lslq","text":"(x, stats) = lslq(A, b::AbstractVector{T};\n                  M=I, N=I, sqd::Bool=false, λ::T=zero(T),\n                  atol::T=√eps(T), btol::T=√eps(T), etol::T=√eps(T),\n                  window::Int=5, utol::T=√eps(T), itmax::Int=0,\n                  σ::T=zero(T), transfer_to_lsqr::Bool=false, \n                  conlim::T=1/√eps(T), verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the regularized linear least-squares problem\n\nminimize ‖b - Ax‖₂² + λ²‖x‖₂²\n\nusing the LSLQ method, where λ ≥ 0 is a regularization parameter. LSLQ is formally equivalent to applying SYMMLQ to the normal equations\n\n(AᵀA + λ²I) x = Aᵀb\n\nbut is more stable.\n\nMain features\n\nthe solution estimate is updated along orthogonal directions\nthe norm of the solution estimate ‖xᴸₖ‖₂ is increasing\nthe error ‖eₖ‖₂ := ‖xᴸₖ - x*‖₂ is decreasing\nit is possible to transition cheaply from the LSLQ iterate to the LSQR iterate if there is an advantage (there always is in terms of error)\nif A is rank deficient, identify the minimum least-squares solution\n\nOptional arguments\n\nM: a symmetric and positive definite dual preconditioner\nN: a symmetric and positive definite primal preconditioner\nsqd indicates whether or not we are solving a symmetric and quasi-definite augmented system\n\nIf sqd = true, we solve the symmetric and quasi-definite system\n\n[ E    A ] [ r ]   [ b ]\n[ Aᵀ  -F ] [ x ] = [ 0 ],\n\nwhere E and F are symmetric and positive definite. The system above represents the optimality conditions of\n\nminimize ‖b - Ax‖²_E⁻¹ + ‖x‖²_F.\n\nFor a symmetric and positive definite matrix K, the K-norm of a vector x is ‖x‖²_K = xᵀKx. LSLQ is then equivalent to applying SYMMLQ to (AᵀE⁻¹A + F)x = AᵀE⁻¹b with r = E⁻¹(b - Ax). Preconditioners M = E⁻¹ ≻ 0 and N = F⁻¹ ≻ 0 may be provided in the form of linear operators.\n\nIf sqd is set to false (the default), we solve the symmetric and indefinite system\n\n[ E    A ] [ r ]   [ b ]\n[ Aᵀ   0 ] [ x ] = [ 0 ].\n\nThe system above represents the optimality conditions of\n\nminimize ‖b - Ax‖²_E⁻¹.\n\nIn this case, N can still be specified and indicates the weighted norm in which x and Aᵀr should be measured. r can be recovered by computing E⁻¹(b - Ax).\n\nλ is a regularization parameter (see the problem statement above)\nσ is an underestimate of the smallest nonzero singular value of A–-setting σ too large will result in an error in the course of the iterations\natol is a stopping tolerance based on the residual\nbtol is a stopping tolerance used to detect zero-residual problems\netol is a stopping tolerance based on the lower bound on the error\nwindow is the number of iterations used to accumulate a lower bound on the error\nutol is a stopping tolerance based on the upper bound on the error\ntransfer_to_lsqr return the CG solution estimate (i.e., the LSQR point) instead of the LQ estimate\nitmax is the maximum number of iterations (0 means no imposed limit)\nconlim is the limit on the estimated condition number of A beyond which the solution will be abandoned\nverbose determines verbosity.\n\nReturn values\n\nlslq returns the tuple (x, stats) where\n\nx is the LQ solution estimate\nstats collects other statistics on the run in a LSLQStats\nstats.err_lbnds is a vector of lower bounds on the LQ error–-the vector is empty if window is set to zero\nstats.err_ubnds_lq is a vector of upper bounds on the LQ error–-the vector is empty if σ == 0 is left at zero\nstats.err_ubnds_cg is a vector of upper bounds on the CG error–-the vector is empty if σ == 0 is left at zero\nstats.error_with_bnd is a boolean indicating whether there was an error in the upper bounds computation (cancellation errors, too large σ ...)\n\nStopping conditions\n\nThe iterations stop as soon as one of the following conditions holds true:\n\nthe optimality residual is sufficiently small (stats.status = \"found approximate minimum least-squares solution\") in the sense that either\n‖Aᵀr‖ / (‖A‖ ‖r‖) ≤ atol, or\n1 + ‖Aᵀr‖ / (‖A‖ ‖r‖) ≤ 1\nan approximate zero-residual solution has been found (stats.status = \"found approximate zero-residual solution\") in the sense that either\n‖r‖ / ‖b‖ ≤ btol + atol ‖A‖ * ‖xᴸ‖ / ‖b‖, or\n1 + ‖r‖ / ‖b‖ ≤ 1\nthe estimated condition number of A is too large in the sense that either\n1/cond(A) ≤ 1/conlim (stats.status = \"condition number exceeds tolerance\"), or\n1 + 1/cond(A) ≤ 1 (stats.status = \"condition number seems too large for this machine\")\nthe lower bound on the LQ forward error is less than etol * ‖xᴸ‖\nthe upper bound on the CG forward error is less than utol * ‖xᶜ‖\n\nReferences\n\nR. Estrin, D. Orban and M. A. Saunders, Euclidean-norm error bounds for SYMMLQ and CG, SIAM Journal on Matrix Analysis and Applications, 40(1), pp. 235–253, 2019.\nR. Estrin, D. Orban and M. A. Saunders, LSLQ: An Iterative Method for Linear Least-Squares with an Error Minimization Property, SIAM Journal on Matrix Analysis and Applications, 40(1), pp. 254–275, 2019.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ls/#Krylov.lsqr","page":"Least-squares problems","title":"Krylov.lsqr","text":"(x, stats) = lsqr(A, b::AbstractVector{T};\n                  M=I, N=I, sqd::Bool=false,\n                  λ::T=zero(T), axtol::T=√eps(T), btol::T=√eps(T),\n                  atol::T=zero(T), rtol::T=zero(T),\n                  etol::T=√eps(T), window::Int=5,\n                  itmax::Int=0, conlim::T=1/√eps(T),\n                  radius::T=zero(T), verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the regularized linear least-squares problem\n\nminimize ‖b - Ax‖₂² + λ²‖x‖₂²\n\nusing the LSQR method, where λ ≥ 0 is a regularization parameter. LSQR is formally equivalent to applying CG to the normal equations\n\n(AᵀA + λ²I) x = Aᵀb\n\n(and therefore to CGLS) but is more stable.\n\nLSQR produces monotonic residuals ‖r‖₂ but not optimality residuals ‖Aᵀr‖₂. It is formally equivalent to CGLS, though can be slightly more accurate.\n\nPreconditioners M and N may be provided in the form of linear operators and are assumed to be symmetric and positive definite. If sqd is set to true, we solve the symmetric and quasi-definite system\n\n[ E    A ] [ r ]   [ b ]\n[ Aᵀ  -F ] [ x ] = [ 0 ],\n\nwhere E and F are symmetric and positive definite. The system above represents the optimality conditions of\n\nminimize ‖b - Ax‖²_E⁻¹ + ‖x‖²_F.\n\nFor a symmetric and positive definite matrix K, the K-norm of a vector x is ‖x‖²_K = xᵀKx. LSQR is then equivalent to applying CG to (AᵀE⁻¹A + F)x = AᵀE⁻¹b with r = E⁻¹(b - Ax). Preconditioners M = E⁻¹ ≻ 0 and N = F⁻¹ ≻ 0 may be provided in the form of linear operators.\n\nIf sqd is set to false (the default), we solve the symmetric and indefinite system\n\n[ E    A ] [ r ]   [ b ]\n[ Aᵀ   0 ] [ x ] = [ 0 ].\n\nThe system above represents the optimality conditions of\n\nminimize ‖b - Ax‖²_E⁻¹.\n\nIn this case, N can still be specified and indicates the weighted norm in which x and Aᵀr should be measured. r can be recovered by computing E⁻¹(b - Ax).\n\nReference\n\nC. C. Paige and M. A. Saunders, LSQR: An Algorithm for Sparse Linear Equations and Sparse Least Squares, ACM Transactions on Mathematical Software, 8(1), pp. 43–71, 1982.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ls/#Krylov.lsmr","page":"Least-squares problems","title":"Krylov.lsmr","text":"(x, stats) = lsmr(A, b::AbstractVector{T};\n                  M=I, N=I, sqd::Bool=false,\n                  λ::T=zero(T), axtol::T=√eps(T), btol::T=√eps(T),\n                  atol::T=zero(T), rtol::T=zero(T),\n                  etol::T=√eps(T), window::Int=5,\n                  itmax::Int=0, conlim::T=1/√eps(T),\n                  radius::T=zero(T), verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the regularized linear least-squares problem\n\nminimize ‖b - Ax‖₂² + λ²‖x‖₂²\n\nusing the LSMR method, where λ ≥ 0 is a regularization parameter. LSMR is formally equivalent to applying MINRES to the normal equations\n\n(AᵀA + λ²I) x = Aᵀb\n\n(and therefore to CRLS) but is more stable.\n\nLSMR produces monotonic residuals ‖r‖₂ and optimality residuals ‖Aᵀr‖₂. It is formally equivalent to CRLS, though can be substantially more accurate.\n\nPreconditioners M and N may be provided in the form of linear operators and are assumed to be symmetric and positive definite. If sqd is set to true, we solve the symmetric and quasi-definite system\n\n[ E    A ] [ r ]   [ b ]\n[ Aᵀ  -F ] [ x ] = [ 0 ],\n\nwhere E and F are symmetric and positive definite. The system above represents the optimality conditions of\n\nminimize ‖b - Ax‖²_E⁻¹ + ‖x‖²_F.\n\nFor a symmetric and positive definite matrix K, the K-norm of a vector x is ‖x‖²_K = xᵀKx. LSMR is then equivalent to applying MINRES to (AᵀE⁻¹A + F)x = AᵀE⁻¹b with r = E⁻¹(b - Ax). Preconditioners M = E⁻¹ ≻ 0 and N = F⁻¹ ≻ 0 may be provided in the form of linear operators.\n\nIf sqd is set to false (the default), we solve the symmetric and indefinite system\n\n[ E    A ] [ r ]   [ b ]\n[ Aᵀ   0 ] [ x ] = [ 0 ].\n\nThe system above represents the optimality conditions of\n\nminimize ‖b - Ax‖²_E⁻¹.\n\nIn this case, N can still be specified and indicates the weighted norm in which x and Aᵀr should be measured. r can be recovered by computing E⁻¹(b - Ax).\n\nReference\n\nD. C.-L. Fong and M. A. Saunders, LSMR: An Iterative Algorithm for Sparse, Least Squares Problems, SIAM Journal on Scientific Computing, 33(5), pp. 2950–2971, 2011.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/#Contents","page":"Reference","title":"Contents","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/#Index","page":"Reference","title":"Index","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [Krylov]","category":"page"},{"location":"reference/#Krylov.bicgstab-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T<:AbstractFloat","page":"Reference","title":"Krylov.bicgstab","text":"(x, stats) = bicgstab(A, b::AbstractVector{T}; c::AbstractVector{T}=b,\n                      M=I, N=I, atol::T=√eps(T), rtol::T=√eps(T),\n                      itmax::Int=0, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the square linear system Ax = b using the BICGSTAB method.\n\nThe Biconjugate Gradient Stabilized method is a variant of BiCG, like CGS, but using different updates for the Aᵀ-sequence in order to obtain smoother convergence than CGS.\n\nIf BICGSTAB stagnates, we recommend DQGMRES and BiLQ as alternative methods for unsymmetric square systems.\n\nBICGSTAB stops when itmax iterations are reached or when ‖rₖ‖ ≤ atol + ‖b‖ * rtol. atol is an absolute tolerance and rtol is a relative tolerance.\n\nAdditional details can be displayed if verbose mode is enabled (verbose > 0). Information will be displayed every verbose iterations.\n\nThis implementation allows a left preconditioner M and a right preconditioner N.\n\nReferences\n\nH. A. van der Vorst, Bi-CGSTAB: A fast and smoothly converging variant of Bi-CG for the solution of nonsymmetric linear systems, SIAM Journal on Scientific and Statistical Computing, 13(2), pp. 631–644, 1992.\nG. L.G. Sleijpen and D. R. Fokkema, BiCGstab(ℓ) for linear equations involving unsymmetric matrices with complex spectrum, Electronic Transactions on Numerical Analysis, 1, pp. 11–32, 1993.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.bilq-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T<:AbstractFloat","page":"Reference","title":"Krylov.bilq","text":"(x, stats) = bilq(A, b::AbstractVector{T}; c::AbstractVector{T}=b,\n                  atol::T=√eps(T), rtol::T=√eps(T), transfer_to_bicg::Bool=true,\n                  itmax::Int=0, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the square linear system Ax = b using the BiLQ method.\n\nBiLQ is based on the Lanczos biorthogonalization process. When A is symmetric and b = c, BiLQ is equivalent to SYMMLQ.\n\nAn option gives the possibility of transferring to the BiCG point, when it exists. The transfer is based on the residual norm.\n\nReference\n\nA. Montoison and D. Orban, BiLQ: An Iterative Method for Nonsymmetric Linear Systems with a Quasi-Minimum Error Property, SIAM Journal on Matrix Analysis and Applications, 41(3), pp. 1145–1166, 2020.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.bilqr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}, AbstractVector{T}}} where T<:AbstractFloat","page":"Reference","title":"Krylov.bilqr","text":"(x, t, stats) = bilqr(A, b::AbstractVector{T}, c::AbstractVector{T};\n                      atol::T=√eps(T), rtol::T=√eps(T), transfer_to_bicg::Bool=true,\n                      itmax::Int=0, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nCombine BiLQ and QMR to solve adjoint systems.\n\n[0  A] [t] = [b]\n[Aᵀ 0] [x]   [c]\n\nBiLQ is used for solving primal system Ax = b. QMR is used for solving dual system Aᵀt = c.\n\nAn option gives the possibility of transferring from the BiLQ point to the BiCG point, when it exists. The transfer is based on the residual norm.\n\nReference\n\nA. Montoison and D. Orban, BiLQ: An Iterative Method for Nonsymmetric Linear Systems with a Quasi-Minimum Error Property, SIAM Journal on Matrix Analysis and Applications, 41(3), pp. 1145–1166, 2020.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.cg-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T<:AbstractFloat","page":"Reference","title":"Krylov.cg","text":"(x, stats) = cg(A, b::AbstractVector{T};\n                M=I, atol::T=√eps(T), rtol::T=√eps(T), restart::Bool=false,\n                itmax::Int=0, radius::T=zero(T), linesearch::Bool=false,\n                verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nThe conjugate gradient method to solve the symmetric linear system Ax=b.\n\nThe method does not abort if A is not definite.\n\nA preconditioner M may be provided in the form of a linear operator and is assumed to be symmetric and positive definite. M also indicates the weighted norm in which residuals are measured.\n\nIf itmax=0, the default number of iterations is set to 2 * n, with n = length(b).\n\nReference\n\nM. R. Hestenes and E. Stiefel, Methods of conjugate gradients for solving linear systems, Journal of Research of the National Bureau of Standards, 49(6), pp. 409–436, 1952.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.cg_lanczos-Union{Tuple{T}, Tuple{Any, AbstractVector{T}, AbstractVector{T}}} where T<:AbstractFloat","page":"Reference","title":"Krylov.cg_lanczos","text":"(x, stats) = cg_lanczos(A, b::AbstractVector{T}, shifts::AbstractVector{T};\n                        M=I, atol::T=√eps(T), rtol::T=√eps(T), itmax::Int=0,\n                        check_curvature::Bool=false, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nThe Lanczos version of the conjugate gradient method to solve a family of shifted systems\n\n(A + αI) x = b  (α = α₁, ..., αₙ)\n\nThe method does not abort if A + αI is not definite.\n\nA preconditioner M may be provided in the form of a linear operator and is assumed to be symmetric and positive definite.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.cg_lanczos-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T<:AbstractFloat","page":"Reference","title":"Krylov.cg_lanczos","text":"(x, stats) = cg_lanczos(A, b::AbstractVector{T};\n                        M=I, atol::T=√eps(T), rtol::T=√eps(T), itmax::Int=0,\n                        check_curvature::Bool=false, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nThe Lanczos version of the conjugate gradient method to solve the symmetric linear system\n\nAx = b\n\nThe method does not abort if A is not definite.\n\nA preconditioner M may be provided in the form of a linear operator and is assumed to be symmetric and positive definite.\n\nReferences\n\nA. Frommer and P. Maass, Fast CG-Based Methods for Tikhonov-Phillips Regularization, SIAM Journal on Scientific Computing, 20(5), pp. 1831–1850, 1999.\nC. C. Paige and M. A. Saunders, Solution of Sparse Indefinite Systems of Linear Equations, SIAM Journal on Numerical Analysis, 12(4), pp. 617–629, 1975.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.cgls-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T<:AbstractFloat","page":"Reference","title":"Krylov.cgls","text":"(x, stats) = cgls(A, b::AbstractVector{T};\n                  M=I, λ::T=zero(T), atol::T=√eps(T), rtol::T=√eps(T),\n                  radius::T=zero(T), itmax::Int=0, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the regularized linear least-squares problem\n\nminimize ‖b - Ax‖₂² + λ‖x‖₂²\n\nusing the Conjugate Gradient (CG) method, where λ ≥ 0 is a regularization parameter. This method is equivalent to applying CG to the normal equations\n\n(AᵀA + λI) x = Aᵀb\n\nbut is more stable.\n\nCGLS produces monotonic residuals ‖r‖₂ but not optimality residuals ‖Aᵀr‖₂. It is formally equivalent to LSQR, though can be slightly less accurate, but simpler to implement.\n\nReferences\n\nM. R. Hestenes and E. Stiefel. Methods of conjugate gradients for solving linear systems, Journal of Research of the National Bureau of Standards, 49(6), pp. 409–436, 1952.\nA. Björck, T. Elfving and Z. Strakos, Stability of Conjugate Gradient and Lanczos Methods for Linear Least Squares Problems, SIAM Journal on Matrix Analysis and Applications, 19(3), pp. 720–736, 1998.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.cgne-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T<:AbstractFloat","page":"Reference","title":"Krylov.cgne","text":"(x, stats) = cgne(A, b::AbstractVector{T};\n                  M=I, λ::T=zero(T), atol::T=√eps(T), rtol::T=√eps(T),\n                  itmax::Int=0, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the consistent linear system\n\nAx + √λs = b\n\nusing the Conjugate Gradient (CG) method, where λ ≥ 0 is a regularization parameter. This method is equivalent to applying CG to the normal equations of the second kind\n\n(AAᵀ + λI) y = b\n\nbut is more stable. When λ = 0, this method solves the minimum-norm problem\n\nmin ‖x‖₂  s.t. Ax = b.\n\nWhen λ > 0, it solves the problem\n\nmin ‖(x,s)‖₂  s.t. Ax + √λs = b.\n\nCGNE produces monotonic errors ‖x-x*‖₂ but not residuals ‖r‖₂. It is formally equivalent to CRAIG, though can be slightly less accurate, but simpler to implement. Only the x-part of the solution is returned.\n\nA preconditioner M may be provided in the form of a linear operator.\n\nReferences\n\nJ. E. Craig, The N-step iteration procedures, Journal of Mathematics and Physics, 34(1), pp. 64–73, 1955.\nJ. E. Craig, Iterations Procedures for Simultaneous Equations, Ph.D. Thesis, Department of Electrical Engineering, MIT, 1954.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.cgs-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T<:AbstractFloat","page":"Reference","title":"Krylov.cgs","text":"(x, stats) = cgs(A, b::AbstractVector{T}; c::AbstractVector{T}=b,\n                 M=I, N=I, atol::T=√eps(T), rtol::T=√eps(T),\n                 itmax::Int=0, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the consistent linear system Ax = b using conjugate gradient squared algorithm.\n\nFrom \"Iterative Methods for Sparse Linear Systems (Y. Saad)\" :\n\n«The method is based on a polynomial variant of the conjugate gradients algorithm. Although related to the so-called bi-conjugate gradients (BCG) algorithm, it does not involve adjoint matrix-vector multiplications, and the expected convergence rate is about twice that of the BCG algorithm.\n\nThe Conjugate Gradient Squared algorithm works quite well in many cases. However, one difficulty is that, since the polynomials are squared, rounding errors tend to be more damaging than in the standard BCG algorithm. In particular, very high variations of the residual vectors often cause the residual norms computed to become inaccurate.\n\nTFQMR and BICGSTAB were developed to remedy this difficulty.»\n\nThis implementation allows a left preconditioner M and a right preconditioner N.\n\nReference\n\nP. Sonneveld, CGS, A Fast Lanczos-Type Solver for Nonsymmetric Linear systems, SIAM Journal on Scientific and Statistical Computing, 10(1), pp. 36–52, 1989.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.cr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T<:AbstractFloat","page":"Reference","title":"Krylov.cr","text":"(x, stats) = cr(A, b::AbstractVector{T};\n                M=I, atol::T=√eps(T), rtol::T=√eps(T), γ::T=√eps(T), itmax::Int=0,\n                radius::T=zero(T), verbose::Int=0, linesearch::Bool=false, history::Bool=false) where T <: AbstractFloat\n\nA truncated version of Stiefel’s Conjugate Residual method to solve the symmetric linear system Ax = b or the least-squares problem min ‖b - Ax‖. The matrix A must be positive semi-definite.\n\nA preconditioner M may be provided in the form of a linear operator and is assumed to be symmetric and positive definite. M also indicates the weighted norm in which residuals are measured.\n\nIn a linesearch context, 'linesearch' must be set to 'true'.\n\nIf itmax=0, the default number of iterations is set to 2 * n, with n = length(b).\n\nReferences\n\nM. R. Hestenes and E. Stiefel, Methods of conjugate gradients for solving linear systems, Journal of Research of the National Bureau of Standards, 49(6), pp. 409–436, 1952.\nM-A. Dahito and D. Orban, The Conjugate Residual Method in Linesearch and Trust-Region Methods, SIAM Journal on Optimization, 29(3), pp. 1988–2025, 2019.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.craig-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T<:AbstractFloat","page":"Reference","title":"Krylov.craig","text":"(x, y, stats) = craig(A, b::AbstractVector{T};\n                      M=I, N=I, sqd::Bool=false, λ::T=zero(T), atol::T=√eps(T),\n                      btol::T=√eps(T), rtol::T=√eps(T), conlim::T=1/√eps(T), itmax::Int=0,\n                      verbose::Int=0, transfer_to_lsqr::Bool=false, history::Bool=false) where T <: AbstractFloat\n\nFind the least-norm solution of the consistent linear system\n\nAx + λs = b\n\nusing the Golub-Kahan implementation of Craig's method, where λ ≥ 0 is a regularization parameter. This method is equivalent to CGNE but is more stable.\n\nFor a system in the form Ax = b, Craig's method is equivalent to applying CG to AAᵀy = b and recovering x = Aᵀy. Note that y are the Lagrange multipliers of the least-norm problem\n\nminimize ‖x‖  s.t.  Ax = b.\n\nPreconditioners M⁻¹ and N⁻¹ may be provided in the form of linear operators and are assumed to be symmetric and positive definite. If sqd = true, CRAIG solves the symmetric and quasi-definite system\n\n[ -N   Aᵀ ] [ x ]   [ 0 ]\n[  A   M  ] [ y ] = [ b ],\n\nwhich is equivalent to applying CG to (AN⁻¹Aᵀ + M)y = b with Nx = Aᵀy.\n\nIf sqd = false, CRAIG solves the symmetric and indefinite system\n\n[ -N   Aᵀ ] [ x ]   [ 0 ]\n[  A   0  ] [ y ] = [ b ].\n\nIn this case, M⁻¹ can still be specified and indicates the weighted norm in which residuals are measured.\n\nIn this implementation, both the x and y-parts of the solution are returned.\n\nReferences\n\nC. C. Paige and M. A. Saunders, LSQR: An Algorithm for Sparse Linear Equations and Sparse Least Squares, ACM Transactions on Mathematical Software, 8(1), pp. 43–71, 1982.\nM. A. Saunders, Solutions of Sparse Rectangular Systems Using LSQR and CRAIG, BIT Numerical Mathematics, 35(4), pp. 588–604, 1995.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.craigmr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T<:AbstractFloat","page":"Reference","title":"Krylov.craigmr","text":"(x, y, stats) = craigmr(A, b::AbstractVector{T};\n                        M=I, N=I, λ::T=zero(T), atol::T=√eps(T),\n                        rtol::T=√eps(T), itmax::Int=0, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the consistent linear system\n\nAx + √λs = b\n\nusing the CRAIG-MR method, where λ ≥ 0 is a regularization parameter. This method is equivalent to applying the Conjugate Residuals method to the normal equations of the second kind\n\n(AAᵀ + λI) y = b\n\nbut is more stable. When λ = 0, this method solves the minimum-norm problem\n\nmin ‖x‖₂  s.t.  x ∈ argmin ‖Ax - b‖₂.\n\nWhen λ > 0, this method solves the problem\n\nmin ‖(x,s)‖₂  s.t. Ax + √λs = b.\n\nPreconditioners M⁻¹ and N⁻¹ may be provided in the form of linear operators and are assumed to be symmetric and positive definite. Afterward CRAIGMR solves the symmetric and quasi-definite system\n\n[ -N   Aᵀ ] [ x ]   [ 0 ]\n[  A   M  ] [ y ] = [ b ],\n\nwhich is equivalent to applying MINRES to (M + AN⁻¹Aᵀ)y = b.\n\nCRAIGMR produces monotonic residuals ‖r‖₂. It is formally equivalent to CRMR, though can be slightly more accurate, and intricate to implement. Both the x- and y-parts of the solution are returned.\n\nReferences\n\nD. Orban and M. Arioli. Iterative Solution of Symmetric Quasi-Definite Linear Systems, Volume 3 of Spotlights. SIAM, Philadelphia, PA, 2017.\nD. Orban, The Projected Golub-Kahan Process for Constrained, Linear Least-Squares Problems. Cahier du GERAD G-2014-15, 2014.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.crls-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T<:AbstractFloat","page":"Reference","title":"Krylov.crls","text":"(x, stats) = crls(A, b::AbstractVector{T};\n                  M=I, λ::T=zero(T), atol::T=√eps(T), rtol::T=√eps(T),\n                  radius::T=zero(T), itmax::Int=0, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the linear least-squares problem\n\nminimize ‖b - Ax‖₂² + λ‖x‖₂²\n\nusing the Conjugate Residuals (CR) method. This method is equivalent to applying MINRES to the normal equations\n\n(AᵀA + λI) x = Aᵀb.\n\nThis implementation recurs the residual r := b - Ax.\n\nCRLS produces monotonic residuals ‖r‖₂ and optimality residuals ‖Aᵀr‖₂. It is formally equivalent to LSMR, though can be substantially less accurate, but simpler to implement.\n\nReference\n\nD. C.-L. Fong, Minimum-Residual Methods for Sparse, Least-Squares using Golubg-Kahan Bidiagonalization, Ph.D. Thesis, Stanford University, 2011.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.crmr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T<:AbstractFloat","page":"Reference","title":"Krylov.crmr","text":"(x, stats) = crmr(A, b::AbstractVector{T};\n                  M=I, λ::T=zero(T), atol::T=√eps(T),\n                  rtol::T=√eps(T), itmax::Int=0, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the consistent linear system\n\nAx + √λs = b\n\nusing the Conjugate Residual (CR) method, where λ ≥ 0 is a regularization parameter. This method is equivalent to applying CR to the normal equations of the second kind\n\n(AAᵀ + λI) y = b\n\nbut is more stable. When λ = 0, this method solves the minimum-norm problem\n\nmin ‖x‖₂  s.t.  x ∈ argmin ‖Ax - b‖₂.\n\nWhen λ > 0, this method solves the problem\n\nmin ‖(x,s)‖₂  s.t. Ax + √λs = b.\n\nCGMR produces monotonic residuals ‖r‖₂. It is formally equivalent to CRAIG-MR, though can be slightly less accurate, but simpler to implement. Only the x-part of the solution is returned.\n\nA preconditioner M may be provided in the form of a linear operator.\n\nReferences\n\nD. Orban and M. Arioli, Iterative Solution of Symmetric Quasi-Definite Linear Systems, Volume 3 of Spotlights. SIAM, Philadelphia, PA, 2017.\nD. Orban, The Projected Golub-Kahan Process for Constrained Linear Least-Squares Problems. Cahier du GERAD G-2014-15, 2014.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.diom-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T<:AbstractFloat","page":"Reference","title":"Krylov.diom","text":"(x, stats) = diom(A, b::AbstractVector{T};\n                  M=I, N=I, atol::T=√eps(T), rtol::T=√eps(T), itmax::Int=0,\n                  memory::Int=20, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the consistent linear system Ax = b using direct incomplete orthogonalization method.\n\nDIOM is similar to CG with partial reorthogonalization.\n\nAn advantage of DIOM is that nonsymmetric or symmetric indefinite or both nonsymmetric and indefinite systems of linear equations can be handled by this single algorithm.\n\nThis implementation allows a left preconditioner M and a right preconditioner N.\n\nLeft  preconditioning : M⁻¹Ax = M⁻¹b\nRight preconditioning : AN⁻¹u = b with x = N⁻¹u\nSplit preconditioning : M⁻¹AN⁻¹u = M⁻¹b with x = N⁻¹u\n\nReference\n\nY. Saad, Practical use of some krylov subspace methods for solving indefinite and nonsymmetric linear systems, SIAM journal on scientific and statistical computing, 5(1), pp. 203–228, 1984.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.dqgmres-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T<:AbstractFloat","page":"Reference","title":"Krylov.dqgmres","text":"(x, stats) = dqgmres(A, b::AbstractVector{T};\n                     M=I, N=I, atol::T=√eps(T), rtol::T=√eps(T),\n                     itmax::Int=0, memory::Int=20, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the consistent linear system Ax = b using DQGMRES method.\n\nDQGMRES algorithm is based on the incomplete Arnoldi orthogonalization process and computes a sequence of approximate solutions with the quasi-minimal residual property.\n\nThis implementation allows a left preconditioner M and a right preconditioner N.\n\nLeft  preconditioning : M⁻¹Ax = M⁻¹b\nRight preconditioning : AN⁻¹u = b with x = N⁻¹u\nSplit preconditioning : M⁻¹AN⁻¹u = M⁻¹b with x = N⁻¹u\n\nReference\n\nY. Saad and K. Wu, DQGMRES: a quasi minimal residual algorithm based on incomplete orthogonalization, Numerical Linear Algebra with Applications, Vol. 3(4), pp. 329–343, 1996.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.fom-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T<:AbstractFloat","page":"Reference","title":"Krylov.fom","text":"(x, stats) = fom(A, b::AbstractVector{T};\n                 M=I, N=I, atol::T=√eps(T), rtol::T=√eps(T),\n                 reorthogonalization::Bool=false, itmax::Int=0,\n                 memory::Int=20, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the linear system Ax = b using FOM method.\n\nFOM algorithm is based on the Arnoldi process and a Galerkin condition.\n\nThis implementation allows a left preconditioner M and a right preconditioner N.\n\nLeft  preconditioning : M⁻¹Ax = M⁻¹b\nRight preconditioning : AN⁻¹u = b with x = N⁻¹u\nSplit preconditioning : M⁻¹AN⁻¹u = M⁻¹b with x = N⁻¹u\n\nReference\n\nY. Saad, Krylov subspace methods for solving unsymmetric linear systems, Mathematics of computation, Vol. 37(155), pp. 105–126, 1981.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.gmres-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T<:AbstractFloat","page":"Reference","title":"Krylov.gmres","text":"(x, stats) = gmres(A, b::AbstractVector{T};\n                   M=I, N=I, atol::T=√eps(T), rtol::T=√eps(T),\n                   reorthogonalization::Bool=false, itmax::Int=0,\n                   memory::Int=20, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the linear system Ax = b using GMRES method.\n\nGMRES algorithm is based on the Arnoldi process and computes a sequence of approximate solutions with the minimal residual property.\n\nThis implementation allows a left preconditioner M and a right preconditioner N.\n\nLeft  preconditioning : M⁻¹Ax = M⁻¹b\nRight preconditioning : AN⁻¹u = b with x = N⁻¹u\nSplit preconditioning : M⁻¹AN⁻¹u = M⁻¹b with x = N⁻¹u\n\nReference\n\nY. Saad and M. H. Schultz, GMRES: A Generalized Minimal Residual Algorithm for Solving Nonsymmetric Linear Systems, SIAM Journal on Scientific and Statistical Computing, Vol. 7(3), pp. 856–869, 1986.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.kones-Tuple{Any, Any}","page":"Reference","title":"Krylov.kones","text":"v = kones(S, n)\n\nCreate an AbstractVector of storage type S of length n only composed of one.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.ktypeof-Tuple{AbstractVector{T} where T}","page":"Reference","title":"Krylov.ktypeof","text":"S = ktypeof(v)\n\nReturn a dense storage type S based on the type of v.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.kzeros-Tuple{Any, Any}","page":"Reference","title":"Krylov.kzeros","text":"v = kzeros(S, n)\n\nCreate an AbstractVector of storage type S of length n only composed of zero.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.lnlq-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T<:AbstractFloat","page":"Reference","title":"Krylov.lnlq","text":"(x, y, stats) = lnlq(A, b::AbstractVector{T};\n                     M=I, N=I, sqd::Bool=false, λ::T=zero(T), σ::T=zero(T),\n                     atol::T=√eps(T), rtol::T=√eps(T), etolx::T=√eps(T), etoly::T=√eps(T), itmax::Int=0,\n                     transfer_to_craig::Bool=true, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nFind the least-norm solution of the consistent linear system\n\nAx + λs = b\n\nusing the LNLQ method, where λ ≥ 0 is a regularization parameter.\n\nFor a system in the form Ax = b, LNLQ method is equivalent to applying SYMMLQ to AAᵀy = b and recovering x = Aᵀy but is more stable. Note that y are the Lagrange multipliers of the least-norm problem\n\nminimize ‖x‖  s.t.  Ax = b.\n\nIf sqd = true, LNLQ solves the symmetric and quasi-definite system\n\n[ -F   Aᵀ ] [ x ]   [ 0 ]\n[  A   E  ] [ y ] = [ b ],\n\nwhere E and F are symmetric and positive definite. LNLQ is then equivalent to applying SYMMLQ to (AF⁻¹Aᵀ + E)y = b with Fx = Aᵀy. Preconditioners M = E⁻¹ ≻ 0 and N = F⁻¹ ≻ 0 may be provided in the form of linear operators.\n\nIf sqd = false, LNLQ solves the symmetric and indefinite system\n\n[ -F   Aᵀ ] [ x ]   [ 0 ]\n[  A   0  ] [ y ] = [ b ].\n\nIn this case, M can still be specified and indicates the weighted norm in which residuals are measured.\n\nIn this implementation, both the x and y-parts of the solution are returned.\n\netolx and etoly are tolerances on the upper bound of the distance to the solution ‖x-xₛ‖ and ‖y-yₛ‖, respectively. The bound is valid if λ>0 or σ>0 where σ should be strictly smaller than the smallest positive singular value. For instance σ:=(1-1e-7)σₘᵢₙ .\n\nReference\n\nR. Estrin, D. Orban, M.A. Saunders, LNLQ: An Iterative Method for Least-Norm Problems with an Error Minimization Property, SIAM Journal on Matrix Analysis and Applications, 40(3), pp. 1102–1124, 2019.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.lslq-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T<:AbstractFloat","page":"Reference","title":"Krylov.lslq","text":"(x, stats) = lslq(A, b::AbstractVector{T};\n                  M=I, N=I, sqd::Bool=false, λ::T=zero(T),\n                  atol::T=√eps(T), btol::T=√eps(T), etol::T=√eps(T),\n                  window::Int=5, utol::T=√eps(T), itmax::Int=0,\n                  σ::T=zero(T), transfer_to_lsqr::Bool=false, \n                  conlim::T=1/√eps(T), verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the regularized linear least-squares problem\n\nminimize ‖b - Ax‖₂² + λ²‖x‖₂²\n\nusing the LSLQ method, where λ ≥ 0 is a regularization parameter. LSLQ is formally equivalent to applying SYMMLQ to the normal equations\n\n(AᵀA + λ²I) x = Aᵀb\n\nbut is more stable.\n\nMain features\n\nthe solution estimate is updated along orthogonal directions\nthe norm of the solution estimate ‖xᴸₖ‖₂ is increasing\nthe error ‖eₖ‖₂ := ‖xᴸₖ - x*‖₂ is decreasing\nit is possible to transition cheaply from the LSLQ iterate to the LSQR iterate if there is an advantage (there always is in terms of error)\nif A is rank deficient, identify the minimum least-squares solution\n\nOptional arguments\n\nM: a symmetric and positive definite dual preconditioner\nN: a symmetric and positive definite primal preconditioner\nsqd indicates whether or not we are solving a symmetric and quasi-definite augmented system\n\nIf sqd = true, we solve the symmetric and quasi-definite system\n\n[ E    A ] [ r ]   [ b ]\n[ Aᵀ  -F ] [ x ] = [ 0 ],\n\nwhere E and F are symmetric and positive definite. The system above represents the optimality conditions of\n\nminimize ‖b - Ax‖²_E⁻¹ + ‖x‖²_F.\n\nFor a symmetric and positive definite matrix K, the K-norm of a vector x is ‖x‖²_K = xᵀKx. LSLQ is then equivalent to applying SYMMLQ to (AᵀE⁻¹A + F)x = AᵀE⁻¹b with r = E⁻¹(b - Ax). Preconditioners M = E⁻¹ ≻ 0 and N = F⁻¹ ≻ 0 may be provided in the form of linear operators.\n\nIf sqd is set to false (the default), we solve the symmetric and indefinite system\n\n[ E    A ] [ r ]   [ b ]\n[ Aᵀ   0 ] [ x ] = [ 0 ].\n\nThe system above represents the optimality conditions of\n\nminimize ‖b - Ax‖²_E⁻¹.\n\nIn this case, N can still be specified and indicates the weighted norm in which x and Aᵀr should be measured. r can be recovered by computing E⁻¹(b - Ax).\n\nλ is a regularization parameter (see the problem statement above)\nσ is an underestimate of the smallest nonzero singular value of A–-setting σ too large will result in an error in the course of the iterations\natol is a stopping tolerance based on the residual\nbtol is a stopping tolerance used to detect zero-residual problems\netol is a stopping tolerance based on the lower bound on the error\nwindow is the number of iterations used to accumulate a lower bound on the error\nutol is a stopping tolerance based on the upper bound on the error\ntransfer_to_lsqr return the CG solution estimate (i.e., the LSQR point) instead of the LQ estimate\nitmax is the maximum number of iterations (0 means no imposed limit)\nconlim is the limit on the estimated condition number of A beyond which the solution will be abandoned\nverbose determines verbosity.\n\nReturn values\n\nlslq returns the tuple (x, stats) where\n\nx is the LQ solution estimate\nstats collects other statistics on the run in a LSLQStats\nstats.err_lbnds is a vector of lower bounds on the LQ error–-the vector is empty if window is set to zero\nstats.err_ubnds_lq is a vector of upper bounds on the LQ error–-the vector is empty if σ == 0 is left at zero\nstats.err_ubnds_cg is a vector of upper bounds on the CG error–-the vector is empty if σ == 0 is left at zero\nstats.error_with_bnd is a boolean indicating whether there was an error in the upper bounds computation (cancellation errors, too large σ ...)\n\nStopping conditions\n\nThe iterations stop as soon as one of the following conditions holds true:\n\nthe optimality residual is sufficiently small (stats.status = \"found approximate minimum least-squares solution\") in the sense that either\n‖Aᵀr‖ / (‖A‖ ‖r‖) ≤ atol, or\n1 + ‖Aᵀr‖ / (‖A‖ ‖r‖) ≤ 1\nan approximate zero-residual solution has been found (stats.status = \"found approximate zero-residual solution\") in the sense that either\n‖r‖ / ‖b‖ ≤ btol + atol ‖A‖ * ‖xᴸ‖ / ‖b‖, or\n1 + ‖r‖ / ‖b‖ ≤ 1\nthe estimated condition number of A is too large in the sense that either\n1/cond(A) ≤ 1/conlim (stats.status = \"condition number exceeds tolerance\"), or\n1 + 1/cond(A) ≤ 1 (stats.status = \"condition number seems too large for this machine\")\nthe lower bound on the LQ forward error is less than etol * ‖xᴸ‖\nthe upper bound on the CG forward error is less than utol * ‖xᶜ‖\n\nReferences\n\nR. Estrin, D. Orban and M. A. Saunders, Euclidean-norm error bounds for SYMMLQ and CG, SIAM Journal on Matrix Analysis and Applications, 40(1), pp. 235–253, 2019.\nR. Estrin, D. Orban and M. A. Saunders, LSLQ: An Iterative Method for Linear Least-Squares with an Error Minimization Property, SIAM Journal on Matrix Analysis and Applications, 40(1), pp. 254–275, 2019.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.lsmr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T<:AbstractFloat","page":"Reference","title":"Krylov.lsmr","text":"(x, stats) = lsmr(A, b::AbstractVector{T};\n                  M=I, N=I, sqd::Bool=false,\n                  λ::T=zero(T), axtol::T=√eps(T), btol::T=√eps(T),\n                  atol::T=zero(T), rtol::T=zero(T),\n                  etol::T=√eps(T), window::Int=5,\n                  itmax::Int=0, conlim::T=1/√eps(T),\n                  radius::T=zero(T), verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the regularized linear least-squares problem\n\nminimize ‖b - Ax‖₂² + λ²‖x‖₂²\n\nusing the LSMR method, where λ ≥ 0 is a regularization parameter. LSMR is formally equivalent to applying MINRES to the normal equations\n\n(AᵀA + λ²I) x = Aᵀb\n\n(and therefore to CRLS) but is more stable.\n\nLSMR produces monotonic residuals ‖r‖₂ and optimality residuals ‖Aᵀr‖₂. It is formally equivalent to CRLS, though can be substantially more accurate.\n\nPreconditioners M and N may be provided in the form of linear operators and are assumed to be symmetric and positive definite. If sqd is set to true, we solve the symmetric and quasi-definite system\n\n[ E    A ] [ r ]   [ b ]\n[ Aᵀ  -F ] [ x ] = [ 0 ],\n\nwhere E and F are symmetric and positive definite. The system above represents the optimality conditions of\n\nminimize ‖b - Ax‖²_E⁻¹ + ‖x‖²_F.\n\nFor a symmetric and positive definite matrix K, the K-norm of a vector x is ‖x‖²_K = xᵀKx. LSMR is then equivalent to applying MINRES to (AᵀE⁻¹A + F)x = AᵀE⁻¹b with r = E⁻¹(b - Ax). Preconditioners M = E⁻¹ ≻ 0 and N = F⁻¹ ≻ 0 may be provided in the form of linear operators.\n\nIf sqd is set to false (the default), we solve the symmetric and indefinite system\n\n[ E    A ] [ r ]   [ b ]\n[ Aᵀ   0 ] [ x ] = [ 0 ].\n\nThe system above represents the optimality conditions of\n\nminimize ‖b - Ax‖²_E⁻¹.\n\nIn this case, N can still be specified and indicates the weighted norm in which x and Aᵀr should be measured. r can be recovered by computing E⁻¹(b - Ax).\n\nReference\n\nD. C.-L. Fong and M. A. Saunders, LSMR: An Iterative Algorithm for Sparse, Least Squares Problems, SIAM Journal on Scientific Computing, 33(5), pp. 2950–2971, 2011.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.lsqr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T<:AbstractFloat","page":"Reference","title":"Krylov.lsqr","text":"(x, stats) = lsqr(A, b::AbstractVector{T};\n                  M=I, N=I, sqd::Bool=false,\n                  λ::T=zero(T), axtol::T=√eps(T), btol::T=√eps(T),\n                  atol::T=zero(T), rtol::T=zero(T),\n                  etol::T=√eps(T), window::Int=5,\n                  itmax::Int=0, conlim::T=1/√eps(T),\n                  radius::T=zero(T), verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the regularized linear least-squares problem\n\nminimize ‖b - Ax‖₂² + λ²‖x‖₂²\n\nusing the LSQR method, where λ ≥ 0 is a regularization parameter. LSQR is formally equivalent to applying CG to the normal equations\n\n(AᵀA + λ²I) x = Aᵀb\n\n(and therefore to CGLS) but is more stable.\n\nLSQR produces monotonic residuals ‖r‖₂ but not optimality residuals ‖Aᵀr‖₂. It is formally equivalent to CGLS, though can be slightly more accurate.\n\nPreconditioners M and N may be provided in the form of linear operators and are assumed to be symmetric and positive definite. If sqd is set to true, we solve the symmetric and quasi-definite system\n\n[ E    A ] [ r ]   [ b ]\n[ Aᵀ  -F ] [ x ] = [ 0 ],\n\nwhere E and F are symmetric and positive definite. The system above represents the optimality conditions of\n\nminimize ‖b - Ax‖²_E⁻¹ + ‖x‖²_F.\n\nFor a symmetric and positive definite matrix K, the K-norm of a vector x is ‖x‖²_K = xᵀKx. LSQR is then equivalent to applying CG to (AᵀE⁻¹A + F)x = AᵀE⁻¹b with r = E⁻¹(b - Ax). Preconditioners M = E⁻¹ ≻ 0 and N = F⁻¹ ≻ 0 may be provided in the form of linear operators.\n\nIf sqd is set to false (the default), we solve the symmetric and indefinite system\n\n[ E    A ] [ r ]   [ b ]\n[ Aᵀ   0 ] [ x ] = [ 0 ].\n\nThe system above represents the optimality conditions of\n\nminimize ‖b - Ax‖²_E⁻¹.\n\nIn this case, N can still be specified and indicates the weighted norm in which x and Aᵀr should be measured. r can be recovered by computing E⁻¹(b - Ax).\n\nReference\n\nC. C. Paige and M. A. Saunders, LSQR: An Algorithm for Sparse Linear Equations and Sparse Least Squares, ACM Transactions on Mathematical Software, 8(1), pp. 43–71, 1982.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.minres-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T<:AbstractFloat","page":"Reference","title":"Krylov.minres","text":"(x, stats) = minres(A, b::AbstractVector{T};\n                    M=I, λ::T=zero(T), atol::T=√eps(T)/100,\n                    rtol::T=√eps(T)/100, ratol :: T=zero(T), \n                    rrtol :: T=zero(T), etol::T=√eps(T),\n                    window::Int=5, itmax::Int=0, conlim::T=1/√eps(T),\n                    verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the shifted linear least-squares problem\n\nminimize ‖b - (A + λI)x‖₂²\n\nor the shifted linear system\n\n(A + λI) x = b\n\nusing the MINRES method, where λ ≥ 0 is a shift parameter, where A is square and symmetric.\n\nMINRES is formally equivalent to applying CR to Ax=b when A is positive definite, but is typically more stable and also applies to the case where A is indefinite.\n\nMINRES produces monotonic residuals ‖r‖₂ and optimality residuals ‖Aᵀr‖₂.\n\nA preconditioner M may be provided in the form of a linear operator and is assumed to be symmetric and positive definite.\n\nReference\n\nC. C. Paige and M. A. Saunders, Solution of Sparse Indefinite Systems of Linear Equations, SIAM Journal on Numerical Analysis, 12(4), pp. 617–629, 1975.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.minres_qlp-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T<:AbstractFloat","page":"Reference","title":"Krylov.minres_qlp","text":"(x, stats) = minres_qlp(A, b::AbstractVector{T};\n                        M=I, atol::T=√eps(T), rtol::T=√eps(T), λ::T=zero(T),\n                        itmax::Int=0, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nMINRES-QLP is the only method based on the Lanczos process that returns the minimum-norm solution on singular inconsistent systems (A + λI)x = b, where λ is a shift parameter. It is significantly more complex but can be more reliable than MINRES when A is ill-conditioned.\n\nA preconditioner M may be provided in the form of a linear operator and is assumed to be symmetric and positive definite. M also indicates the weighted norm in which residuals are measured.\n\nReferences\n\nS.-C. T. Choi, Iterative methods for singular linear equations and least-squares problems, Ph.D. thesis, ICME, Stanford University, 2006.\nS.-C. T. Choi, C. C. Paige and M. A. Saunders, MINRES-QLP: A Krylov subspace method for indefinite or singular symmetric systems, SIAM Journal on Scientific Computing, Vol. 33(4), pp. 1810–1836, 2011.\nS.-C. T. Choi and M. A. Saunders, Algorithm 937: MINRES-QLP for symmetric and Hermitian linear equations and least-squares problems, ACM Transactions on Mathematical Software, 40(2), pp. 1–12, 2014.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.qmr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T<:AbstractFloat","page":"Reference","title":"Krylov.qmr","text":"(x, stats) = qmr(A, b::AbstractVector{T}; c::AbstractVector{T}=b,\n                 atol::T=√eps(T), rtol::T=√eps(T),\n                 itmax::Int=0, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the square linear system Ax = b using the QMR method.\n\nQMR is based on the Lanczos biorthogonalization process. When A is symmetric and b = c, QMR is equivalent to MINRES.\n\nReferences\n\nR. W. Freund and N. M. Nachtigal, QMR : a quasi-minimal residual method for non-Hermitian linear systems, Numerische mathematik, Vol. 60(1), pp. 315–339, 1991.\nR. W. Freund and N. M. Nachtigal, An implementation of the QMR method based on coupled two-term recurrences, SIAM Journal on Scientific Computing, Vol. 15(2), pp. 313–337, 1994.\nA. Montoison and D. Orban, BiLQ: An Iterative Method for Nonsymmetric Linear Systems with a Quasi-Minimum Error Property, SIAM Journal on Matrix Analysis and Applications, 41(3), pp. 1145–1166, 2020.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.roots_quadratic-Union{Tuple{T}, Tuple{T, T, T}} where T<:AbstractFloat","page":"Reference","title":"Krylov.roots_quadratic","text":"roots = roots_quadratic(q₂, q₁, q₀; nitref)\n\nFind the real roots of the quadratic\n\nq(x) = q₂ x² + q₁ x + q₀,\n\nwhere q₂, q₁ and q₀ are real. Care is taken to avoid numerical cancellation. Optionally, nitref steps of iterative refinement may be performed to improve accuracy. By default, nitref=1.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.sym_givens-Union{Tuple{T}, Tuple{Complex{T}, Complex{T}}} where T<:AbstractFloat","page":"Reference","title":"Krylov.sym_givens","text":"Numerically stable symmetric Givens reflection. Given a and b complexes, return (c, s, ρ) with c real and (s, ρ) complexes such that\n\n[ c   s ] [ a ] = [ ρ ]\n[ s̅  -c ] [ b ] = [ 0 ].\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.sym_givens-Union{Tuple{T}, Tuple{T, T}} where T<:AbstractFloat","page":"Reference","title":"Krylov.sym_givens","text":"(c, s, ρ) = sym_givens(a, b)\n\nNumerically stable symmetric Givens reflection. Given a and b reals, return (c, s, ρ) such that\n\n[ c  s ] [ a ] = [ ρ ]\n[ s -c ] [ b ] = [ 0 ].\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.symmlq-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T<:AbstractFloat","page":"Reference","title":"Krylov.symmlq","text":"(x, stats) = symmlq(A, b::AbstractVector{T};\n                    M=I, λ::T=zero(T), transfer_to_cg::Bool=true,\n                    λest::T=zero(T), atol::T=√eps(T), rtol::T=√eps(T),\n                    etol::T=√eps(T), window::Int=0, itmax::Int=0,\n                    conlim::T=1/√eps(T), verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the shifted linear system\n\n(A + λI) x = b\n\nusing the SYMMLQ method, where λ is a shift parameter, and A is square and symmetric.\n\nSYMMLQ produces monotonic errors ‖x*-x‖₂.\n\nA preconditioner M may be provided in the form of a linear operator and is assumed to be symmetric and positive definite.\n\nReference\n\nC. C. Paige and M. A. Saunders, Solution of Sparse Indefinite Systems of Linear Equations, SIAM Journal on Numerical Analysis, 12(4), pp. 617–629, 1975.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.to_boundary-Union{Tuple{T}, Tuple{Vector{T}, Vector{T}, T}} where T<:Number","page":"Reference","title":"Krylov.to_boundary","text":"roots = to_boundary(x, d, radius; flip, xNorm2, dNorm2)\n\nGiven a trust-region radius radius, a vector x lying inside the trust-region and a direction d, return σ1 and σ2 such that\n\n‖x + σi d‖ = radius, i = 1, 2\n\nin the Euclidean norm. If known, ‖x‖² may be supplied in xNorm2.\n\nIf flip is set to true, σ1 and σ2 are computed such that\n\n‖x - σi d‖ = radius, i = 1, 2.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.tricg-Union{Tuple{T}, Tuple{Any, AbstractVector{T}, AbstractVector{T}}} where T<:AbstractFloat","page":"Reference","title":"Krylov.tricg","text":"(x, y, stats) = tricg(A, b::AbstractVector{T}, c::AbstractVector{T};\n                      M=I, N=I, atol::T=√eps(T), rtol::T=√eps(T),\n                      spd::Bool=false, snd::Bool=false, flip::Bool=false,\n                      τ::T=one(T), ν::T=-one(T), itmax::Int=0, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nTriCG solves the symmetric linear system\n\n[ τE    A ] [ x ] = [ b ]\n[  Aᵀ  νF ] [ y ]   [ c ],\n\nwhere τ and ν are real numbers, E = M⁻¹ ≻ 0 and F = N⁻¹ ≻ 0. TriCG could breakdown if τ = 0 or ν = 0. It's recommended to use TriMR in these cases.\n\nBy default, TriCG solves symmetric and quasi-definite linear systems with τ = 1 and ν = -1. If flip = true, TriCG solves another known variant of SQD systems where τ = -1 and ν = 1. If spd = true, τ = ν = 1 and the associated symmetric and positive definite linear system is solved. If snd = true, τ = ν = -1 and the associated symmetric and negative definite linear system is solved. τ and ν are also keyword arguments that can be directly modified for more specific problems.\n\nTriCG is based on the preconditioned orthogonal tridiagonalization process and its relation with the preconditioned block-Lanczos process.\n\n[ M   0 ]\n[ 0   N ]\n\nindicates the weighted norm in which residuals are measured. It's the Euclidean norm when M and N are identity operators.\n\nTriCG stops when itmax iterations are reached or when ‖rₖ‖ ≤ atol + ‖r₀‖ * rtol. atol is an absolute tolerance and rtol is a relative tolerance.\n\nAdditional details can be displayed if verbose mode is enabled (verbose > 0). Information will be displayed every verbose iterations.\n\nReference\n\nA. Montoison and D. Orban, TriCG and TriMR: Two Iterative Methods for Symmetric Quasi-Definite Systems, SIAM Journal on Scientific Computing, 43(4), pp. 2502–2525, 2021.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.trilqr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}, AbstractVector{T}}} where T<:AbstractFloat","page":"Reference","title":"Krylov.trilqr","text":"(x, t, stats) = trilqr(A, b::AbstractVector{T}, c::AbstractVector{T};\n                       atol::T=√eps(T), rtol::T=√eps(T), transfer_to_usymcg::Bool=true,\n                       itmax::Int=0, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nCombine USYMLQ and USYMQR to solve adjoint systems.\n\n[0  A] [t] = [b]\n[Aᵀ 0] [x]   [c]\n\nUSYMLQ is used for solving primal system Ax = b. USYMQR is used for solving dual system Aᵀt = c.\n\nAn option gives the possibility of transferring from the USYMLQ point to the USYMCG point, when it exists. The transfer is based on the residual norm.\n\nReference\n\nA. Montoison and D. Orban, BiLQ: An Iterative Method for Nonsymmetric Linear Systems with a Quasi-Minimum Error Property, SIAM Journal on Matrix Analysis and Applications, 41(3), pp. 1145–1166, 2020.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.trimr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}, AbstractVector{T}}} where T<:AbstractFloat","page":"Reference","title":"Krylov.trimr","text":"(x, y, stats) = trimr(A, b::AbstractVector{T}, c::AbstractVector{T};\n                      M=I, N=I, atol::T=√eps(T), rtol::T=√eps(T),\n                      spd::Bool=false, snd::Bool=false, flip::Bool=false, sp::Bool=false,\n                      τ::T=one(T), ν::T=-one(T), itmax::Int=0, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nTriMR solves the symmetric linear system\n\n[ τE    A ] [ x ] = [ b ]\n[  Aᵀ  νF ] [ y ]   [ c ],\n\nwhere τ and ν are real numbers, E = M⁻¹ ≻ 0 and F = N⁻¹ ≻ 0. TriMR handles saddle-point systems (τ = 0 or ν = 0) and adjoint systems (τ = 0 and ν = 0) without any risk of breakdown.\n\nBy default, TriMR solves symmetric and quasi-definite linear systems with τ = 1 and ν = -1. If flip = true, TriMR solves another known variant of SQD systems where τ = -1 and ν = 1. If spd = true, τ = ν = 1 and the associated symmetric and positive definite linear system is solved. If snd = true, τ = ν = -1 and the associated symmetric and negative definite linear system is solved. If sp = true, τ = 1, ν = 0 and the associated saddle-point linear system is solved. τ and ν are also keyword arguments that can be directly modified for more specific problems.\n\nTriMR is based on the preconditioned orthogonal tridiagonalization process and its relation with the preconditioned block-Lanczos process.\n\n[ M   0 ]\n[ 0   N ]\n\nindicates the weighted norm in which residuals are measured. It's the Euclidean norm when M and N are identity operators.\n\nTriMR stops when itmax iterations are reached or when ‖rₖ‖ ≤ atol + ‖r₀‖ * rtol. atol is an absolute tolerance and rtol is a relative tolerance.\n\nAdditional details can be displayed if verbose mode is enabled (verbose > 0). Information will be displayed every verbose iterations.\n\nReference\n\nA. Montoison and D. Orban, TriCG and TriMR: Two Iterative Methods for Symmetric Quasi-Definite Systems, SIAM Journal on Scientific Computing, 43(4), pp. 2502–2525, 2021.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.usymlq-Union{Tuple{T}, Tuple{Any, AbstractVector{T}, AbstractVector{T}}} where T<:AbstractFloat","page":"Reference","title":"Krylov.usymlq","text":"(x, stats) = usymlq(A, b::AbstractVector{T}, c::AbstractVector{T};\n                    atol::T=√eps(T), rtol::T=√eps(T), transfer_to_usymcg::Bool=true,\n                    itmax::Int=0, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the linear system Ax = b using the USYMLQ method.\n\nUSYMLQ is based on a tridiagonalization process for unsymmetric matrices. The error norm ‖x - x*‖ monotonously decreases in USYMLQ. It's considered as a generalization of SYMMLQ.\n\nIt can also be applied to under-determined and over-determined problems. In all cases, problems must be consistent.\n\nAn option gives the possibility of transferring to the USYMCG point, when it exists. The transfer is based on the residual norm.\n\nReferences\n\nM. A. Saunders, H. D. Simon, and E. L. Yip, Two Conjugate-Gradient-Type Methods for Unsymmetric Linear Equations, SIAM Journal on Numerical Analysis, 25(4), pp. 927–940, 1988.\nA. Buttari, D. Orban, D. Ruiz and D. Titley-Peloquin, A tridiagonalization method for symmetric saddle-point and quasi-definite systems, SIAM Journal on Scientific Computing, 41(5), pp. 409–432, 2019.\nA. Montoison and D. Orban, BiLQ: An Iterative Method for Nonsymmetric Linear Systems with a Quasi-Minimum Error Property, SIAM Journal on Matrix Analysis and Applications, 41(3), pp. 1145–1166, 2020.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.usymqr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}, AbstractVector{T}}} where T<:AbstractFloat","page":"Reference","title":"Krylov.usymqr","text":"(x, stats) = usymqr(A, b::AbstractVector{T}, c::AbstractVector{T};\n                    atol::T=√eps(T), rtol::T=√eps(T),\n                    itmax::Int=0, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the linear system Ax = b using the USYMQR method.\n\nUSYMQR is based on a tridiagonalization process for unsymmetric matrices. The residual norm ‖b - Ax‖ monotonously decreases in USYMQR. It's considered as a generalization of MINRES.\n\nIt can also be applied to under-determined and over-determined problems. USYMQR finds the minimum-norm solution if problems are inconsistent.\n\nReferences\n\nM. A. Saunders, H. D. Simon, and E. L. Yip, Two Conjugate-Gradient-Type Methods for Unsymmetric Linear Equations, SIAM Journal on Numerical Analysis, 25(4), pp. 927–940, 1988.\nA. Buttari, D. Orban, D. Ruiz and D. Titley-Peloquin, A tridiagonalization method for symmetric saddle-point and quasi-definite systems, SIAM Journal on Scientific Computing, 41(5), pp. 409–432, 2019.\nA. Montoison and D. Orban, BiLQ: An Iterative Method for Nonsymmetric Linear Systems with a Quasi-Minimum Error Property, SIAM Journal on Matrix Analysis and Applications, 41(3), pp. 1145–1166, 2020.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Krylov.vec2str-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T<:Union{Missing, AbstractFloat}","page":"Reference","title":"Krylov.vec2str","text":"s = vec2str(x; ndisp)\n\nDisplay an array in the form\n\n[ -3.0e-01 -5.1e-01  1.9e-01 ... -2.3e-01 -4.4e-01  2.4e-01 ]\n\nwith (ndisp - 1)/2 elements on each side.\n\n\n\n\n\n","category":"method"},{"location":"solvers/unsymmetric/","page":"Unsymmetric linear systems","title":"Unsymmetric linear systems","text":"bilq\nqmr\nusymlq\nusymqr\ncgs\nbicgstab\ndiom\nfom\ndqgmres\ngmres","category":"page"},{"location":"solvers/unsymmetric/#Krylov.bilq","page":"Unsymmetric linear systems","title":"Krylov.bilq","text":"(x, stats) = bilq(A, b::AbstractVector{T}; c::AbstractVector{T}=b,\n                  atol::T=√eps(T), rtol::T=√eps(T), transfer_to_bicg::Bool=true,\n                  itmax::Int=0, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the square linear system Ax = b using the BiLQ method.\n\nBiLQ is based on the Lanczos biorthogonalization process. When A is symmetric and b = c, BiLQ is equivalent to SYMMLQ.\n\nAn option gives the possibility of transferring to the BiCG point, when it exists. The transfer is based on the residual norm.\n\nReference\n\nA. Montoison and D. Orban, BiLQ: An Iterative Method for Nonsymmetric Linear Systems with a Quasi-Minimum Error Property, SIAM Journal on Matrix Analysis and Applications, 41(3), pp. 1145–1166, 2020.\n\n\n\n\n\n","category":"function"},{"location":"solvers/unsymmetric/#Krylov.qmr","page":"Unsymmetric linear systems","title":"Krylov.qmr","text":"(x, stats) = qmr(A, b::AbstractVector{T}; c::AbstractVector{T}=b,\n                 atol::T=√eps(T), rtol::T=√eps(T),\n                 itmax::Int=0, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the square linear system Ax = b using the QMR method.\n\nQMR is based on the Lanczos biorthogonalization process. When A is symmetric and b = c, QMR is equivalent to MINRES.\n\nReferences\n\nR. W. Freund and N. M. Nachtigal, QMR : a quasi-minimal residual method for non-Hermitian linear systems, Numerische mathematik, Vol. 60(1), pp. 315–339, 1991.\nR. W. Freund and N. M. Nachtigal, An implementation of the QMR method based on coupled two-term recurrences, SIAM Journal on Scientific Computing, Vol. 15(2), pp. 313–337, 1994.\nA. Montoison and D. Orban, BiLQ: An Iterative Method for Nonsymmetric Linear Systems with a Quasi-Minimum Error Property, SIAM Journal on Matrix Analysis and Applications, 41(3), pp. 1145–1166, 2020.\n\n\n\n\n\n","category":"function"},{"location":"solvers/unsymmetric/#Krylov.usymlq","page":"Unsymmetric linear systems","title":"Krylov.usymlq","text":"(x, stats) = usymlq(A, b::AbstractVector{T}, c::AbstractVector{T};\n                    atol::T=√eps(T), rtol::T=√eps(T), transfer_to_usymcg::Bool=true,\n                    itmax::Int=0, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the linear system Ax = b using the USYMLQ method.\n\nUSYMLQ is based on a tridiagonalization process for unsymmetric matrices. The error norm ‖x - x*‖ monotonously decreases in USYMLQ. It's considered as a generalization of SYMMLQ.\n\nIt can also be applied to under-determined and over-determined problems. In all cases, problems must be consistent.\n\nAn option gives the possibility of transferring to the USYMCG point, when it exists. The transfer is based on the residual norm.\n\nReferences\n\nM. A. Saunders, H. D. Simon, and E. L. Yip, Two Conjugate-Gradient-Type Methods for Unsymmetric Linear Equations, SIAM Journal on Numerical Analysis, 25(4), pp. 927–940, 1988.\nA. Buttari, D. Orban, D. Ruiz and D. Titley-Peloquin, A tridiagonalization method for symmetric saddle-point and quasi-definite systems, SIAM Journal on Scientific Computing, 41(5), pp. 409–432, 2019.\nA. Montoison and D. Orban, BiLQ: An Iterative Method for Nonsymmetric Linear Systems with a Quasi-Minimum Error Property, SIAM Journal on Matrix Analysis and Applications, 41(3), pp. 1145–1166, 2020.\n\n\n\n\n\n","category":"function"},{"location":"solvers/unsymmetric/#Krylov.usymqr","page":"Unsymmetric linear systems","title":"Krylov.usymqr","text":"(x, stats) = usymqr(A, b::AbstractVector{T}, c::AbstractVector{T};\n                    atol::T=√eps(T), rtol::T=√eps(T),\n                    itmax::Int=0, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the linear system Ax = b using the USYMQR method.\n\nUSYMQR is based on a tridiagonalization process for unsymmetric matrices. The residual norm ‖b - Ax‖ monotonously decreases in USYMQR. It's considered as a generalization of MINRES.\n\nIt can also be applied to under-determined and over-determined problems. USYMQR finds the minimum-norm solution if problems are inconsistent.\n\nReferences\n\nM. A. Saunders, H. D. Simon, and E. L. Yip, Two Conjugate-Gradient-Type Methods for Unsymmetric Linear Equations, SIAM Journal on Numerical Analysis, 25(4), pp. 927–940, 1988.\nA. Buttari, D. Orban, D. Ruiz and D. Titley-Peloquin, A tridiagonalization method for symmetric saddle-point and quasi-definite systems, SIAM Journal on Scientific Computing, 41(5), pp. 409–432, 2019.\nA. Montoison and D. Orban, BiLQ: An Iterative Method for Nonsymmetric Linear Systems with a Quasi-Minimum Error Property, SIAM Journal on Matrix Analysis and Applications, 41(3), pp. 1145–1166, 2020.\n\n\n\n\n\n","category":"function"},{"location":"solvers/unsymmetric/#Krylov.cgs","page":"Unsymmetric linear systems","title":"Krylov.cgs","text":"(x, stats) = cgs(A, b::AbstractVector{T}; c::AbstractVector{T}=b,\n                 M=I, N=I, atol::T=√eps(T), rtol::T=√eps(T),\n                 itmax::Int=0, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the consistent linear system Ax = b using conjugate gradient squared algorithm.\n\nFrom \"Iterative Methods for Sparse Linear Systems (Y. Saad)\" :\n\n«The method is based on a polynomial variant of the conjugate gradients algorithm. Although related to the so-called bi-conjugate gradients (BCG) algorithm, it does not involve adjoint matrix-vector multiplications, and the expected convergence rate is about twice that of the BCG algorithm.\n\nThe Conjugate Gradient Squared algorithm works quite well in many cases. However, one difficulty is that, since the polynomials are squared, rounding errors tend to be more damaging than in the standard BCG algorithm. In particular, very high variations of the residual vectors often cause the residual norms computed to become inaccurate.\n\nTFQMR and BICGSTAB were developed to remedy this difficulty.»\n\nThis implementation allows a left preconditioner M and a right preconditioner N.\n\nReference\n\nP. Sonneveld, CGS, A Fast Lanczos-Type Solver for Nonsymmetric Linear systems, SIAM Journal on Scientific and Statistical Computing, 10(1), pp. 36–52, 1989.\n\n\n\n\n\n","category":"function"},{"location":"solvers/unsymmetric/#Krylov.bicgstab","page":"Unsymmetric linear systems","title":"Krylov.bicgstab","text":"(x, stats) = bicgstab(A, b::AbstractVector{T}; c::AbstractVector{T}=b,\n                      M=I, N=I, atol::T=√eps(T), rtol::T=√eps(T),\n                      itmax::Int=0, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the square linear system Ax = b using the BICGSTAB method.\n\nThe Biconjugate Gradient Stabilized method is a variant of BiCG, like CGS, but using different updates for the Aᵀ-sequence in order to obtain smoother convergence than CGS.\n\nIf BICGSTAB stagnates, we recommend DQGMRES and BiLQ as alternative methods for unsymmetric square systems.\n\nBICGSTAB stops when itmax iterations are reached or when ‖rₖ‖ ≤ atol + ‖b‖ * rtol. atol is an absolute tolerance and rtol is a relative tolerance.\n\nAdditional details can be displayed if verbose mode is enabled (verbose > 0). Information will be displayed every verbose iterations.\n\nThis implementation allows a left preconditioner M and a right preconditioner N.\n\nReferences\n\nH. A. van der Vorst, Bi-CGSTAB: A fast and smoothly converging variant of Bi-CG for the solution of nonsymmetric linear systems, SIAM Journal on Scientific and Statistical Computing, 13(2), pp. 631–644, 1992.\nG. L.G. Sleijpen and D. R. Fokkema, BiCGstab(ℓ) for linear equations involving unsymmetric matrices with complex spectrum, Electronic Transactions on Numerical Analysis, 1, pp. 11–32, 1993.\n\n\n\n\n\n","category":"function"},{"location":"solvers/unsymmetric/#Krylov.diom","page":"Unsymmetric linear systems","title":"Krylov.diom","text":"(x, stats) = diom(A, b::AbstractVector{T};\n                  M=I, N=I, atol::T=√eps(T), rtol::T=√eps(T), itmax::Int=0,\n                  memory::Int=20, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the consistent linear system Ax = b using direct incomplete orthogonalization method.\n\nDIOM is similar to CG with partial reorthogonalization.\n\nAn advantage of DIOM is that nonsymmetric or symmetric indefinite or both nonsymmetric and indefinite systems of linear equations can be handled by this single algorithm.\n\nThis implementation allows a left preconditioner M and a right preconditioner N.\n\nLeft  preconditioning : M⁻¹Ax = M⁻¹b\nRight preconditioning : AN⁻¹u = b with x = N⁻¹u\nSplit preconditioning : M⁻¹AN⁻¹u = M⁻¹b with x = N⁻¹u\n\nReference\n\nY. Saad, Practical use of some krylov subspace methods for solving indefinite and nonsymmetric linear systems, SIAM journal on scientific and statistical computing, 5(1), pp. 203–228, 1984.\n\n\n\n\n\n","category":"function"},{"location":"solvers/unsymmetric/#Krylov.fom","page":"Unsymmetric linear systems","title":"Krylov.fom","text":"(x, stats) = fom(A, b::AbstractVector{T};\n                 M=I, N=I, atol::T=√eps(T), rtol::T=√eps(T),\n                 reorthogonalization::Bool=false, itmax::Int=0,\n                 memory::Int=20, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the linear system Ax = b using FOM method.\n\nFOM algorithm is based on the Arnoldi process and a Galerkin condition.\n\nThis implementation allows a left preconditioner M and a right preconditioner N.\n\nLeft  preconditioning : M⁻¹Ax = M⁻¹b\nRight preconditioning : AN⁻¹u = b with x = N⁻¹u\nSplit preconditioning : M⁻¹AN⁻¹u = M⁻¹b with x = N⁻¹u\n\nReference\n\nY. Saad, Krylov subspace methods for solving unsymmetric linear systems, Mathematics of computation, Vol. 37(155), pp. 105–126, 1981.\n\n\n\n\n\n","category":"function"},{"location":"solvers/unsymmetric/#Krylov.dqgmres","page":"Unsymmetric linear systems","title":"Krylov.dqgmres","text":"(x, stats) = dqgmres(A, b::AbstractVector{T};\n                     M=I, N=I, atol::T=√eps(T), rtol::T=√eps(T),\n                     itmax::Int=0, memory::Int=20, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the consistent linear system Ax = b using DQGMRES method.\n\nDQGMRES algorithm is based on the incomplete Arnoldi orthogonalization process and computes a sequence of approximate solutions with the quasi-minimal residual property.\n\nThis implementation allows a left preconditioner M and a right preconditioner N.\n\nLeft  preconditioning : M⁻¹Ax = M⁻¹b\nRight preconditioning : AN⁻¹u = b with x = N⁻¹u\nSplit preconditioning : M⁻¹AN⁻¹u = M⁻¹b with x = N⁻¹u\n\nReference\n\nY. Saad and K. Wu, DQGMRES: a quasi minimal residual algorithm based on incomplete orthogonalization, Numerical Linear Algebra with Applications, Vol. 3(4), pp. 329–343, 1996.\n\n\n\n\n\n","category":"function"},{"location":"solvers/unsymmetric/#Krylov.gmres","page":"Unsymmetric linear systems","title":"Krylov.gmres","text":"(x, stats) = gmres(A, b::AbstractVector{T};\n                   M=I, N=I, atol::T=√eps(T), rtol::T=√eps(T),\n                   reorthogonalization::Bool=false, itmax::Int=0,\n                   memory::Int=20, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the linear system Ax = b using GMRES method.\n\nGMRES algorithm is based on the Arnoldi process and computes a sequence of approximate solutions with the minimal residual property.\n\nThis implementation allows a left preconditioner M and a right preconditioner N.\n\nLeft  preconditioning : M⁻¹Ax = M⁻¹b\nRight preconditioning : AN⁻¹u = b with x = N⁻¹u\nSplit preconditioning : M⁻¹AN⁻¹u = M⁻¹b with x = N⁻¹u\n\nReference\n\nY. Saad and M. H. Schultz, GMRES: A Generalized Minimal Residual Algorithm for Solving Nonsymmetric Linear Systems, SIAM Journal on Scientific and Statistical Computing, Vol. 7(3), pp. 856–869, 1986.\n\n\n\n\n\n","category":"function"},{"location":"solvers/sid/","page":"Symmetric indefinite linear systems","title":"Symmetric indefinite linear systems","text":"symmlq\nminres\nminres_qlp","category":"page"},{"location":"solvers/sid/#Krylov.symmlq","page":"Symmetric indefinite linear systems","title":"Krylov.symmlq","text":"(x, stats) = symmlq(A, b::AbstractVector{T};\n                    M=I, λ::T=zero(T), transfer_to_cg::Bool=true,\n                    λest::T=zero(T), atol::T=√eps(T), rtol::T=√eps(T),\n                    etol::T=√eps(T), window::Int=0, itmax::Int=0,\n                    conlim::T=1/√eps(T), verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the shifted linear system\n\n(A + λI) x = b\n\nusing the SYMMLQ method, where λ is a shift parameter, and A is square and symmetric.\n\nSYMMLQ produces monotonic errors ‖x*-x‖₂.\n\nA preconditioner M may be provided in the form of a linear operator and is assumed to be symmetric and positive definite.\n\nReference\n\nC. C. Paige and M. A. Saunders, Solution of Sparse Indefinite Systems of Linear Equations, SIAM Journal on Numerical Analysis, 12(4), pp. 617–629, 1975.\n\n\n\n\n\n","category":"function"},{"location":"solvers/sid/#Krylov.minres","page":"Symmetric indefinite linear systems","title":"Krylov.minres","text":"(x, stats) = minres(A, b::AbstractVector{T};\n                    M=I, λ::T=zero(T), atol::T=√eps(T)/100,\n                    rtol::T=√eps(T)/100, ratol :: T=zero(T), \n                    rrtol :: T=zero(T), etol::T=√eps(T),\n                    window::Int=5, itmax::Int=0, conlim::T=1/√eps(T),\n                    verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the shifted linear least-squares problem\n\nminimize ‖b - (A + λI)x‖₂²\n\nor the shifted linear system\n\n(A + λI) x = b\n\nusing the MINRES method, where λ ≥ 0 is a shift parameter, where A is square and symmetric.\n\nMINRES is formally equivalent to applying CR to Ax=b when A is positive definite, but is typically more stable and also applies to the case where A is indefinite.\n\nMINRES produces monotonic residuals ‖r‖₂ and optimality residuals ‖Aᵀr‖₂.\n\nA preconditioner M may be provided in the form of a linear operator and is assumed to be symmetric and positive definite.\n\nReference\n\nC. C. Paige and M. A. Saunders, Solution of Sparse Indefinite Systems of Linear Equations, SIAM Journal on Numerical Analysis, 12(4), pp. 617–629, 1975.\n\n\n\n\n\n","category":"function"},{"location":"solvers/sid/#Krylov.minres_qlp","page":"Symmetric indefinite linear systems","title":"Krylov.minres_qlp","text":"(x, stats) = minres_qlp(A, b::AbstractVector{T};\n                        M=I, atol::T=√eps(T), rtol::T=√eps(T), λ::T=zero(T),\n                        itmax::Int=0, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nMINRES-QLP is the only method based on the Lanczos process that returns the minimum-norm solution on singular inconsistent systems (A + λI)x = b, where λ is a shift parameter. It is significantly more complex but can be more reliable than MINRES when A is ill-conditioned.\n\nA preconditioner M may be provided in the form of a linear operator and is assumed to be symmetric and positive definite. M also indicates the weighted norm in which residuals are measured.\n\nReferences\n\nS.-C. T. Choi, Iterative methods for singular linear equations and least-squares problems, Ph.D. thesis, ICME, Stanford University, 2006.\nS.-C. T. Choi, C. C. Paige and M. A. Saunders, MINRES-QLP: A Krylov subspace method for indefinite or singular symmetric systems, SIAM Journal on Scientific Computing, Vol. 33(4), pp. 1810–1836, 2011.\nS.-C. T. Choi and M. A. Saunders, Algorithm 937: MINRES-QLP for symmetric and Hermitian linear equations and least-squares problems, ACM Transactions on Mathematical Software, 40(2), pp. 1–12, 2014.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ln/","page":"Least-norm problems","title":"Least-norm problems","text":"cgne\ncrmr\nlnlq\ncraig\ncraigmr","category":"page"},{"location":"solvers/ln/#Krylov.cgne","page":"Least-norm problems","title":"Krylov.cgne","text":"(x, stats) = cgne(A, b::AbstractVector{T};\n                  M=I, λ::T=zero(T), atol::T=√eps(T), rtol::T=√eps(T),\n                  itmax::Int=0, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the consistent linear system\n\nAx + √λs = b\n\nusing the Conjugate Gradient (CG) method, where λ ≥ 0 is a regularization parameter. This method is equivalent to applying CG to the normal equations of the second kind\n\n(AAᵀ + λI) y = b\n\nbut is more stable. When λ = 0, this method solves the minimum-norm problem\n\nmin ‖x‖₂  s.t. Ax = b.\n\nWhen λ > 0, it solves the problem\n\nmin ‖(x,s)‖₂  s.t. Ax + √λs = b.\n\nCGNE produces monotonic errors ‖x-x*‖₂ but not residuals ‖r‖₂. It is formally equivalent to CRAIG, though can be slightly less accurate, but simpler to implement. Only the x-part of the solution is returned.\n\nA preconditioner M may be provided in the form of a linear operator.\n\nReferences\n\nJ. E. Craig, The N-step iteration procedures, Journal of Mathematics and Physics, 34(1), pp. 64–73, 1955.\nJ. E. Craig, Iterations Procedures for Simultaneous Equations, Ph.D. Thesis, Department of Electrical Engineering, MIT, 1954.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ln/#Krylov.crmr","page":"Least-norm problems","title":"Krylov.crmr","text":"(x, stats) = crmr(A, b::AbstractVector{T};\n                  M=I, λ::T=zero(T), atol::T=√eps(T),\n                  rtol::T=√eps(T), itmax::Int=0, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the consistent linear system\n\nAx + √λs = b\n\nusing the Conjugate Residual (CR) method, where λ ≥ 0 is a regularization parameter. This method is equivalent to applying CR to the normal equations of the second kind\n\n(AAᵀ + λI) y = b\n\nbut is more stable. When λ = 0, this method solves the minimum-norm problem\n\nmin ‖x‖₂  s.t.  x ∈ argmin ‖Ax - b‖₂.\n\nWhen λ > 0, this method solves the problem\n\nmin ‖(x,s)‖₂  s.t. Ax + √λs = b.\n\nCGMR produces monotonic residuals ‖r‖₂. It is formally equivalent to CRAIG-MR, though can be slightly less accurate, but simpler to implement. Only the x-part of the solution is returned.\n\nA preconditioner M may be provided in the form of a linear operator.\n\nReferences\n\nD. Orban and M. Arioli, Iterative Solution of Symmetric Quasi-Definite Linear Systems, Volume 3 of Spotlights. SIAM, Philadelphia, PA, 2017.\nD. Orban, The Projected Golub-Kahan Process for Constrained Linear Least-Squares Problems. Cahier du GERAD G-2014-15, 2014.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ln/#Krylov.lnlq","page":"Least-norm problems","title":"Krylov.lnlq","text":"(x, y, stats) = lnlq(A, b::AbstractVector{T};\n                     M=I, N=I, sqd::Bool=false, λ::T=zero(T), σ::T=zero(T),\n                     atol::T=√eps(T), rtol::T=√eps(T), etolx::T=√eps(T), etoly::T=√eps(T), itmax::Int=0,\n                     transfer_to_craig::Bool=true, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nFind the least-norm solution of the consistent linear system\n\nAx + λs = b\n\nusing the LNLQ method, where λ ≥ 0 is a regularization parameter.\n\nFor a system in the form Ax = b, LNLQ method is equivalent to applying SYMMLQ to AAᵀy = b and recovering x = Aᵀy but is more stable. Note that y are the Lagrange multipliers of the least-norm problem\n\nminimize ‖x‖  s.t.  Ax = b.\n\nIf sqd = true, LNLQ solves the symmetric and quasi-definite system\n\n[ -F   Aᵀ ] [ x ]   [ 0 ]\n[  A   E  ] [ y ] = [ b ],\n\nwhere E and F are symmetric and positive definite. LNLQ is then equivalent to applying SYMMLQ to (AF⁻¹Aᵀ + E)y = b with Fx = Aᵀy. Preconditioners M = E⁻¹ ≻ 0 and N = F⁻¹ ≻ 0 may be provided in the form of linear operators.\n\nIf sqd = false, LNLQ solves the symmetric and indefinite system\n\n[ -F   Aᵀ ] [ x ]   [ 0 ]\n[  A   0  ] [ y ] = [ b ].\n\nIn this case, M can still be specified and indicates the weighted norm in which residuals are measured.\n\nIn this implementation, both the x and y-parts of the solution are returned.\n\netolx and etoly are tolerances on the upper bound of the distance to the solution ‖x-xₛ‖ and ‖y-yₛ‖, respectively. The bound is valid if λ>0 or σ>0 where σ should be strictly smaller than the smallest positive singular value. For instance σ:=(1-1e-7)σₘᵢₙ .\n\nReference\n\nR. Estrin, D. Orban, M.A. Saunders, LNLQ: An Iterative Method for Least-Norm Problems with an Error Minimization Property, SIAM Journal on Matrix Analysis and Applications, 40(3), pp. 1102–1124, 2019.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ln/#Krylov.craig","page":"Least-norm problems","title":"Krylov.craig","text":"(x, y, stats) = craig(A, b::AbstractVector{T};\n                      M=I, N=I, sqd::Bool=false, λ::T=zero(T), atol::T=√eps(T),\n                      btol::T=√eps(T), rtol::T=√eps(T), conlim::T=1/√eps(T), itmax::Int=0,\n                      verbose::Int=0, transfer_to_lsqr::Bool=false, history::Bool=false) where T <: AbstractFloat\n\nFind the least-norm solution of the consistent linear system\n\nAx + λs = b\n\nusing the Golub-Kahan implementation of Craig's method, where λ ≥ 0 is a regularization parameter. This method is equivalent to CGNE but is more stable.\n\nFor a system in the form Ax = b, Craig's method is equivalent to applying CG to AAᵀy = b and recovering x = Aᵀy. Note that y are the Lagrange multipliers of the least-norm problem\n\nminimize ‖x‖  s.t.  Ax = b.\n\nPreconditioners M⁻¹ and N⁻¹ may be provided in the form of linear operators and are assumed to be symmetric and positive definite. If sqd = true, CRAIG solves the symmetric and quasi-definite system\n\n[ -N   Aᵀ ] [ x ]   [ 0 ]\n[  A   M  ] [ y ] = [ b ],\n\nwhich is equivalent to applying CG to (AN⁻¹Aᵀ + M)y = b with Nx = Aᵀy.\n\nIf sqd = false, CRAIG solves the symmetric and indefinite system\n\n[ -N   Aᵀ ] [ x ]   [ 0 ]\n[  A   0  ] [ y ] = [ b ].\n\nIn this case, M⁻¹ can still be specified and indicates the weighted norm in which residuals are measured.\n\nIn this implementation, both the x and y-parts of the solution are returned.\n\nReferences\n\nC. C. Paige and M. A. Saunders, LSQR: An Algorithm for Sparse Linear Equations and Sparse Least Squares, ACM Transactions on Mathematical Software, 8(1), pp. 43–71, 1982.\nM. A. Saunders, Solutions of Sparse Rectangular Systems Using LSQR and CRAIG, BIT Numerical Mathematics, 35(4), pp. 588–604, 1995.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ln/#Krylov.craigmr","page":"Least-norm problems","title":"Krylov.craigmr","text":"(x, y, stats) = craigmr(A, b::AbstractVector{T};\n                        M=I, N=I, λ::T=zero(T), atol::T=√eps(T),\n                        rtol::T=√eps(T), itmax::Int=0, verbose::Int=0, history::Bool=false) where T <: AbstractFloat\n\nSolve the consistent linear system\n\nAx + √λs = b\n\nusing the CRAIG-MR method, where λ ≥ 0 is a regularization parameter. This method is equivalent to applying the Conjugate Residuals method to the normal equations of the second kind\n\n(AAᵀ + λI) y = b\n\nbut is more stable. When λ = 0, this method solves the minimum-norm problem\n\nmin ‖x‖₂  s.t.  x ∈ argmin ‖Ax - b‖₂.\n\nWhen λ > 0, this method solves the problem\n\nmin ‖(x,s)‖₂  s.t. Ax + √λs = b.\n\nPreconditioners M⁻¹ and N⁻¹ may be provided in the form of linear operators and are assumed to be symmetric and positive definite. Afterward CRAIGMR solves the symmetric and quasi-definite system\n\n[ -N   Aᵀ ] [ x ]   [ 0 ]\n[  A   M  ] [ y ] = [ b ],\n\nwhich is equivalent to applying MINRES to (M + AN⁻¹Aᵀ)y = b.\n\nCRAIGMR produces monotonic residuals ‖r‖₂. It is formally equivalent to CRMR, though can be slightly more accurate, and intricate to implement. Both the x- and y-parts of the solution are returned.\n\nReferences\n\nD. Orban and M. Arioli. Iterative Solution of Symmetric Quasi-Definite Linear Systems, Volume 3 of Spotlights. SIAM, Philadelphia, PA, 2017.\nD. Orban, The Projected Golub-Kahan Process for Constrained, Linear Least-Squares Problems. Cahier du GERAD G-2014-15, 2014.\n\n\n\n\n\n","category":"function"},{"location":"gpu/#GPU-support","page":"GPU","title":"GPU support","text":"","category":"section"},{"location":"gpu/","page":"GPU","title":"GPU","text":"All solvers in Krylov.jl can be used with CuArrays and allow computations with Nvidia GPU. Problems stored in CPU format (Matrix and Vector) must first be converted to GPU format (CuMatrix and CuVector).","category":"page"},{"location":"gpu/","page":"GPU","title":"GPU","text":"using CUDA, Krylov\n\n# CPU Arrays\nA_cpu = rand(20, 20)\nb_cpu = rand(20)\n\n# GPU Arrays\nA_gpu = CuMatrix(A_cpu)\nb_gpu = CuVector(b_cpu)\n\n# Solve a square and dense system on GPU\nx, stats = bilq(A_gpu, b_gpu)","category":"page"},{"location":"gpu/","page":"GPU","title":"GPU","text":"Sparse matrices have a specific storage on GPU (CuSparseMatrixCSC or CuSparseMatrixCSR):","category":"page"},{"location":"gpu/","page":"GPU","title":"GPU","text":"using CUDA, Krylov\nusing CUDA.CUSPARSE, SparseArrays\n\n# CPU Arrays\nA_cpu = sprand(200, 100, 0.3)\nb_cpu = rand(200)\n\n# GPU Arrays\nA_gpu = CuSparseMatrixCSC(A_cpu)\nb_gpu = CuVector(b_cpu)\n\n# Solve a rectangular and sparse system on GPU\nx, stats = lsmr(A_gpu, b_gpu)","category":"page"},{"location":"gpu/","page":"GPU","title":"GPU","text":"Optimized operator-vector products that exploit GPU features can be also used by means of linear operators.","category":"page"},{"location":"gpu/","page":"GPU","title":"GPU","text":"Preconditioners, especially incomplete Cholesky or Incomplete LU factorizations that involve triangular solves, can be applied directly on GPU thanks to efficient operators that take advantage of CUSPARSE routines.","category":"page"},{"location":"gpu/#Example-with-a-symmetric-positive-definite-system","page":"GPU","title":"Example with a symmetric positive-definite system","text":"","category":"section"},{"location":"gpu/","page":"GPU","title":"GPU","text":"using CUDA, Krylov, LinearOperators\nusing CUDA.CUSPARSE, SparseArrays\n\n# LLᵀ ≈ A for CuSparseMatrixCSC matrices\nP = ic02(A_gpu, 'O')\n\n# Solve Py = x\nfunction ldiv!(y, P, x)\n  copyto!(y, x)                        # Variant for CuSparseMatrixCSR\n  sv2!('T', 'U', 'N', 1.0, P, y, 'O')  # sv2!('N', 'L', 'N', 1.0, P, y, 'O')\n  sv2!('N', 'U', 'N', 1.0, P, y, 'O')  # sv2!('T', 'L', 'N', 1.0, P, y, 'O')\n  return y\nend\n\n# Operator that model P⁻¹\ny = similar(b_gpu); n = length(b_gpu); T = eltype(b_gpu)\nopM = LinearOperator(T, n, n, true, true, x -> ldiv!(y, P, x))\n\n# Solve a symmetric positive definite system with an incomplete Cholesky preconditioner on GPU\n(x, stats) = cg(A_gpu, b_gpu, M=opM)","category":"page"},{"location":"gpu/#Example-with-a-general-square-system","page":"GPU","title":"Example with a general square system","text":"","category":"section"},{"location":"gpu/","page":"GPU","title":"GPU","text":"using CUDA, Krylov, LinearOperators\nusing CUDA.CUSPARSE, SparseArrays\n\n# LU ≈ A for CuSparseMatrixCSC matrices\nP = ilu02(A_gpu, 'O')\n\n# Solve Py = x\nfunction ldiv!(y, P, x)\n  copyto!(y, x)                        # Variant for CuSparseMatrixCSR\n  sv2!('N', 'L', 'N', 1.0, P, y, 'O')  # sv2!('N', 'L', 'U', 1.0, P, y, 'O')\n  sv2!('N', 'U', 'U', 1.0, P, y, 'O')  # sv2!('N', 'U', 'N', 1.0, P, y, 'O')\n  return y\nend\n\n# Operator that model P⁻¹\ny = similar(b_gpu); n = length(b_gpu); T = eltype(b_gpu)\nopM = LinearOperator(T, n, n, false, false, x -> ldiv!(y, P, x))\n\n# Solve an unsymmetric system with an incomplete LU preconditioner on GPU\n(x, stats) = bicgstab(A_gpu, b_gpu, M=opM)","category":"page"},{"location":"#Home","page":"Home","title":"Krylov.jl documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package provides implementations of certain of the most useful Krylov method for a variety of problems:","category":"page"},{"location":"","page":"Home","title":"Home","text":"1 - Square or rectangular full-rank systems","category":"page"},{"location":"","page":"Home","title":"Home","text":"  Ax = b","category":"page"},{"location":"","page":"Home","title":"Home","text":"should be solved when b lies in the range space of A. This situation occurs when","category":"page"},{"location":"","page":"Home","title":"Home","text":"A is square and nonsingular,\nA is tall and has full column rank and b lies in the range of A.","category":"page"},{"location":"","page":"Home","title":"Home","text":"2 - Linear least-squares problems","category":"page"},{"location":"","page":"Home","title":"Home","text":"  min b - Ax","category":"page"},{"location":"","page":"Home","title":"Home","text":"should be solved when b is not in the range of A (inconsistent systems), regardless of the shape and rank of A. This situation mainly occurs when","category":"page"},{"location":"","page":"Home","title":"Home","text":"A is square and singular,\nA is tall and thin.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Underdetermined sytems are less common but also occur.","category":"page"},{"location":"","page":"Home","title":"Home","text":"If there are infinitely many such x (because A is column rank-deficient), one with minimum norm is identified","category":"page"},{"location":"","page":"Home","title":"Home","text":"  min x quad textsubject to quad x in argmin b - Ax","category":"page"},{"location":"","page":"Home","title":"Home","text":"3 - Linear least-norm problems","category":"page"},{"location":"","page":"Home","title":"Home","text":"  min x quad textsubject to quad Ax = b","category":"page"},{"location":"","page":"Home","title":"Home","text":"sould be solved when A is column rank-deficient but b is in the range of A (consistent systems), regardless of the shape of A. This situation mainly occurs when","category":"page"},{"location":"","page":"Home","title":"Home","text":"A is square and singular,\nA is short and wide.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Overdetermined sytems are less common but also occur.","category":"page"},{"location":"","page":"Home","title":"Home","text":"4 - Adjoint systems","category":"page"},{"location":"","page":"Home","title":"Home","text":"  Ax = b quad textand quad A^T y = c","category":"page"},{"location":"","page":"Home","title":"Home","text":"where A can have any shape.","category":"page"},{"location":"","page":"Home","title":"Home","text":"5 - Saddle-point or symmetric quasi-definite (SQD) systems","category":"page"},{"location":"","page":"Home","title":"Home","text":"  beginbmatrix M  phantom-A  A^T  -N endbmatrix beginbmatrix x  y endbmatrix = left(beginbmatrix b  0 endbmatrixbeginbmatrix 0  c endbmatrixbeginbmatrix b  c endbmatrixright)","category":"page"},{"location":"","page":"Home","title":"Home","text":"where A can have any shape.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Krylov solvers are particularly appropriate in situations where such problems must be solved but a factorization is not possible, either because:","category":"page"},{"location":"","page":"Home","title":"Home","text":"A is not available explicitly,\nA would be dense or would consume an excessive amount of memory if it were materialized,\nfactors would consume an excessive amount of memory.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Iterative methods are recommended in either of the following situations:","category":"page"},{"location":"","page":"Home","title":"Home","text":"the problem is sufficiently large that a factorization is not feasible or would be slow,\nan effective preconditioner is known in cases where the problem has unfavorable spectral structure,\nthe operator can be represented efficiently as a sparse matrix,\nthe operator is fast, i.e., can be applied with better complexity than if it were materialized as a matrix. Certain fast operators would materialize as dense matrices.","category":"page"},{"location":"#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"All solvers in Krylov.jl have in-place version, are compatible with GPU and work in any floating-point data type.","category":"page"},{"location":"#How-to-Install","page":"Home","title":"How to Install","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Krylov can be installed and tested through the Julia package manager:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> ]\npkg> add Krylov\npkg> test Krylov","category":"page"}]
}
