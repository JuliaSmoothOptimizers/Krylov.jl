var documenterSearchIndex = {"docs":
[{"location":"api/#Types","page":"API","title":"Types","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Krylov.KrylovStats\nKrylov.SimpleStats\nKrylov.LanczosStats\nKrylov.SymmlqStats\nKrylov.AdjointStats","category":"page"},{"location":"api/#Krylov.KrylovStats","page":"API","title":"Krylov.KrylovStats","text":"Abstract type for statistics returned by a solver\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.SimpleStats","page":"API","title":"Krylov.SimpleStats","text":"Type for statistics returned by the majority of Krylov solvers, the attributes are:\n\nsolved\ninconsistent\nresiduals\nAresiduals\nstatus\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.LanczosStats","page":"API","title":"Krylov.LanczosStats","text":"Type for statistics returned by Lanczos solvers CG-LANCZOS and CG-LANCZOS-SHIFT-SEQ, the attributes are:\n\nsolved\nresiduals\nflagged\nAnorm\nAcond\nstatus\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.SymmlqStats","page":"API","title":"Krylov.SymmlqStats","text":"Type for statistics returned by SYMMLQ, the attributes are:\n\nsolved\nresiduals\nresidualscg\nerrors\nerrorscg\nAnorm\nAcond\nstatus\n\n\n\n\n\n","category":"type"},{"location":"api/#Krylov.AdjointStats","page":"API","title":"Krylov.AdjointStats","text":"Type for statistics returned by adjoint systems solvers BiLQR and TriLQR, the attributes are:\n\nsolved_primal\nsolved_dual\nresiduals_primal\nresiduals_dual\nstatus\n\n\n\n\n\n","category":"type"},{"location":"api/#Utilities","page":"API","title":"Utilities","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Krylov.roots_quadratic\nKrylov.sym_givens\nKrylov.to_boundary\nKrylov.vec2str\nKrylov.kzeros\nKrylov.kones","category":"page"},{"location":"api/#Krylov.roots_quadratic","page":"API","title":"Krylov.roots_quadratic","text":"roots = roots_quadratic(q₂, q₁, q₀; nitref)\n\nFind the real roots of the quadratic\n\nq(x) = q₂ x² + q₁ x + q₀,\n\nwhere q₂, q₁ and q₀ are real. Care is taken to avoid numerical cancellation. Optionally, nitref steps of iterative refinement may be performed to improve accuracy. By default, nitref=1.\n\n\n\n\n\n","category":"function"},{"location":"api/#Krylov.sym_givens","page":"API","title":"Krylov.sym_givens","text":"(c, s, ρ) = sym_givens(a, b)\n\nNumerically stable symmetric Givens reflection. Given a and b reals, return (c, s, ρ) such that\n\n[ c  s ] [ a ] = [ ρ ]\n[ s -c ] [ b ] = [ 0 ].\n\n\n\n\n\nNumerically stable symmetric Givens reflection. Given a and b complexes, return (c, s, ρ) with c real and (s, ρ) complexes such that\n\n[ c   s ] [ a ] = [ ρ ]\n[ s̅  -c ] [ b ] = [ 0 ].\n\n\n\n\n\n","category":"function"},{"location":"api/#Krylov.to_boundary","page":"API","title":"Krylov.to_boundary","text":"roots = to_boundary(x, d, radius; flip, xNorm2, dNorm2)\n\nGiven a trust-region radius radius, a vector x lying inside the trust-region and a direction d, return σ1 and σ2 such that\n\n‖x + σi d‖ = radius, i = 1, 2\n\nin the Euclidean norm. If known, ‖x‖² may be supplied in xNorm2.\n\nIf flip is set to true, σ1 and σ2 are computed such that\n\n‖x - σi d‖ = radius, i = 1, 2.\n\n\n\n\n\n","category":"function"},{"location":"api/#Krylov.vec2str","page":"API","title":"Krylov.vec2str","text":"s = vec2str(x; ndisp)\n\nDisplay an array in the form\n\n[ -3.0e-01 -5.1e-01  1.9e-01 ... -2.3e-01 -4.4e-01  2.4e-01 ]\n\nwith (ndisp - 1)/2 elements on each side.\n\n\n\n\n\n","category":"function"},{"location":"api/#Krylov.kzeros","page":"API","title":"Krylov.kzeros","text":"v = kzeros(S, n)\n\nCreate an AbstractVector of storage type S of length n only composed of zero.\n\n\n\n\n\n","category":"function"},{"location":"api/#Krylov.kones","page":"API","title":"Krylov.kones","text":"v = kones(S, n)\n\nCreate an AbstractVector of storage type S of length n only composed of one.\n\n\n\n\n\n","category":"function"},{"location":"solvers/","page":"Solvers","title":"Solvers","text":"Detailed examples may be found here.","category":"page"},{"location":"solvers/#Krylov-methods","page":"Solvers","title":"Krylov methods","text":"","category":"section"},{"location":"solvers/","page":"Solvers","title":"Solvers","text":"cg\ncr\nsymmlq\ncg_lanczos\ncg_lanczos_shift_seq\nminres\nminres_qlp\ndiom\ndqgmres\nusymlq\nusymqr\ntricg\ntrimr\ntrilqr\nbilq\ncgs\nbicgstab\nqmr\nbilqr\ncgls\ncrls\ncgne\ncrmr\nlslq\nlsqr\nlsmr\nlnlq\ncraig\ncraigmr","category":"page"},{"location":"solvers/#Krylov.cg","page":"Solvers","title":"Krylov.cg","text":"(x, stats) = cg(A, b::AbstractVector{T};\n                M=opEye(), atol::T=√eps(T), rtol::T=√eps(T),\n                itmax::Int=0, radius::T=zero(T), linesearch::Bool=false,\n                verbose::Int=0) where T <: AbstractFloat\n\nThe conjugate gradient method to solve the symmetric linear system Ax=b.\n\nThe method does not abort if A is not definite.\n\nA preconditioner M may be provided in the form of a linear operator and is assumed to be symmetric and positive definite. M also indicates the weighted norm in which residuals are measured.\n\nIf itmax=0, the default number of iterations is set to 2 * n, with n = length(b).\n\nReference\n\nM. R. Hestenes and E. Stiefel, Methods of conjugate gradients for solving linear systems, Journal of Research of the National Bureau of Standards, 49(6), pp. 409–436, 1952.\n\n\n\n\n\n","category":"function"},{"location":"solvers/#Krylov.cr","page":"Solvers","title":"Krylov.cr","text":"(x, stats) = cr(A, b::AbstractVector{T};\n                M=opEye(), atol::T=√eps(T), rtol::T=√eps(T), γ::T=√eps(T), itmax::Int=0,\n                radius::T=zero(T), verbose::Int=0, linesearch::Bool=false) where T <: AbstractFloat\n\nA truncated version of Stiefel’s Conjugate Residual method to solve the symmetric linear system Ax = b or the least-squares problem min ‖b - Ax‖. The matrix A must be positive semi-definite.\n\nA preconditioner M may be provided in the form of a linear operator and is assumed to be symmetric and positive definite. M also indicates the weighted norm in which residuals are measured.\n\nIn a linesearch context, 'linesearch' must be set to 'true'.\n\nIf itmax=0, the default number of iterations is set to 2 * n, with n = length(b).\n\nReferences\n\nM. R. Hestenes and E. Stiefel, Methods of conjugate gradients for solving linear systems, Journal of Research of the National Bureau of Standards, 49(6), pp. 409–436, 1952.\nM-A. Dahito and D. Orban, The Conjugate Residual Method in Linesearch and Trust-Region Methods, SIAM Journal on Optimization, 29(3), pp. 1988–2025, 2019.\n\n\n\n\n\n","category":"function"},{"location":"solvers/#Krylov.symmlq","page":"Solvers","title":"Krylov.symmlq","text":"(x, stats) = symmlq(A, b::AbstractVector{T};\n                    M=opEye(), λ::T=zero(T), transfer_to_cg::Bool=true,\n                    λest::T=zero(T), atol::T=√eps(T), rtol::T=√eps(T),\n                    etol::T=√eps(T), window::Int=0, itmax::Int=0,\n                    conlim::T=1/√eps(T), verbose::Int=0) where T <: AbstractFloat\n\nSolve the shifted linear system\n\n(A + λ I) x = b\n\nusing the SYMMLQ method, where λ is a shift parameter, and A is square and symmetric.\n\nSYMMLQ produces monotonic errors ‖x*-x‖₂.\n\nA preconditioner M may be provided in the form of a linear operator and is assumed to be symmetric and positive definite.\n\nReference\n\nC. C. Paige and M. A. Saunders, Solution of Sparse Indefinite Systems of Linear Equations, SIAM Journal on Numerical Analysis, 12(4), pp. 617–629, 1975.\n\n\n\n\n\n","category":"function"},{"location":"solvers/#Krylov.cg_lanczos","page":"Solvers","title":"Krylov.cg_lanczos","text":"(x, stats) = cg_lanczos(A, b::AbstractVector{T};\n                        M=opEye(), atol::T=√eps(T), rtol::T=√eps(T), itmax::Int=0,\n                        check_curvature::Bool=false, verbose::Int=0) where T <: AbstractFloat\n\nThe Lanczos version of the conjugate gradient method to solve the symmetric linear system\n\nAx = b\n\nThe method does not abort if A is not definite.\n\nA preconditioner M may be provided in the form of a linear operator and is assumed to be symmetric and positive definite.\n\nReferences\n\nA. Frommer and P. Maass, Fast CG-Based Methods for Tikhonov-Phillips Regularization, SIAM Journal on Scientific Computing, 20(5), pp. 1831–1850, 1999.\nC. C. Paige and M. A. Saunders, Solution of Sparse Indefinite Systems of Linear Equations, SIAM Journal on Numerical Analysis, 12(4), pp. 617–629, 1975.\n\n\n\n\n\n","category":"function"},{"location":"solvers/#Krylov.cg_lanczos_shift_seq","page":"Solvers","title":"Krylov.cg_lanczos_shift_seq","text":"(x, stats) = cg_lanczos_shift_seq(A, b::AbstractVector{T}, shifts::AbstractVector{T};\n                                  M=opEye(), atol::T=√eps(T), rtol::T=√eps(T), itmax::Int=0,\n                                  check_curvature::Bool=false, verbose::Int=0) where T <: AbstractFloat\n\nThe Lanczos version of the conjugate gradient method to solve a family of shifted systems\n\n(A + αI) x = b  (α = α₁, ..., αₙ)\n\nThe method does not abort if A + αI is not definite.\n\nA preconditioner M may be provided in the form of a linear operator and is assumed to be symmetric and positive definite.\n\n\n\n\n\n","category":"function"},{"location":"solvers/#Krylov.minres","page":"Solvers","title":"Krylov.minres","text":"(x, stats) = minres(A, b::AbstractVector{T};\n                    M=opEye(), λ::T=zero(T), atol::T=√eps(T)/100,\n                    rtol::T=√eps(T)/100, etol::T=√eps(T),\n                    window::Int=5, itmax::Int=0, conlim::T=1/√eps(T),\n                    verbose::Int=0) where T <: AbstractFloat\n\nSolve the shifted linear least-squares problem\n\nminimize ‖b - (A + λ I)x‖₂²\n\nor the shifted linear system\n\n(A + λ I) x = b\n\nusing the MINRES method, where λ ≥ 0 is a shift parameter, where A is square and symmetric.\n\nMINRES is formally equivalent to applying CR to Ax=b when A is positive definite, but is typically more stable and also applies to the case where A is indefinite.\n\nMINRES produces monotonic residuals ‖r‖₂ and optimality residuals ‖Aᵀr‖₂.\n\nA preconditioner M may be provided in the form of a linear operator and is assumed to be symmetric and positive definite.\n\nReference\n\nC. C. Paige and M. A. Saunders, Solution of Sparse Indefinite Systems of Linear Equations, SIAM Journal on Numerical Analysis, 12(4), pp. 617–629, 1975.\n\n\n\n\n\n","category":"function"},{"location":"solvers/#Krylov.minres_qlp","page":"Solvers","title":"Krylov.minres_qlp","text":"(x, stats) = minres_qlp(A, b::AbstractVector{T};\n                        M=opEye(), atol::T=√eps(T), rtol::T=√eps(T), λ::T=zero(T),\n                        itmax::Int=0, verbose::Int=0) where T <: AbstractFloat\n\nMINRES-QLP is the only method based on the Lanczos process that returns the minimum-norm solution on singular inconsistent systems (A + λI)x = b, where λ is a shift parameter. It is significantly more complex but can be more reliable than MINRES when A is ill-conditioned.\n\nA preconditioner M may be provided in the form of a linear operator and is assumed to be symmetric and positive definite. M also indicates the weighted norm in which residuals are measured.\n\nReferences\n\nS.-C. T. Choi, Iterative methods for singular linear equations and least-squares problems, Ph.D. thesis, ICME, Stanford University, 2006.\nS.-C. T. Choi, C. C. Paige and M. A. Saunders, MINRES-QLP: A Krylov subspace method for indefinite or singular symmetric systems, SIAM Journal on Scientific Computing, Vol. 33(4), pp. 1810–1836, 2011.\nS.-C. T. Choi and M. A. Saunders, Algorithm 937: MINRES-QLP for symmetric and Hermitian linear equations and least-squares problems, ACM Transactions on Mathematical Software, 40(2), pp. 1–12, 2014.\n\n\n\n\n\n","category":"function"},{"location":"solvers/#Krylov.diom","page":"Solvers","title":"Krylov.diom","text":"(x, stats) = diom(A, b::AbstractVector{T};\n                  M=opEye(), N=opEye(), atol::T=√eps(T), rtol::T=√eps(T), itmax::Int=0,\n                  memory::Int=20, pivoting::Bool=false, verbose::Int=0) where T <: AbstractFloat\n\nSolve the consistent linear system Ax = b using direct incomplete orthogonalization method.\n\nDIOM is similar to CG with partial reorthogonalization.\n\nAn advantage of DIOM is that nonsymmetric or symmetric indefinite or both nonsymmetric and indefinite systems of linear equations can be handled by this single algorithm.\n\nThis implementation allows a left preconditioner M and a right preconditioner N.\n\nLeft  preconditioning : M⁻¹Ax = M⁻¹b\nRight preconditioning : AN⁻¹u = b with x = N⁻¹u\nSplit preconditioning : M⁻¹AN⁻¹u = M⁻¹b with x = N⁻¹u\n\nReference\n\nY. Saad, Practical use of some krylov subspace methods for solving indefinite and nonsymmetric linear systems, SIAM journal on scientific and statistical computing, 5(1), pp. 203–228, 1984.\n\n\n\n\n\n","category":"function"},{"location":"solvers/#Krylov.dqgmres","page":"Solvers","title":"Krylov.dqgmres","text":"(x, stats) = dqgmres(A, b::AbstractVector{T};\n                     M=opEye(), N=opEye(), atol::T=√eps(T), rtol::T=√eps(T),\n                     itmax::Int=0, memory::Int=20, verbose::Int=0) where T <: AbstractFloat\n\nSolve the consistent linear system Ax = b using DQGMRES method.\n\nDQGMRES algorithm is based on the incomplete Arnoldi orthogonalization process and computes a sequence of approximate solutions with the quasi-minimal residual property.\n\nThis implementation allows a left preconditioner M and a right preconditioner N.\n\nLeft  preconditioning : M⁻¹Ax = M⁻¹b\nRight preconditioning : AN⁻¹u = b with x = N⁻¹u\nSplit preconditioning : M⁻¹AN⁻¹u = M⁻¹b with x = N⁻¹u\n\nReference\n\nY. Saad and K. Wu, DQGMRES: a quasi minimal residual algorithm based on incomplete orthogonalization, Numerical Linear Algebra with Applications, Vol. 3(4), pp. 329–343, 1996.\n\n\n\n\n\n","category":"function"},{"location":"solvers/#Krylov.usymlq","page":"Solvers","title":"Krylov.usymlq","text":"(x, stats) = usymlq(A, b::AbstractVector{T}, c::AbstractVector{T};\n                    atol::T=√eps(T), rtol::T=√eps(T), transfer_to_usymcg::Bool=true,\n                    itmax::Int=0, verbose::Int=0) where T <: AbstractFloat\n\nSolve the linear system Ax = b using the USYMLQ method.\n\nUSYMLQ is based on a tridiagonalization process for unsymmetric matrices. The error norm ‖x - x*‖ monotonously decreases in USYMLQ. It's considered as a generalization of SYMMLQ.\n\nIt can also be applied to under-determined and over-determined problems. In all cases, problems must be consistent.\n\nAn option gives the possibility of transferring to the USYMCG point, when it exists. The transfer is based on the residual norm.\n\nReferences\n\nM. A. Saunders, H. D. Simon, and E. L. Yip, Two Conjugate-Gradient-Type Methods for Unsymmetric Linear Equations, SIAM Journal on Numerical Analysis, 25(4), pp. 927–940, 1988.\nA. Buttari, D. Orban, D. Ruiz and D. Titley-Peloquin, A tridiagonalization method for symmetric saddle-point and quasi-definite systems, SIAM Journal on Scientific Computing, 41(5), pp. 409–432, 2019.\nA. Montoison and D. Orban, BiLQ: An Iterative Method for Nonsymmetric Linear Systems with a Quasi-Minimum Error Property, SIAM Journal on Matrix Analysis and Applications, 41(3), pp. 1145–1166, 2020.\n\n\n\n\n\n","category":"function"},{"location":"solvers/#Krylov.usymqr","page":"Solvers","title":"Krylov.usymqr","text":"(x, stats) = usymqr(A, b::AbstractVector{T}, c::AbstractVector{T};\n                    atol::T=√eps(T), rtol::T=√eps(T),\n                    itmax::Int=0, verbose::Int=0) where T <: AbstractFloat\n\nSolve the linear system Ax = b using the USYMQR method.\n\nUSYMQR is based on a tridiagonalization process for unsymmetric matrices. The residual norm ‖b - Ax‖ monotonously decreases in USYMQR. It's considered as a generalization of MINRES.\n\nIt can also be applied to under-determined and over-determined problems. USYMQR finds the minimum-norm solution if problems are inconsistent.\n\nReferences\n\nM. A. Saunders, H. D. Simon, and E. L. Yip, Two Conjugate-Gradient-Type Methods for Unsymmetric Linear Equations, SIAM Journal on Numerical Analysis, 25(4), pp. 927–940, 1988.\nA. Buttari, D. Orban, D. Ruiz and D. Titley-Peloquin, A tridiagonalization method for symmetric saddle-point and quasi-definite systems, SIAM Journal on Scientific Computing, 41(5), pp. 409–432, 2019.\nA. Montoison and D. Orban, BiLQ: An Iterative Method for Nonsymmetric Linear Systems with a Quasi-Minimum Error Property, SIAM Journal on Matrix Analysis and Applications, 41(3), pp. 1145–1166, 2020.\n\n\n\n\n\n","category":"function"},{"location":"solvers/#Krylov.tricg","page":"Solvers","title":"Krylov.tricg","text":"(x, y, stats) = tricg(A, b::AbstractVector{T}, c::AbstractVector{T};\n                      M=opEye(), N=opEye(), atol::T=√eps(T), rtol::T=√eps(T),\n                      spd::Bool=false, snd::Bool=false, flip::Bool=false,\n                      τ::T=one(T), ν::T=-one(T), itmax::Int=0, verbose::Int=0) where T <: AbstractFloat\n\nTriCG solves the symmetric linear system\n\n[ τE    A ] [ x ] = [ b ]\n[  Aᵀ  νF ] [ y ]   [ c ],\n\nwhere τ and ν are real numbers, E = M⁻¹ ≻ 0 and F = N⁻¹ ≻ 0. TriCG could breakdown if τ = 0 or ν = 0. It's recommended to use TriMR in these cases.\n\nBy default, TriCG solves symmetric and quasi-definite linear systems with τ = 1 and ν = -1. If flip = true, TriCG solves another known variant of SQD systems where τ = -1 and ν = 1. If spd = true, τ = ν = 1 and the associated symmetric and positive definite linear system is solved. If snd = true, τ = ν = -1 and the associated symmetric and negative definite linear system is solved. τ and ν are also keyword arguments that can be directly modified for more specific problems.\n\nTriCG is based on the preconditioned orthogonal tridiagonalization process and its relation with the preconditioned block-Lanczos process.\n\n[ M   O ]\n[ 0   N ]\n\nindicates the weighted norm in which residuals are measured. It's the Euclidean norm when M and N are identity operators.\n\nTriCG stops when itmax iterations are reached or when ‖rₖ‖ ≤ atol + ‖r₀‖ * rtol. atol is an absolute tolerance and rtol is a relative tolerance.\n\nAdditional details can be displayed if verbose mode is enabled (verbose > 0). Information will be displayed every verbose iterations.\n\nReference\n\nA. Montoison and D. Orban, TriCG and TriMR: Two Iterative Methods for Symmetric Quasi-Definite Systems, Cahier du GERAD G-2020-41, GERAD, Montréal, 2020.\n\n\n\n\n\n","category":"function"},{"location":"solvers/#Krylov.trimr","page":"Solvers","title":"Krylov.trimr","text":"(x, y, stats) = trimr(A, b::AbstractVector{T}, c::AbstractVector{T};\n                      M=opEye(), N=opEye(), atol::T=√eps(T), rtol::T=√eps(T),\n                      spd::Bool=false, snd::Bool=false, flip::Bool=false, sp::Bool=false,\n                      τ::T=one(T), ν::T=-one(T), itmax::Int=0, verbose::Int=0) where T <: AbstractFloat\n\nTriMR solves the symmetric linear system\n\n[ τE    A ] [ x ] = [ b ]\n[  Aᵀ  νF ] [ y ]   [ c ],\n\nwhere τ and ν are real numbers, E = M⁻¹ ≻ 0 and F = N⁻¹ ≻ 0. TriMR handles saddle-point systems (τ = 0 or ν = 0) and adjoint systems (τ = 0 and ν = 0) without any risk of breakdown.\n\nBy default, TriMR solves symmetric and quasi-definite linear systems with τ = 1 and ν = -1. If flip = true, TriMR solves another known variant of SQD systems where τ = -1 and ν = 1. If spd = true, τ = ν = 1 and the associated symmetric and positive definite linear system is solved. If snd = true, τ = ν = -1 and the associated symmetric and negative definite linear system is solved. If sp = true, τ = 1, ν = 0 and the associated saddle-point linear system is solved. τ and ν are also keyword arguments that can be directly modified for more specific problems.\n\nTriMR is based on the preconditioned orthogonal tridiagonalization process and its relation with the preconditioned block-Lanczos process.\n\n[ M   O ]\n[ 0   N ]\n\nindicates the weighted norm in which residuals are measured. It's the Euclidean norm when M and N are identity operators.\n\nTriMR stops when itmax iterations are reached or when ‖rₖ‖ ≤ atol + ‖r₀‖ * rtol. atol is an absolute tolerance and rtol is a relative tolerance.\n\nAdditional details can be displayed if verbose mode is enabled (verbose > 0). Information will be displayed every verbose iterations.\n\nReference\n\nA. Montoison and D. Orban, TriCG and TriMR: Two Iterative Methods for Symmetric Quasi-Definite Systems, Cahier du GERAD G-2020-41, GERAD, Montréal, 2020.\n\n\n\n\n\n","category":"function"},{"location":"solvers/#Krylov.trilqr","page":"Solvers","title":"Krylov.trilqr","text":"(x, t, stats) = trilqr(A, b::AbstractVector{T}, c::AbstractVector{T};\n                       atol::T=√eps(T), rtol::T=√eps(T), transfer_to_usymcg::Bool=true,\n                       itmax::Int=0, verbose::Int=0) where T <: AbstractFloat\n\nCombine USYMLQ and USYMQR to solve adjoint systems.\n\n[0  A] [t] = [b]\n[Aᵀ 0] [x]   [c]\n\nUSYMLQ is used for solving primal system Ax = b. USYMQR is used for solving dual system Aᵀt = c.\n\nAn option gives the possibility of transferring from the USYMLQ point to the USYMCG point, when it exists. The transfer is based on the residual norm.\n\nReference\n\nA. Montoison and D. Orban, BiLQ: An Iterative Method for Nonsymmetric Linear Systems with a Quasi-Minimum Error Property, SIAM Journal on Matrix Analysis and Applications, 41(3), pp. 1145–1166, 2020.\n\n\n\n\n\n","category":"function"},{"location":"solvers/#Krylov.bilq","page":"Solvers","title":"Krylov.bilq","text":"(x, stats) = bilq(A, b::AbstractVector{T}; c::AbstractVector{T}=b,\n                  atol::T=√eps(T), rtol::T=√eps(T), transfer_to_bicg::Bool=true,\n                  itmax::Int=0, verbose::Int=0) where T <: AbstractFloat\n\nSolve the square linear system Ax = b using the BiLQ method.\n\nBiLQ is based on the Lanczos biorthogonalization process. When A is symmetric and b = c, BiLQ is equivalent to SYMMLQ.\n\nAn option gives the possibility of transferring to the BiCG point, when it exists. The transfer is based on the residual norm.\n\nReference\n\nA. Montoison and D. Orban, BiLQ: An Iterative Method for Nonsymmetric Linear Systems with a Quasi-Minimum Error Property, SIAM Journal on Matrix Analysis and Applications, 41(3), pp. 1145–1166, 2020.\n\n\n\n\n\n","category":"function"},{"location":"solvers/#Krylov.cgs","page":"Solvers","title":"Krylov.cgs","text":"(x, stats) = cgs(A, b::AbstractVector{T}; c::AbstractVector{T}=b,\n                 M=opEye(), N=opEye(), atol::T=√eps(T), rtol::T=√eps(T),\n                 itmax::Int=0, verbose::Int=0) where T <: AbstractFloat\n\nSolve the consistent linear system Ax = b using conjugate gradient squared algorithm.\n\nFrom \"Iterative Methods for Sparse Linear Systems (Y. Saad)\" :\n\n«The method is based on a polynomial variant of the conjugate gradients algorithm. Although related to the so-called bi-conjugate gradients (BCG) algorithm, it does not involve adjoint matrix-vector multiplications, and the expected convergence rate is about twice that of the BCG algorithm.\n\nThe Conjugate Gradient Squared algorithm works quite well in many cases. However, one difficulty is that, since the polynomials are squared, rounding errors tend to be more damaging than in the standard BCG algorithm. In particular, very high variations of the residual vectors often cause the residual norms computed to become inaccurate.\n\nTFQMR and BICGSTAB were developed to remedy this difficulty.»\n\nThis implementation allows a left preconditioner M and a right preconditioner N.\n\nReference\n\nP. Sonneveld, CGS, A Fast Lanczos-Type Solver for Nonsymmetric Linear systems, SIAM Journal on Scientific and Statistical Computing, 10(1), pp. 36–52, 1989.\n\n\n\n\n\n","category":"function"},{"location":"solvers/#Krylov.bicgstab","page":"Solvers","title":"Krylov.bicgstab","text":"(x, stats) = bicgstab(A, b::AbstractVector{T}; c::AbstractVector{T}=b,\n                      M=opEye(), N=opEye(), atol::T=√eps(T), rtol::T=√eps(T),\n                      itmax::Int=0, verbose::Int=0) where T <: AbstractFloat\n\nSolve the square linear system Ax = b using the BICGSTAB method.\n\nThe Biconjugate Gradient Stabilized method is a variant of BiCG, like CGS, but using different updates for the Aᵀ-sequence in order to obtain smoother convergence than CGS.\n\nIf BICGSTAB stagnates, we recommend DQGMRES and BiLQ as alternative methods for unsymmetric square systems.\n\nBICGSTAB stops when itmax iterations are reached or when ‖rₖ‖ ≤ atol + ‖b‖ * rtol. atol is an absolute tolerance and rtol is a relative tolerance.\n\nAdditional details can be displayed if verbose mode is enabled (verbose > 0). Information will be displayed every verbose iterations.\n\nThis implementation allows a left preconditioner M and a right preconditioner N.\n\nReferences\n\nH. A. van der Vorst, Bi-CGSTAB: A fast and smoothly converging variant of Bi-CG for the solution of nonsymmetric linear systems, SIAM Journal on Scientific and Statistical Computing, 13(2), pp. 631–644, 1992.\nG. L.G. Sleijpen and D. R. Fokkema, BiCGstab(ℓ) for linear equations involving unsymmetric matrices with complex spectrum, Electronic Transactions on Numerical Analysis, 1, pp. 11–32, 1993.\n\n\n\n\n\n","category":"function"},{"location":"solvers/#Krylov.qmr","page":"Solvers","title":"Krylov.qmr","text":"(x, stats) = qmr(A, b::AbstractVector{T}; c::AbstractVector{T}=b,\n                 atol::T=√eps(T), rtol::T=√eps(T),\n                 itmax::Int=0, verbose::Int=0) where T <: AbstractFloat\n\nSolve the square linear system Ax = b using the QMR method.\n\nQMR is based on the Lanczos biorthogonalization process. When A is symmetric and b = c, QMR is equivalent to MINRES.\n\nReferences\n\nR. W. Freund and N. M. Nachtigal, QMR : a quasi-minimal residual method for non-Hermitian linear systems, Numerische mathematik, Vol. 60(1), pp. 315–339, 1991.\nR. W. Freund and N. M. Nachtigal, An implementation of the QMR method based on coupled two-term recurrences, SIAM Journal on Scientific Computing, Vol. 15(2), pp. 313–337, 1994.\nA. Montoison and D. Orban, BiLQ: An Iterative Method for Nonsymmetric Linear Systems with a Quasi-Minimum Error Property, SIAM Journal on Matrix Analysis and Applications, 41(3), pp. 1145–1166, 2020.\n\n\n\n\n\n","category":"function"},{"location":"solvers/#Krylov.bilqr","page":"Solvers","title":"Krylov.bilqr","text":"(x, t, stats) = bilqr(A, b::AbstractVector{T}, c::AbstractVector{T};\n                      atol::T=√eps(T), rtol::T=√eps(T), transfer_to_bicg::Bool=true,\n                      itmax::Int=0, verbose::Int=0) where T <: AbstractFloat\n\nCombine BiLQ and QMR to solve adjoint systems.\n\n[0  A] [t] = [b]\n[Aᵀ 0] [x]   [c]\n\nBiLQ is used for solving primal system Ax = b. QMR is used for solving dual system Aᵀt = c.\n\nAn option gives the possibility of transferring from the BiLQ point to the BiCG point, when it exists. The transfer is based on the residual norm.\n\nReference\n\nA. Montoison and D. Orban, BiLQ: An Iterative Method for Nonsymmetric Linear Systems with a Quasi-Minimum Error Property, SIAM Journal on Matrix Analysis and Applications, 41(3), pp. 1145–1166, 2020.\n\n\n\n\n\n","category":"function"},{"location":"solvers/#Krylov.cgls","page":"Solvers","title":"Krylov.cgls","text":"(x, stats) = cgls(A, b::AbstractVector{T};\n                  M=opEye(), λ::T=zero(T), atol::T=√eps(T), rtol::T=√eps(T),\n                  radius::T=zero(T), itmax::Int=0, verbose::Int=0) where T <: AbstractFloat\n\nSolve the regularized linear least-squares problem\n\nminimize ‖b - Ax‖₂² + λ ‖x‖₂²\n\nusing the Conjugate Gradient (CG) method, where λ ≥ 0 is a regularization parameter. This method is equivalent to applying CG to the normal equations\n\n(AᵀA + λI) x = Aᵀb\n\nbut is more stable.\n\nCGLS produces monotonic residuals ‖r‖₂ but not optimality residuals ‖Aᵀr‖₂. It is formally equivalent to LSQR, though can be slightly less accurate, but simpler to implement.\n\nReferences\n\nM. R. Hestenes and E. Stiefel. Methods of conjugate gradients for solving linear systems, Journal of Research of the National Bureau of Standards, 49(6), pp. 409–436, 1952.\nA. Björck, T. Elfving and Z. Strakos, Stability of Conjugate Gradient and Lanczos Methods for Linear Least Squares Problems, SIAM Journal on Matrix Analysis and Applications, 19(3), pp. 720–736, 1998.\n\n\n\n\n\n","category":"function"},{"location":"solvers/#Krylov.crls","page":"Solvers","title":"Krylov.crls","text":"(x, stats) = crls(A, b::AbstractVector{T};\n                  M=opEye(), λ::T=zero(T), atol::T=√eps(T), rtol::T=√eps(T),\n                  radius::T=zero(T), itmax::Int=0, verbose::Int=0) where T <: AbstractFloat\n\nSolve the linear least-squares problem\n\nminimize ‖b - Ax‖₂² + λ ‖x‖₂²\n\nusing the Conjugate Residuals (CR) method. This method is equivalent to applying MINRES to the normal equations\n\n(AᵀA + λI) x = Aᵀb.\n\nThis implementation recurs the residual r := b - Ax.\n\nCRLS produces monotonic residuals ‖r‖₂ and optimality residuals ‖Aᵀr‖₂. It is formally equivalent to LSMR, though can be substantially less accurate, but simpler to implement.\n\nReference\n\nD. C.-L. Fong, Minimum-Residual Methods for Sparse, Least-Squares using Golubg-Kahan Bidiagonalization, Ph.D. Thesis, Stanford University, 2011.\n\n\n\n\n\n","category":"function"},{"location":"solvers/#Krylov.cgne","page":"Solvers","title":"Krylov.cgne","text":"(x, stats) = cgne(A, b::AbstractVector{T};\n                  M=opEye(), λ::T=zero(T), atol::T=√eps(T), rtol::T=√eps(T),\n                  itmax::Int=0, verbose::Int=0) where T <: AbstractFloat\n\nSolve the consistent linear system\n\nAx + √λs = b\n\nusing the Conjugate Gradient (CG) method, where λ ≥ 0 is a regularization parameter. This method is equivalent to applying CG to the normal equations of the second kind\n\n(AAᵀ + λI) y = b\n\nbut is more stable. When λ = 0, this method solves the minimum-norm problem\n\nmin ‖x‖₂  s.t. Ax = b.\n\nWhen λ > 0, it solves the problem\n\nmin ‖(x,s)‖₂  s.t. Ax + √λs = b.\n\nCGNE produces monotonic errors ‖x-x*‖₂ but not residuals ‖r‖₂. It is formally equivalent to CRAIG, though can be slightly less accurate, but simpler to implement. Only the x-part of the solution is returned.\n\nA preconditioner M may be provided in the form of a linear operator.\n\nReferences\n\nJ. E. Craig, The N-step iteration procedures, Journal of Mathematics and Physics, 34(1), pp. 64–73, 1955.\nJ. E. Craig, Iterations Procedures for Simultaneous Equations, Ph.D. Thesis, Department of Electrical Engineering, MIT, 1954.\n\n\n\n\n\n","category":"function"},{"location":"solvers/#Krylov.crmr","page":"Solvers","title":"Krylov.crmr","text":"(x, stats) = crmr(A, b::AbstractVector{T};\n                  M=opEye(), λ::T=zero(T), atol::T=√eps(T),\n                  rtol::T=√eps(T), itmax::Int=0, verbose::Int=0) where T <: AbstractFloat\n\nSolve the consistent linear system\n\nAx + √λs = b\n\nusing the Conjugate Residual (CR) method, where λ ≥ 0 is a regularization parameter. This method is equivalent to applying CR to the normal equations of the second kind\n\n(AAᵀ + λI) y = b\n\nbut is more stable. When λ = 0, this method solves the minimum-norm problem\n\nmin ‖x‖₂  s.t.  x ∈ argmin ‖Ax - b‖₂.\n\nWhen λ > 0, this method solves the problem\n\nmin ‖(x,s)‖₂  s.t. Ax + √λs = b.\n\nCGMR produces monotonic residuals ‖r‖₂. It is formally equivalent to CRAIG-MR, though can be slightly less accurate, but simpler to implement. Only the x-part of the solution is returned.\n\nA preconditioner M may be provided in the form of a linear operator.\n\nReferences\n\nD. Orban and M. Arioli, Iterative Solution of Symmetric Quasi-Definite Linear Systems, Volume 3 of Spotlights. SIAM, Philadelphia, PA, 2017.\nD. Orban, The Projected Golub-Kahan Process for Constrained Linear Least-Squares Problems. Cahier du GERAD G-2014-15, 2014.\n\n\n\n\n\n","category":"function"},{"location":"solvers/#Krylov.lslq","page":"Solvers","title":"Krylov.lslq","text":"(x_lq, x_cg, err_lbnds, err_ubnds_lq, err_ubnds_cg, stats) =\n    lslq(A, b::AbstractVector{T};\n         M=opEye(), N=opEye(), sqd::Bool=false, λ::T=zero(T),\n         atol::T=√eps(T), btol::T=√eps(T), etol::T=√eps(T),\n         window::Int=5, utol::T=√eps(T), itmax::Int=0,\n         σ::T=zero(T), conlim::T=1/√eps(T), verbose::Int=0) where T <: AbstractFloat\n\nSolve the regularized linear least-squares problem\n\nminimize ‖b - Ax‖₂² + λ² ‖x‖₂²\n\nusing the LSLQ method, where λ ≥ 0 is a regularization parameter. LSLQ is formally equivalent to applying SYMMLQ to the normal equations\n\n(AᵀA + λ² I) x = Aᵀb\n\nbut is more stable.\n\nMain features\n\nthe solution estimate is updated along orthogonal directions\nthe norm of the solution estimate ‖xᴸₖ‖₂ is increasing\nthe error ‖eₖ‖₂ := ‖xᴸₖ - x*‖₂ is decreasing\nit is possible to transition cheaply from the LSLQ iterate to the LSQR iterate if there is an advantage (there always is in terms of error)\nif A is rank deficient, identify the minimum least-squares solution\n\nInput arguments\n\nA::AbstractLinearOperator\nb::Vector{Float64}\n\nOptional arguments\n\nM::AbstractLinearOperator=opEye(): a symmetric and positive definite dual preconditioner\nN::AbstractLinearOperator=opEye(): a symmetric and positive definite primal preconditioner\nsqd::Bool=false indicates whether or not we are solving a symmetric and quasi-definite augmented system\n\nIf sqd = true, we solve the symmetric and quasi-definite system\n\n[ E    A ] [ r ]   [ b ]\n[ Aᵀ  -F ] [ x ] = [ 0 ],\n\nwhere E and F are symmetric and positive definite. LSLQ is then equivalent to applying SYMMLQ to (AᵀE⁻¹A + F)y = AᵀE⁻¹b with r = E⁻¹(b - Ax). Preconditioners M = E⁻¹ ≻ 0 and N = F⁻¹ ≻ 0 may be provided in the form of linear operators.\n\nIf sqd is set to false (the default), we solve the symmetric and indefinite system\n\n[ E    A ] [ r ]   [ b ]\n[ Aᵀ   0 ] [ x ] = [ 0 ].\n\nIn this case, N can still be specified and indicates the weighted norm in which x and Aᵀr should be measured. r can be recovered by computing E⁻¹(b - Ax).\n\nλ::Float64=0.0 is a regularization parameter (see the problem statement above)\nσ::Float64=0.0 is an underestimate of the smallest nonzero singular value of A–-setting σ too large will result in an error in the course of the iterations\natol::Float64=1.0e-8 is a stopping tolerance based on the residual\nbtol::Float64=1.0e-8 is a stopping tolerance used to detect zero-residual problems\netol::Float64=1.0e-8 is a stopping tolerance based on the lower bound on the error\nwindow::Int=5 is the number of iterations used to accumulate a lower bound on the error\nutol::Float64=1.0e-8 is a stopping tolerance based on the upper bound on the error\nitmax::Int=0 is the maximum number of iterations (0 means no imposed limit)\nconlim::Float64=1.0e+8 is the limit on the estimated condition number of A beyond which the solution will be abandoned\nverbose::Int=0 determines verbosity.\n\nReturn values\n\nlslq() returns the tuple (x_lq, x_cg, err_lbnds, err_ubnds_lq, err_ubnds_cg, stats) where\n\nx_lq::Vector{Float64} is the LQ solution estimate\nx_cg::Vector{Float64} is the CG solution estimate (i.e., the LSQR point)\nerr_lbnds::Vector{Float64} is a vector of lower bounds on the LQ error–-the vector is empty if window is set to zero\nerr_ubnds_lq::Vector{Float64} is a vector of upper bounds on the LQ error–-the vector is empty if σ == 0 is left at zero\nerr_ubnds_cg::Vector{Float64} is a vector of upper bounds on the CG error–-the vector is empty if σ == 0 is left at zero\nstats::SimpleStats collects other statistics on the run.\n\nStopping conditions\n\nThe iterations stop as soon as one of the following conditions holds true:\n\nthe optimality residual is sufficiently small (stats.status = \"found approximate minimum least-squares solution\") in the sense that either\n‖Aᵀr‖ / (‖A‖ ‖r‖) ≤ atol, or\n1 + ‖Aᵀr‖ / (‖A‖ ‖r‖) ≤ 1\nan approximate zero-residual solution has been found (stats.status = \"found approximate zero-residual solution\") in the sense that either\n‖r‖ / ‖b‖ ≤ btol + atol ‖A‖ * ‖xᴸ‖ / ‖b‖, or\n1 + ‖r‖ / ‖b‖ ≤ 1\nthe estimated condition number of A is too large in the sense that either\n1/cond(A) ≤ 1/conlim (stats.status = \"condition number exceeds tolerance\"), or\n1 + 1/cond(A) ≤ 1 (stats.status = \"condition number seems too large for this machine\")\nthe lower bound on the LQ forward error is less than etol * ‖xᴸ‖\nthe upper bound on the CG forward error is less than utol * ‖xᶜ‖\n\nReferences\n\nR. Estrin, D. Orban and M. A. Saunders, Euclidean-norm error bounds for SYMMLQ and CG, SIAM Journal on Matrix Analysis and Applications, 40(1), pp. 235–253, 2019.\nR. Estrin, D. Orban and M. A. Saunders, LSLQ: An Iterative Method for Linear Least-Squares with an Error Minimization Property, SIAM Journal on Matrix Analysis and Applications, 40(1), pp. 254–275, 2019.\n\n\n\n\n\n","category":"function"},{"location":"solvers/#Krylov.lsqr","page":"Solvers","title":"Krylov.lsqr","text":"(x, stats) = lsqr(A, b::AbstractVector{T};\n                  M=opEye(), N=opEye(), sqd::Bool=false,\n                  λ::T=zero(T), axtol::T=√eps(T), btol::T=√eps(T),\n                  atol::T=zero(T), rtol::T=zero(T),\n                  etol::T=√eps(T), window::Int=5,\n                  itmax::Int=0, conlim::T=1/√eps(T),\n                  radius::T=zero(T), verbose::Int=0) where T <: AbstractFloat\n\nSolve the regularized linear least-squares problem\n\nminimize ‖b - Ax‖₂² + λ² ‖x‖₂²\n\nusing the LSQR method, where λ ≥ 0 is a regularization parameter. LSQR is formally equivalent to applying CG to the normal equations\n\n(AᵀA + λ² I) x = Aᵀb\n\n(and therefore to CGLS) but is more stable.\n\nLSQR produces monotonic residuals ‖r‖₂ but not optimality residuals ‖Aᵀr‖₂. It is formally equivalent to CGLS, though can be slightly more accurate.\n\nPreconditioners M and N may be provided in the form of linear operators and are assumed to be symmetric and positive definite. If sqd is set to true, we solve the symmetric and quasi-definite system\n\n[ E    A ] [ r ]   [ b ]\n[ Aᵀ  -F ] [ x ] = [ 0 ],\n\nwhere E and F are symmetric and positive definite. LSQR is then equivalent to applying CG to (AᵀE⁻¹A + F)y = AᵀE⁻¹b with r = E⁻¹(b - Ax). Preconditioners M = E⁻¹ ≻ 0 and N = F⁻¹ ≻ 0 may be provided in the form of linear operators.\n\nIf sqd is set to false (the default), we solve the symmetric and indefinite system\n\n[ E    A ] [ r ]   [ b ]\n[ Aᵀ   0 ] [ x ] = [ 0 ].\n\nIn this case, N can still be specified and indicates the weighted norm in which x and Aᵀr should be measured. r can be recovered by computing E⁻¹(b - Ax).\n\nReference\n\nC. C. Paige and M. A. Saunders, LSQR: An Algorithm for Sparse Linear Equations and Sparse Least Squares, ACM Transactions on Mathematical Software, 8(1), pp. 43–71, 1982.\n\n\n\n\n\n","category":"function"},{"location":"solvers/#Krylov.lsmr","page":"Solvers","title":"Krylov.lsmr","text":"(x, stats) = lsmr(A, b::AbstractVector{T};\n                  M=opEye(), N=opEye(), sqd::Bool=false,\n                  λ::T=zero(T), axtol::T=√eps(T), btol::T=√eps(T),\n                  atol::T=zero(T), rtol::T=zero(T),\n                  etol::T=√eps(T), window::Int=5,\n                  itmax::Int=0, conlim::T=1/√eps(T),\n                  radius::T=zero(T), verbose::Int=0) where T <: AbstractFloat\n\nSolve the regularized linear least-squares problem\n\nminimize ‖b - Ax‖₂² + λ² ‖x‖₂²\n\nusing the LSMR method, where λ ≥ 0 is a regularization parameter. LSQR is formally equivalent to applying MINRES to the normal equations\n\n(AᵀA + λ² I) x = Aᵀb\n\n(and therefore to CRLS) but is more stable.\n\nLSMR produces monotonic residuals ‖r‖₂ and optimality residuals ‖Aᵀr‖₂. It is formally equivalent to CRLS, though can be substantially more accurate.\n\nPreconditioners M and N may be provided in the form of linear operators and are assumed to be symmetric and positive definite. If sqd is set to true, we solve the symmetric and quasi-definite system\n\n[ E    A ] [ r ]   [ b ]\n[ Aᵀ  -F ] [ x ] = [ 0 ],\n\nwhere E and F are symmetric and positive definite. LSMR is then equivalent to applying MINRES to (AᵀE⁻¹A + F)y = AᵀE⁻¹b with r = E⁻¹(b - Ax). Preconditioners M = E⁻¹ ≻ 0 and N = F⁻¹ ≻ 0 may be provided in the form of linear operators.\n\nIf sqd is set to false (the default), we solve the symmetric and indefinite system\n\n[ E    A ] [ r ]   [ b ]\n[ Aᵀ   0 ] [ x ] = [ 0 ].\n\nIn this case, N can still be specified and indicates the weighted norm in which x and Aᵀr should be measured. r can be recovered by computing E⁻¹(b - Ax).\n\nReference\n\nD. C.-L. Fong and M. A. Saunders, LSMR: An Iterative Algorithm for Sparse, Least Squares Problems, SIAM Journal on Scientific Computing, 33(5), pp. 2950–2971, 2011.\n\n\n\n\n\n","category":"function"},{"location":"solvers/#Krylov.lnlq","page":"Solvers","title":"Krylov.lnlq","text":"(x, y, stats) = lnlq(A, b :: AbstractVector{T};\n                     M=opEye(), N=opEye(), sqd :: Bool=false, λ :: T=zero(T),\n                     atol :: T=√eps(T), rtol :: T=√eps(T), itmax :: Int=0,\n                     transfer_to_craig :: Bool=true, verbose :: Int=0) where T <: AbstractFloat\n\nFind the least-norm solution of the consistent linear system\n\nAx + λs = b\n\nusing the LNLQ method, where λ ≥ 0 is a regularization parameter.\n\nFor a system in the form Ax = b, LNLQ method is equivalent to applying SYMMLQ to AAᵀy = b and recovering x = Aᵀy but is more stable. Note that y are the Lagrange multipliers of the least-norm problem\n\nminimize ‖x‖  s.t.  Ax = b.\n\nIf sqd = true, LNLQ solves the symmetric and quasi-definite system\n\n[ -F   Aᵀ ] [ x ]   [ 0 ]\n[  A   E  ] [ y ] = [ b ],\n\nwhere E and F are symmetric and positive definite. LNLQ is then equivalent to applying SYMMLQ to (AF⁻¹Aᵀ + E)y = b with Fx = Aᵀy. Preconditioners M = E⁻¹ ≻ 0 and N = F⁻¹ ≻ 0 may be provided in the form of linear operators.\n\nIf sqd = false, LNLQ solves the symmetric and indefinite system\n\n[ -F   Aᵀ ] [ x ]   [ 0 ]\n[  A   0  ] [ y ] = [ b ].\n\nIn this case, M can still be specified and indicates the weighted norm in which residuals are measured.\n\nIn this implementation, both the x and y-parts of the solution are returned.\n\nReference\n\nR. Estrin, D. Orban, M.A. Saunders, LNLQ: An Iterative Method for Least-Norm Problems with an Error Minimization Property, SIAM Journal on Matrix Analysis and Applications, 40(3), pp. 1102–1124, 2019.\n\n\n\n\n\n","category":"function"},{"location":"solvers/#Krylov.craig","page":"Solvers","title":"Krylov.craig","text":"(x, y, stats) = craig(A, b::AbstractVector{T};\n                      M=opEye(), N=opEye(), sqd::Bool=false, λ::T=zero(T), atol::T=√eps(T),\n                      btol::T=√eps(T), rtol::T=√eps(T), conlim::T=1/√eps(T), itmax::Int=0,\n                      verbose::Int=0, transfer_to_lsqr::Bool=false) where T <: AbstractFloat\n\nFind the least-norm solution of the consistent linear system\n\nAx + λs = b\n\nusing the Golub-Kahan implementation of Craig's method, where λ ≥ 0 is a regularization parameter. This method is equivalent to CGNE but is more stable.\n\nFor a system in the form Ax = b, Craig's method is equivalent to applying CG to AAᵀy = b and recovering x = Aᵀy. Note that y are the Lagrange multipliers of the least-norm problem\n\nminimize ‖x‖  s.t.  Ax = b.\n\nPreconditioners M⁻¹ and N⁻¹ may be provided in the form of linear operators and are assumed to be symmetric and positive definite. If sqd = true, CRAIG solves the symmetric and quasi-definite system\n\n[ -N   Aᵀ ] [ x ]   [ 0 ]\n[  A   M  ] [ y ] = [ b ],\n\nwhich is equivalent to applying CG to (AN⁻¹Aᵀ + M)y = b with Nx = Aᵀy.\n\nIf sqd = false, CRAIG solves the symmetric and indefinite system\n\n[ -N   Aᵀ ] [ x ]   [ 0 ]\n[  A   0  ] [ y ] = [ b ].\n\nIn this case, M⁻¹ can still be specified and indicates the weighted norm in which residuals are measured.\n\nIn this implementation, both the x and y-parts of the solution are returned.\n\nReferences\n\nC. C. Paige and M. A. Saunders, LSQR: An Algorithm for Sparse Linear Equations and Sparse Least Squares, ACM Transactions on Mathematical Software, 8(1), pp. 43–71, 1982.\nM. A. Saunders, Solutions of Sparse Rectangular Systems Using LSQR and CRAIG, BIT Numerical Mathematics, 35(4), pp. 588–604, 1995.\n\n\n\n\n\n","category":"function"},{"location":"solvers/#Krylov.craigmr","page":"Solvers","title":"Krylov.craigmr","text":"(x, y, stats) = craigmr(A, b::AbstractVector{T};\n                        M=opEye(), N=opEye(), λ::T=zero(T), atol::T=√eps(T),\n                        rtol::T=√eps(T), itmax::Int=0, verbose::Int=0) where T <: AbstractFloat\n\nSolve the consistent linear system\n\nAx + √λs = b\n\nusing the CRAIG-MR method, where λ ≥ 0 is a regularization parameter. This method is equivalent to applying the Conjugate Residuals method to the normal equations of the second kind\n\n(AAᵀ + λI) y = b\n\nbut is more stable. When λ = 0, this method solves the minimum-norm problem\n\nmin ‖x‖₂  s.t.  x ∈ argmin ‖Ax - b‖₂.\n\nWhen λ > 0, this method solves the problem\n\nmin ‖(x,s)‖₂  s.t. Ax + √λs = b.\n\nPreconditioners M⁻¹ and N⁻¹ may be provided in the form of linear operators and are assumed to be symmetric and positive definite. Afterward CRAIGMR solves the symmetric and quasi-definite system\n\n[ -N   Aᵀ ] [ x ]   [ 0 ]\n[  A   M  ] [ y ] = [ b ],\n\nwhich is equivalent to applying MINRES to (M + AN⁻¹Aᵀ)y = b.\n\nCRAIGMR produces monotonic residuals ‖r‖₂. It is formally equivalent to CRMR, though can be slightly more accurate, and intricate to implement. Both the x- and y-parts of the solution are returned.\n\nReferences\n\nD. Orban and M. Arioli. Iterative Solution of Symmetric Quasi-Definite Linear Systems, Volume 3 of Spotlights. SIAM, Philadelphia, PA, 2017.\nD. Orban, The Projected Golub-Kahan Process for Constrained, Linear Least-Squares Problems. Cahier du GERAD G-2014-15, 2014.\n\n\n\n\n\n","category":"function"},{"location":"matrix-free/#Matrix-free-operators","page":"Matrix-free operators","title":"Matrix-free operators","text":"","category":"section"},{"location":"matrix-free/","page":"Matrix-free operators","title":"Matrix-free operators","text":"All methods are matrix free, which means that you only need to provide operator-vector products.","category":"page"},{"location":"matrix-free/","page":"Matrix-free operators","title":"Matrix-free operators","text":"The A, M or N input arguments of Krylov.jl solvers can be any object that represents a linear operator. That object must implement *, for multiplication with a vector, size() and eltype(). For certain methods it must also implement adjoint().","category":"page"},{"location":"matrix-free/","page":"Matrix-free operators","title":"Matrix-free operators","text":"Some methods only require A * v products, whereas other ones also require A' * u products. In the latter case, adjoint(A) must also be implemented.","category":"page"},{"location":"matrix-free/","page":"Matrix-free operators","title":"Matrix-free operators","text":"A * v A * v and A' * u\nCG, CR CGLS, CRLS, CGNE, CRMR\nSYMMLQ, CG-LANCZOS, MINRES, MINRES-QLP LSLQ, LSQR, LSMR, LNLQ, CRAIG, CRAIGMR\nDQGMRES, DIOM BiLQ, QMR, BiLQR, USYMLQ, USYMQR, TriLQR\nCGS, BICGSTAB TriCG, TriMR, USYMLQR","category":"page"},{"location":"matrix-free/","page":"Matrix-free operators","title":"Matrix-free operators","text":"We strongly recommend LinearOperators.jl to model matrix-free operators, but other packages such as LinearMaps.jl, DiffEqOperators.jl or your own operator can be used as well.","category":"page"},{"location":"matrix-free/","page":"Matrix-free operators","title":"Matrix-free operators","text":"With LinearOperators.jl, operators are defined as","category":"page"},{"location":"matrix-free/","page":"Matrix-free operators","title":"Matrix-free operators","text":"A = LinearOperator(type, nrows, ncols, symmetric, hermitian, prod, tprod, ctprod)","category":"page"},{"location":"matrix-free/","page":"Matrix-free operators","title":"Matrix-free operators","text":"where","category":"page"},{"location":"matrix-free/","page":"Matrix-free operators","title":"Matrix-free operators","text":"type is the operator element type;\nnrow and ncol are its dimensions;\nsymmetric and hermitian should be set to true or false;\nprod(v), tprod(w) and ctprod(u) are called when writing A * v, tranpose(A) * w, and A' * u, respectively.","category":"page"},{"location":"matrix-free/","page":"Matrix-free operators","title":"Matrix-free operators","text":"See the tutorial and the detailed documentation for more informations on LinearOperators.jl.","category":"page"},{"location":"matrix-free/","page":"Matrix-free operators","title":"Matrix-free operators","text":"note: Note\nWhen the A, M or N input arguments are not matrices, the operator op should follow the convention that y = op * v and z = op' * u are not allocating and act like the 3-arg mul! to be memory efficient.","category":"page"},{"location":"matrix-free/#Examples","page":"Matrix-free operators","title":"Examples","text":"","category":"section"},{"location":"matrix-free/","page":"Matrix-free operators","title":"Matrix-free operators","text":"In the field of nonlinear optimization, finding critical points of a continuous function frequently involves linear systems with a Hessian or Jacobian as coefficient. Materializing such operators as matrices is expensive in terms of operations and memory consumption and is unreasonable for high-dimensional problems. However, it is often possible to implement efficient Hessian-vector and Jacobian-vector products, for example with the help of automatic differentiation tools, and used within Krylov solvers. We now illustrate variants with explicit matrices and with matrix-free operators for two well-known optimization methods.","category":"page"},{"location":"matrix-free/#Example-1:-Newton's-Method-for-convex-optimization","page":"Matrix-free operators","title":"Example 1: Newton's Method for convex optimization","text":"","category":"section"},{"location":"matrix-free/","page":"Matrix-free operators","title":"Matrix-free operators","text":"At each iteration of Newton's method applied to a mathcalC^2 strictly convex function f  mathbbR^n rightarrow mathbbR, a descent direction direction is determined by minimizing the quadratic Taylor model of f:","category":"page"},{"location":"matrix-free/","page":"Matrix-free operators","title":"Matrix-free operators","text":"min_d in mathbbR^nf(x_k) + nabla f(x_k)^T d + tfrac12d^T nabla^2 f(x_k) d","category":"page"},{"location":"matrix-free/","page":"Matrix-free operators","title":"Matrix-free operators","text":"which is equivalent to solving the symmetric and positive-definite system","category":"page"},{"location":"matrix-free/","page":"Matrix-free operators","title":"Matrix-free operators","text":"nabla^2 f(x_k) d  = -nabla f(x_k)","category":"page"},{"location":"matrix-free/","page":"Matrix-free operators","title":"Matrix-free operators","text":"The system above can be solved with the conjugate gradient method as follows, using the explicit Hessian:","category":"page"},{"location":"matrix-free/","page":"Matrix-free operators","title":"Matrix-free operators","text":"using ForwardDiff, Krylov\n\nxk = -ones(4)\n\nf(x) = (x[1] - 1)^2 + (x[2] - 2)^2 + (x[3] - 3)^2 + (x[4] - 4)^2\n\ng(x) = ForwardDiff.gradient(f, x)\n\nH(x) = ForwardDiff.hessian(f, x)\n\nd, stats = cg(H(xk), -g(xk))","category":"page"},{"location":"matrix-free/","page":"Matrix-free operators","title":"Matrix-free operators","text":"The explicit Hessian can be replaced by a linear operator that only computes Hessian-vector products:","category":"page"},{"location":"matrix-free/","page":"Matrix-free operators","title":"Matrix-free operators","text":"using ForwardDiff, LinearOperators, Krylov\n\nxk = -ones(4)\n\nf(x) = (x[1] - 1)^2 + (x[2] - 2)^2 + (x[3] - 3)^2 + (x[4] - 4)^2\n\ng(x) = ForwardDiff.gradient(f, x)\n\nH(v) = ForwardDiff.derivative(t -> g(xk + t * v), 0.0)\nopH = LinearOperator(Float64, 4, 4, true, true, v -> H(v))\n\ncg(opH, -g(xk))","category":"page"},{"location":"matrix-free/#Example-2:-The-Gauss-Newton-Method-for-Nonlinear-Least-Squares","page":"Matrix-free operators","title":"Example 2: The Gauss-Newton Method for Nonlinear Least Squares","text":"","category":"section"},{"location":"matrix-free/","page":"Matrix-free operators","title":"Matrix-free operators","text":"At each iteration of the Gauss-Newton method applied to a nonlinear least-squares objective f(x) = tfrac12 F(x)^2 where F  mathbbR^n rightarrow mathbbR^m is mathcalC^1, we solve the subproblem:","category":"page"},{"location":"matrix-free/","page":"Matrix-free operators","title":"Matrix-free operators","text":"min_d in mathbbR^ntfrac12J(x_k) d + F(x_k)^2","category":"page"},{"location":"matrix-free/","page":"Matrix-free operators","title":"Matrix-free operators","text":"where J(x) is the Jacobian of F at x.","category":"page"},{"location":"matrix-free/","page":"Matrix-free operators","title":"Matrix-free operators","text":"An appropriate iterative method to solve the above linear least-squares problems is LSMR. We could pass the explicit Jacobian to LSMR as follows:","category":"page"},{"location":"matrix-free/","page":"Matrix-free operators","title":"Matrix-free operators","text":"using ForwardDiff, Krylov\n\nxk = ones(2)\n\nF(x) = [x[1]^4 - 3; exp(x[2]) - 2; log(x[1]) - x[2]^2]\n\nJ(x) = ForwardDiff.jacobian(F, x)\n\nd, stats = lsmr(J(xk), -F(xk))","category":"page"},{"location":"matrix-free/","page":"Matrix-free operators","title":"Matrix-free operators","text":"However, the explicit Jacobian can be replaced by a linear operator that only computes Jacobian-vector and transposed Jacobian-vector products:","category":"page"},{"location":"matrix-free/","page":"Matrix-free operators","title":"Matrix-free operators","text":"using LinearAlgebra, ForwardDiff, LinearOperators, Krylov\n\nxk = ones(2)\n\nF(x) = [x[1]^4 - 3; exp(x[2]) - 2; log(x[1]) - x[2]^2]\n\nJ(v) = ForwardDiff.derivative(t -> F(xk + t * v), 0)\nJᵀ(u) = ForwardDiff.gradient(x -> dot(F(x), u), xk)\nopJ = LinearOperator(Float64, 3, 2, false, false, v -> J(v), w -> Jᵀ(w), u -> Jᵀ(u))\n\nlsmr(opJ, -F(xk))","category":"page"},{"location":"matrix-free/","page":"Matrix-free operators","title":"Matrix-free operators","text":"Note that preconditioners can be also implemented as abstract operators. For instance, we could compute the Cholesky factorization of M and N and create linear operators that perform the forward and backsolves.","category":"page"},{"location":"matrix-free/","page":"Matrix-free operators","title":"Matrix-free operators","text":"Krylov methods combined with matrix free operators allow to reduce computation time and memory requirements considerably by avoiding building and storing the system matrix. In the field of partial differential equations, the implementation of high-performance matrix free operators and assembly free preconditioning is a subject of active research.","category":"page"},{"location":"gpu/#GPU-support","page":"GPU","title":"GPU support","text":"","category":"section"},{"location":"gpu/","page":"GPU","title":"GPU","text":"All solvers in Krylov.jl can be used with CuArrays and allow computations with Nvidia GPU. Problems stored in CPU format (Matrix and Vector) must first be converted to GPU format (CuMatrix and CuVector).","category":"page"},{"location":"gpu/","page":"GPU","title":"GPU","text":"using CUDA, Krylov\n\n# CPU Arrays\nA_cpu = rand(20, 20)\nb_cpu = rand(20)\n\n# GPU Arrays\nA_gpu = CuMatrix(A_cpu)\nb_gpu = CuVector(b_cpu)\n\n# Solve a square and dense system on GPU\nx, stats = bilq(A_gpu, b_gpu)","category":"page"},{"location":"gpu/","page":"GPU","title":"GPU","text":"Sparse matrices have a specific storage on GPU (CuSparseMatrixCSC or CuSparseMatrixCSR):","category":"page"},{"location":"gpu/","page":"GPU","title":"GPU","text":"using CUDA, Krylov\nusing CUDA.CUSPARSE, SparseArrays\n\n# CPU Arrays\nA_cpu = sprand(200, 100, 0.3)\nb_cpu = rand(200)\n\n# GPU Arrays\nA_gpu = CuSparseMatrixCSC(A_cpu)\nb_gpu = CuVector(b_cpu)\n\n# Solve a rectangular and sparse system on GPU\nx, stats = lsmr(A_gpu, b_gpu)","category":"page"},{"location":"gpu/","page":"GPU","title":"GPU","text":"Optimized operator-vector products that exploit GPU features can be also used by means of linear operators.","category":"page"},{"location":"gpu/","page":"GPU","title":"GPU","text":"Preconditioners, especially incomplete Cholesky or Incomplete LU factorizations that involve triangular solves, can be applied directly on GPU thanks to efficient operators that take advantage of CUSPARSE routines.","category":"page"},{"location":"gpu/#Example-with-a-symmetric-positive-definite-system","page":"GPU","title":"Example with a symmetric positive-definite system","text":"","category":"section"},{"location":"gpu/","page":"GPU","title":"GPU","text":"using CUDA, Krylov, LinearOperators\nusing CUDA.CUSPARSE, SparseArrays\n\n# LLᵀ ≈ A for CuSparseMatrixCSC matrices\nP = ic02(A_gpu, 'O')\n\n# Solve Py = x\nfunction ldiv!(y, P, x)\n  copyto!(y, x)                        # Variant for CuSparseMatrixCSR\n  sv2!('T', 'U', 'N', 1.0, P, y, 'O')  # sv2!('N', 'L', 'N', 1.0, P, y, 'O')\n  sv2!('N', 'U', 'N', 1.0, P, y, 'O')  # sv2!('T', 'L', 'N', 1.0, P, y, 'O')\n  return y\nend\n\n# Operator that model P⁻¹\ny = similar(b_gpu); n = length(b_gpu); T = eltype(b_gpu)\nopM = LinearOperator(T, n, n, true, true, x -> ldiv!(y, P, x))\n\n# Solve a symmetric positive definite system with an incomplete Cholesky preconditioner on GPU\n(x, stats) = cg(A_gpu, b_gpu, M=opM)","category":"page"},{"location":"gpu/#Example-with-a-general-square-system","page":"GPU","title":"Example with a general square system","text":"","category":"section"},{"location":"gpu/","page":"GPU","title":"GPU","text":"using CUDA, Krylov, LinearOperators\nusing CUDA.CUSPARSE, SparseArrays\n\n# LU ≈ A for CuSparseMatrixCSC matrices\nP = ilu02(A_gpu, 'O')\n\n# Solve Py = x\nfunction ldiv!(y, P, x)\n  copyto!(y, x)                        # Variant for CuSparseMatrixCSR\n  sv2!('N', 'L', 'N', 1.0, P, y, 'O')  # sv2!('N', 'L', 'U', 1.0, P, y, 'O')\n  sv2!('N', 'U', 'U', 1.0, P, y, 'O')  # sv2!('N', 'U', 'N', 1.0, P, y, 'O')\n  return y\nend\n\n# Operator that model P⁻¹\ny = similar(b_gpu); n = length(b_gpu); T = eltype(b_gpu)\nopM = LinearOperator(T, n, n, false, false, x -> ldiv!(y, P, x))\n\n# Solve an unsymmetric system with an incomplete LU preconditioner on GPU\n(x, stats) = bicgstab(A_gpu, b_gpu, M=opM)","category":"page"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"","category":"page"},{"location":"#Home","page":"Home","title":"Krylov.jl documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package provides implementations of certain of the most useful Krylov method for a variety of problems:","category":"page"},{"location":"","page":"Home","title":"Home","text":"1 - Square or rectangular full-rank systems","category":"page"},{"location":"","page":"Home","title":"Home","text":"  Ax = b","category":"page"},{"location":"","page":"Home","title":"Home","text":"should be solved when b lies in the range space of A. This situation occurs when","category":"page"},{"location":"","page":"Home","title":"Home","text":"A is square and nonsingular,\nA is tall and has full column rank and b lies in the range of A.","category":"page"},{"location":"","page":"Home","title":"Home","text":"2 - Linear least-squares problems","category":"page"},{"location":"","page":"Home","title":"Home","text":"  min b - Ax","category":"page"},{"location":"","page":"Home","title":"Home","text":"should be solved when b is not in the range of A (inconsistent systems), regardless of the shape and rank of A. This situation mainly occurs when","category":"page"},{"location":"","page":"Home","title":"Home","text":"A is square and singular,\nA is tall and thin.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Underdetermined sytems are less common but also occur.","category":"page"},{"location":"","page":"Home","title":"Home","text":"If there are infinitely many such x (because A is column rank-deficient), one with minimum norm is identified","category":"page"},{"location":"","page":"Home","title":"Home","text":"  min x quad textsubject to quad x in argmin b - Ax","category":"page"},{"location":"","page":"Home","title":"Home","text":"3 - Linear least-norm problems","category":"page"},{"location":"","page":"Home","title":"Home","text":"  min x quad textsubject to quad Ax = b","category":"page"},{"location":"","page":"Home","title":"Home","text":"sould be solved when A is column rank-deficient but b is in the range of A (consistent systems), regardless of the shape of A. This situation mainly occurs when","category":"page"},{"location":"","page":"Home","title":"Home","text":"A is square and singular,\nA is short and wide.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Overdetermined sytems are less common but also occur.","category":"page"},{"location":"","page":"Home","title":"Home","text":"4 - Adjoint systems","category":"page"},{"location":"","page":"Home","title":"Home","text":"  Ax = b quad textand quad A^T y = c","category":"page"},{"location":"","page":"Home","title":"Home","text":"where A can have any shape.","category":"page"},{"location":"","page":"Home","title":"Home","text":"5 - Saddle-point or symmetric quasi-definite (SQD) systems","category":"page"},{"location":"","page":"Home","title":"Home","text":"  beginbmatrix M  phantom-A  A^T  -N endbmatrix beginbmatrix x  y endbmatrix = left(beginbmatrix b  0 endbmatrixbeginbmatrix 0  c endbmatrixbeginbmatrix b  c endbmatrixright)","category":"page"},{"location":"","page":"Home","title":"Home","text":"where A can have any shape.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Krylov solvers are particularly appropriate in situations where such problems must be solved but a factorization is not possible, either because:","category":"page"},{"location":"","page":"Home","title":"Home","text":"A is not available explicitly,\nA would be dense or would consume an excessive amount of memory if it were materialized,\nfactors would consume an excessive amount of memory.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Iterative methods are recommended in either of the following situations:","category":"page"},{"location":"","page":"Home","title":"Home","text":"the problem is sufficiently large that a factorization is not feasible or would be slow,\nan effective preconditioner is known in cases where the problem has unfavorable spectral structure,\nthe operator can be represented efficiently as a sparse matrix,\nthe operator is fast, i.e., can be applied with better complexity than if it were materialized as a matrix. Certain fast operators would materialize as dense matrices.","category":"page"},{"location":"#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"All solvers in Krylov.jl are compatible with GPU and work in any floating-point data type.","category":"page"},{"location":"#How-to-Install","page":"Home","title":"How to Install","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Krylov can be installed and tested through the Julia package manager:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> ]\npkg> add Krylov\npkg> test Krylov","category":"page"}]
}
